{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CP2_1팀.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "222459c5ee2944d5bd44cc53ddce4ff3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5234b8b9121e47a5976be5eb465cff3b",
              "IPY_MODEL_1ccc430f513042ccb94bb93cb0f21c16"
            ],
            "layout": "IPY_MODEL_3453e38bd2604db0b300edb7605fc154"
          }
        },
        "5234b8b9121e47a5976be5eb465cff3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ce738a064ec406cadcb6dbaf75502ee",
            "placeholder": "​",
            "style": "IPY_MODEL_d3e8facd521348348a4f5aa96f58c8a2",
            "value": "0.796 MB of 0.796 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "1ccc430f513042ccb94bb93cb0f21c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285adafcb9a7418bbd15fde47a7987ab",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1539ddf240f4aaab9f27b72601939b3",
            "value": 1
          }
        },
        "3453e38bd2604db0b300edb7605fc154": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ce738a064ec406cadcb6dbaf75502ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3e8facd521348348a4f5aa96f58c8a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "285adafcb9a7418bbd15fde47a7987ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1539ddf240f4aaab9f27b72601939b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "039300ff15614fda89ca7a69043d2087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8aac4e46c57243438e60e8ba90e71a5b",
              "IPY_MODEL_acd27e6031c24cfebaa9ad6686f4aff4"
            ],
            "layout": "IPY_MODEL_de97aebc99584dae85c7906c21850a8a"
          }
        },
        "8aac4e46c57243438e60e8ba90e71a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2050859ec454e22b4b6c8e4aa516fa7",
            "placeholder": "​",
            "style": "IPY_MODEL_fe3e7a93a57846d4814f989153b33d4b",
            "value": "0.802 MB of 0.802 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "acd27e6031c24cfebaa9ad6686f4aff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82c3315ac3784b358dd3bfb48f5374a6",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_687736a958b14646bd417b19382e67b9",
            "value": 1
          }
        },
        "de97aebc99584dae85c7906c21850a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2050859ec454e22b4b6c8e4aa516fa7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe3e7a93a57846d4814f989153b33d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82c3315ac3784b358dd3bfb48f5374a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687736a958b14646bd417b19382e67b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STS(Semantic Textual Similarity) Model"
      ],
      "metadata": {
        "id": "gMWzWjpqKKPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 전처리 및 EDA\n",
        "- 데이터 결측치 및 중복문장 제거\n",
        "- 데이터 특수문자 제거\n",
        "- 데이터 특징\n",
        " - 0에 편향된 분포를 가진 Label\n",
        " - binary-labele은 3 이하의 값을 나타낸다"
      ],
      "metadata": {
        "id": "kFF16CgKLjLp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDnuzUs5E99J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51bc367b-1e4b-4353-e2df-e7dc59f7a065"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 3.4 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 25.9 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.12.17-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.12.0+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 62.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 65.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->sentence-transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 55.2 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.2.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.12-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 54.6 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n",
            "Building wheels for collected packages: sentence-transformers, pathtools\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120747 sha256=6e47d05b43d3fc55bb7a3f892c5112abee5c0cf041a499f080e6e85133d2f808\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=dfd8012f36cebbde77c9e730e43636a2ddc9c9427cd7f9e1001c5ce9ad48ca99\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built sentence-transformers pathtools\n",
            "Installing collected packages: smmap, pyyaml, tokenizers, huggingface-hub, gitdb, transformers, shortuuid, setproctitle, sentry-sdk, sentencepiece, pathtools, GitPython, docker-pycreds, wandb, sentence-transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 huggingface-hub-0.7.0 pathtools-0.1.2 pyyaml-6.0 sentence-transformers-2.2.0 sentencepiece-0.1.96 sentry-sdk-1.5.12 setproctitle-1.2.3 shortuuid-1.0.9 smmap-5.0.0 tokenizers-0.12.1 transformers-4.19.2 wandb-0.12.17\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers transformers wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb api키 입력\n",
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "FgJIS-_mFFTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx0DVCmUFFp3",
        "outputId": "ad5323a1-7a50-4bd3-9450-33458610362c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import requests\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel, RobertaModel, RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, random_split\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from sklearn.model_selection import train_test_split\n",
        "from functools import partial\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import pearsonr"
      ],
      "metadata": {
        "id": "1fqxR0nyFHS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96wqfkinFJQW",
        "outputId": "e1436006-2b4b-45c3-d813-d0ed8d0e09df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Klue dataset 다운\n",
        "!wget https://aistages-prod-server-public.s3.amazonaws.com/app/Competitions/000067/data/klue-sts-v1.1.tar.gz"
      ],
      "metadata": {
        "id": "PY4GYvk2X_db"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 압출풀기\n",
        "tar_file = tarfile.open('/content/drive/MyDrive/NLP/klue-sts-v1.1.tar.gz')\n",
        "tar_file.extractall(path='/content/drive/MyDrive/NLP')\n",
        "tar_file.close()"
      ],
      "metadata": {
        "id": "cAxsuB6SYDin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 로드\n",
        "df1 = pd.read_json('/content/drive/MyDrive/NLP/klue-sts-v1.1/klue-sts-v1.1_train.json')\n",
        "test = pd.read_json('/content/drive/MyDrive/NLP/klue-sts-v1.1/klue-sts-v1.1_dev.json')"
      ],
      "metadata": {
        "id": "Jnb_9vF9YE9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "PgR7hhy1YGZN",
        "outputId": "4241180f-ee11-44a6-c1a4-f9a65055637b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      guid           source  \\\n",
              "0  klue-sts-v1_train_00000       airbnb-rtt   \n",
              "1  klue-sts-v1_train_00001   policy-sampled   \n",
              "2  klue-sts-v1_train_00002  paraKQC-sampled   \n",
              "3  klue-sts-v1_train_00003   policy-sampled   \n",
              "4  klue-sts-v1_train_00004       airbnb-rtt   \n",
              "\n",
              "                                           sentence1  \\\n",
              "0                   숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.   \n",
              "1      위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다.   \n",
              "2            회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘.   \n",
              "3  긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...   \n",
              "4                        호스트의 답장이 늦으나, 개선될 것으로 보입니다.   \n",
              "\n",
              "                                    sentence2  \\\n",
              "0  숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.   \n",
              "1       시민들 스스로 자발적인 예방 노력을 한 것은 아산 뿐만이 아니었다.   \n",
              "2                  사람들이 주로 네이버 메일을 쓰는 이유를 알려줘   \n",
              "3   고용보험이 1차 고용안전망이라면, 국민취업지원제도는 2차 고용안전망입니다.   \n",
              "4                  호스트 응답이 늦었지만 개선될 것으로 보입니다.   \n",
              "\n",
              "                                              labels  \\\n",
              "0  {'label': 3.7, 'real-label': 3.714285714285714...   \n",
              "1  {'label': 0.0, 'real-label': 0.0, 'binary-labe...   \n",
              "2  {'label': 0.30000000000000004, 'real-label': 0...   \n",
              "3  {'label': 0.6000000000000001, 'real-label': 0....   \n",
              "4  {'label': 4.7, 'real-label': 4.714285714285714...   \n",
              "\n",
              "                                         annotations  \n",
              "0  {'agreement': '0:0:0:2:5:0', 'annotators': ['0...  \n",
              "1  {'agreement': '5:0:0:0:0:0', 'annotators': ['1...  \n",
              "2  {'agreement': '4:2:0:0:0:0', 'annotators': ['1...  \n",
              "3  {'agreement': '4:2:1:0:0:0', 'annotators': ['1...  \n",
              "4  {'agreement': '0:0:0:0:2:5', 'annotators': ['1...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-39537bca-d4fc-4a3d-8bd9-bb5419ea1b7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>guid</th>\n",
              "      <th>source</th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>labels</th>\n",
              "      <th>annotations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klue-sts-v1_train_00000</td>\n",
              "      <td>airbnb-rtt</td>\n",
              "      <td>숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.</td>\n",
              "      <td>숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.</td>\n",
              "      <td>{'label': 3.7, 'real-label': 3.714285714285714...</td>\n",
              "      <td>{'agreement': '0:0:0:2:5:0', 'annotators': ['0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>klue-sts-v1_train_00001</td>\n",
              "      <td>policy-sampled</td>\n",
              "      <td>위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다.</td>\n",
              "      <td>시민들 스스로 자발적인 예방 노력을 한 것은 아산 뿐만이 아니었다.</td>\n",
              "      <td>{'label': 0.0, 'real-label': 0.0, 'binary-labe...</td>\n",
              "      <td>{'agreement': '5:0:0:0:0:0', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>klue-sts-v1_train_00002</td>\n",
              "      <td>paraKQC-sampled</td>\n",
              "      <td>회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘.</td>\n",
              "      <td>사람들이 주로 네이버 메일을 쓰는 이유를 알려줘</td>\n",
              "      <td>{'label': 0.30000000000000004, 'real-label': 0...</td>\n",
              "      <td>{'agreement': '4:2:0:0:0:0', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>klue-sts-v1_train_00003</td>\n",
              "      <td>policy-sampled</td>\n",
              "      <td>긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...</td>\n",
              "      <td>고용보험이 1차 고용안전망이라면, 국민취업지원제도는 2차 고용안전망입니다.</td>\n",
              "      <td>{'label': 0.6000000000000001, 'real-label': 0....</td>\n",
              "      <td>{'agreement': '4:2:1:0:0:0', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>klue-sts-v1_train_00004</td>\n",
              "      <td>airbnb-rtt</td>\n",
              "      <td>호스트의 답장이 늦으나, 개선될 것으로 보입니다.</td>\n",
              "      <td>호스트 응답이 늦었지만 개선될 것으로 보입니다.</td>\n",
              "      <td>{'label': 4.7, 'real-label': 4.714285714285714...</td>\n",
              "      <td>{'agreement': '0:0:0:0:2:5', 'annotators': ['1...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-39537bca-d4fc-4a3d-8bd9-bb5419ea1b7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-39537bca-d4fc-4a3d-8bd9-bb5419ea1b7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-39537bca-d4fc-4a3d-8bd9-bb5419ea1b7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복문장 확인\n",
        "df1.duplicated(['sentence1', 'sentence2']).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwZD4Bm2YJ9p",
        "outputId": "c26c4717-6380-479a-f648-ae3235f553c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjl3o0QyYLOV",
        "outputId": "fa2fbc0a-67a9-45b5-9683-94fc9bbbd2fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "guid           0\n",
              "source         0\n",
              "sentence1      0\n",
              "sentence2      0\n",
              "labels         0\n",
              "annotations    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복문장 제거\n",
        "d1f = df1.drop_duplicates(['sentence1','sentence2'], keep='first', ignore_index=True)"
      ],
      "metadata": {
        "id": "JmK97My3YMW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3종류의 라벨링 binaly-label은 1이 같은문장 2가 다른문장 \n",
        "df1.labels[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTsLzGdZYNZa",
        "outputId": "e3aee2b1-11e8-4645-b700-489a3d89eec3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'binary-label': 1, 'label': 3.7, 'real-label': 3.714285714285714}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 분리\n",
        "labels = df1.labels.to_list()\n",
        "labels = pd.DataFrame(labels)\n",
        "print(len(labels))\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "9zLiV2YpYOwD",
        "outputId": "43cbcc8b-aabd-44cf-f5ec-0a9f52d18018"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11668\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       label  real-label  binary-label\n",
              "0        3.7    3.714286             1\n",
              "1        0.0    0.000000             0\n",
              "2        0.3    0.333333             0\n",
              "3        0.6    0.571429             0\n",
              "4        4.7    4.714286             1\n",
              "...      ...         ...           ...\n",
              "11663    4.0    4.000000             1\n",
              "11664    0.0    0.000000             0\n",
              "11665    3.7    3.666667             1\n",
              "11666    4.7    4.714286             1\n",
              "11667    3.3    3.333333             1\n",
              "\n",
              "[11668 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1991a46-f19a-4d41-b2c9-c09b51ff1517\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>real-label</th>\n",
              "      <th>binary-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.7</td>\n",
              "      <td>3.714286</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.3</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.6</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4.7</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11663</th>\n",
              "      <td>4.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11664</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11665</th>\n",
              "      <td>3.7</td>\n",
              "      <td>3.666667</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11666</th>\n",
              "      <td>4.7</td>\n",
              "      <td>4.714286</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11667</th>\n",
              "      <td>3.3</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11668 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1991a46-f19a-4d41-b2c9-c09b51ff1517')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1991a46-f19a-4d41-b2c9-c09b51ff1517 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1991a46-f19a-4d41-b2c9-c09b51ff1517');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# binary-labele 은 score 3미만인 데이터로 이루어져 있다.\n",
        "labels['binary-label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbUMyPRvYPvp",
        "outputId": "c0cb3e10-186e-408b-a9d5-7f92a8e2db35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6066\n",
              "1    5602\n",
              "Name: binary-label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습에 필요한 column만 남기기\n",
        "df1 = df1[['sentence1', 'sentence2']].join(labels[['binary-label', 'real-label']])\n",
        "df1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "DsN6OEvEYQ_s",
        "outputId": "788bb2a8-4822-4929-99e2-9c963e6ec42f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           sentence1  \\\n",
              "0                   숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.   \n",
              "1      위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다.   \n",
              "2            회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘.   \n",
              "3  긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...   \n",
              "4                        호스트의 답장이 늦으나, 개선될 것으로 보입니다.   \n",
              "\n",
              "                                    sentence2  binary-label  real-label  \n",
              "0  숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.             1    3.714286  \n",
              "1       시민들 스스로 자발적인 예방 노력을 한 것은 아산 뿐만이 아니었다.             0    0.000000  \n",
              "2                  사람들이 주로 네이버 메일을 쓰는 이유를 알려줘             0    0.333333  \n",
              "3   고용보험이 1차 고용안전망이라면, 국민취업지원제도는 2차 고용안전망입니다.             0    0.571429  \n",
              "4                  호스트 응답이 늦었지만 개선될 것으로 보입니다.             1    4.714286  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fff8189-324a-49e7-816c-e997acaa3f83\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence1</th>\n",
              "      <th>sentence2</th>\n",
              "      <th>binary-label</th>\n",
              "      <th>real-label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>숙소 위치는 찾기 쉽고 일반적인 한국의 반지하 숙소입니다.</td>\n",
              "      <td>숙박시설의 위치는 쉽게 찾을 수 있고 한국의 대표적인 반지하 숙박시설입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>3.714286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>위반행위 조사 등을 거부·방해·기피한 자는 500만원 이하 과태료 부과 대상이다.</td>\n",
              "      <td>시민들 스스로 자발적인 예방 노력을 한 것은 아산 뿐만이 아니었다.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>회사가 보낸 메일은 이 지메일이 아니라 다른 지메일 계정으로 전달해줘.</td>\n",
              "      <td>사람들이 주로 네이버 메일을 쓰는 이유를 알려줘</td>\n",
              "      <td>0</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>긴급 고용안정지원금은 지역고용대응 등 특별지원금, 지자체별 소상공인 지원사업, 취업...</td>\n",
              "      <td>고용보험이 1차 고용안전망이라면, 국민취업지원제도는 2차 고용안전망입니다.</td>\n",
              "      <td>0</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>호스트의 답장이 늦으나, 개선될 것으로 보입니다.</td>\n",
              "      <td>호스트 응답이 늦었지만 개선될 것으로 보입니다.</td>\n",
              "      <td>1</td>\n",
              "      <td>4.714286</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fff8189-324a-49e7-816c-e997acaa3f83')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7fff8189-324a-49e7-816c-e997acaa3f83 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7fff8189-324a-49e7-816c-e997acaa3f83');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# real-label 의 데이터 분포\n",
        "la1 = df1.loc[(df1['real-label'] >= 0) & (df1['real-label'] < 1.0)]\n",
        "la2 = df1.loc[(df1['real-label'] >= 1.0) & (df1['real-label'] < 2.0)]\n",
        "la3 = df1.loc[(df1['real-label'] >= 2.0) & (df1['real-label'] < 3.0)]\n",
        "la4 = df1.loc[(df1['real-label'] >= 3.0) & (df1['real-label'] < 4.0)]\n",
        "la5 = df1.loc[(df1['real-label'] >= 4.0) & (df1['real-label'] < 5.0)]\n",
        "\n",
        "print(f' Score 1미만인 데이터: {len(la1)}\\n Score 2미만인 데이터: {len(la2)}\\n Score 3미만인 데이터: {len(la3)}\\n Score 4미만인 데이터: {len(la4)}\\n Score 5미만인 데이터: {len(la5)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_ujbgJVM-hD",
        "outputId": "cb6a2683-92e2-40b6-c22d-43b3e441541d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Score 1미만인 데이터: 4350\n",
            " Score 2미만인 데이터: 906\n",
            " Score 3미만인 데이터: 810\n",
            " Score 4미만인 데이터: 2852\n",
            " Score 5미만인 데이터: 2705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# binary-label 의 데이터 분포\n",
        "print(f' Score 3미만인 데이터: {len(la1+la2+la3)}\\n Score 3이상인 데이터: {len(la4+la5)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH_SnJ-5Pp5I",
        "outputId": "67e35b58-4af8-4660-de1e-bcd870208b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Score 3미만인 데이터: 6066\n",
            " Score 3이상인 데이터: 5557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트셋 중복 확인\n",
        "test.duplicated(['sentence1', 'sentence2']).sum()"
      ],
      "metadata": {
        "id": "6fAi8wAJnyUx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b77efb63-8c28-49e5-b0b0-dac40440d81d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결측치 확인\n",
        "test.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dm81HVGZDZq",
        "outputId": "7f66281c-ef28-46f3-f868-ed0967daabd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "guid           0\n",
              "source         0\n",
              "sentence1      0\n",
              "sentence2      0\n",
              "labels         0\n",
              "annotations    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 라벨 변경\n",
        "labels2 = test.labels.to_list()\n",
        "labels2 = pd.DataFrame(labels2)\n",
        "test = test[['sentence1', 'sentence2']].join(labels2[['binary-label', 'real-label']])"
      ],
      "metadata": {
        "id": "Ae2wZ0FhZEYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KorNLU Datasets 추가\n",
        "\n",
        "[출처](https://github.com/kakaobrain/KorNLUDatasets/)"
      ],
      "metadata": {
        "id": "jgtFu2Wq7JUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KorNLUdataset 다운\n",
        "!git clone https://github.com/kakaobrain/KorNLUDatasets/"
      ],
      "metadata": {
        "id": "E0w5OAsY7IW5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34021f8e-10fb-4612-f05c-7d05bc157e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'KorNLUDatasets' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/NLP/KorNLUDatasets/KorSTS/sts-train.tsv'"
      ],
      "metadata": {
        "id": "FMFXvDmC7IxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(path, sep=\"\\t+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkqrtZlB8KpJ",
        "outputId": "10c540a1-1c76-43e5-d9e9-bb9be4304398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 스코어로 되어있는 형태\n",
        "df2"
      ],
      "metadata": {
        "id": "udAERkPD8Rth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복값 확인\n",
        "df2.duplicated(['sentence1', 'sentence2']).sum()"
      ],
      "metadata": {
        "id": "bTM1UnGBYWpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 중복값 제거\n",
        "df2 = df2.drop_duplicates(['sentence1','sentence2'], keep='first', ignore_index=True)"
      ],
      "metadata": {
        "id": "8EfwCjasYYFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 위와 같이 스코어 3 미만은 0 3 이상은 1 로 분류\n",
        "df2['binary-label'] = df2['score'].apply(lambda x: 0 if x < 3  else  1)"
      ],
      "metadata": {
        "id": "LeQvUTaWYZFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 스코어 확인\n",
        "la11 = len(df2.loc[(df2['score'] >= 0) & (df2['score'] < 3.0)])\n",
        "la11"
      ],
      "metadata": {
        "id": "Nzf2qiXmYaNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 분류 확인\n",
        "df2['binary-label'].value_counts()"
      ],
      "metadata": {
        "id": "jnvAGWH3Ybat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이름변경\n",
        "df2.rename(columns = {'score':'real-label'},inplace=True)"
      ],
      "metadata": {
        "id": "pbkf-z8KYcOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 column만 남기기\n",
        "df2 = df2[['sentence1', 'sentence2', 'binary-label', 'real-label']]\n",
        "df2"
      ],
      "metadata": {
        "id": "oQuLIbC3YdIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 두 데이터 합치기\n",
        "df = pd.concat([df1,df2],axis=0, join='inner', ignore_index=True)"
      ],
      "metadata": {
        "id": "7oh4--3wYzRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인 11661 + 5700\n",
        "df.shape"
      ],
      "metadata": {
        "id": "VnUNHSbLYzrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# real-label 의 데이터 분포\n",
        "la1 = df.loc[(df['real-label'] >= 0) & (df['real-label'] < 1.0)]\n",
        "la2 = df.loc[(df['real-label'] >= 1.0) & (df['real-label'] < 2.0)]\n",
        "la3 = df.loc[(df['real-label'] >= 2.0) & (df['real-label'] < 3.0)]\n",
        "la4 = df.loc[(df['real-label'] >= 3.0) & (df['real-label'] < 4.0)]\n",
        "la5 = df.loc[(df['real-label'] >= 4.0) & (df['real-label'] < 5.0)]\n",
        "\n",
        "print(f' Score 1미만인 데이터: {len(la1)}\\n Score 2미만인 데이터: {len(la2)}\\n Score 3미만인 데이터: {len(la3)}\\n Score 4미만인 데이터: {len(la4)}\\n Score 5미만인 데이터: {len(la5)}')"
      ],
      "metadata": {
        "id": "RGDSViEDY0w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary-label 의 데이터 분포\n",
        "print(f' Score 3미만인 데이터: {len(la1+la2+la3)}\\n Score 3이상인 데이터: {len(la4+la5)}')"
      ],
      "metadata": {
        "id": "toW2IhnIY4_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# real-label 의 분포도 히스토그램 그래프\n",
        "ax = plt.subplot()\n",
        "sns.distplot(df['real-label'], hist=True, kde=False)\n",
        "plt.xlabel('label')\n",
        "plt.ylabel('value')\n",
        "sns.set(rc = {'figure.figsize':(10,10)})\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_4Q7vGqwYk_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# binary-label의 빈도수 그래프\n",
        "sns.countplot(df['binary-label'], palette= \"RdPu\")"
      ],
      "metadata": {
        "id": "mckIJn2fYmVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 전처리 \n",
        "import html\n",
        "import regex as re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "def preprocess(sentence):\n",
        "    sen = BeautifulSoup(html.unescape(sentence), 'html.parser').text   \n",
        "    sen = sen.replace(\"\\n\", \" \")                                    # \\n 공백으로 대체 \n",
        "    sen = re.sub('\"',' ', sen)                                      # 따음표를 공백으로 대체\n",
        "    sen = re.sub(\"[^a-zA-Z0-9가-힣]\", \" \", sen)                  # 특수문자 제거하고 한글과 영어만 사용\n",
        "    return sen\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_train(df):\n",
        "    sentence1 = df['sentence1'].tolist()\n",
        "    sentence2 = df['sentence2'].tolist()\n",
        "    real_label = df['real-label'].tolist()\n",
        "    label = df['binary-label'].tolist()\n",
        "\n",
        "    processed1 = []\n",
        "    processed2 = []\n",
        "\n",
        "    for sen1 in sentence1:\n",
        "        processed1.append(preprocess(sen1))\n",
        "    for sen2 in sentence2:\n",
        "        processed2.append(preprocess(sen2))\n",
        "    \n",
        "    processed_df = pd.DataFrame(list(zip(processed1, processed2, real_label, label)),\n",
        "                        columns = ['sentence1', 'sentence2', 'real-label', 'binary-label'])\n",
        "\n",
        "\n",
        "    return processed_df"
      ],
      "metadata": {
        "id": "H-OIZqnxZF0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = preprocess_train(df)"
      ],
      "metadata": {
        "id": "ZrxslXNuZJ6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋 9:1 비율로 train과 val 데이터 나누기\n",
        "train, val = train_test_split(df, test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "Ta31xckamH4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스 초기화\n",
        "train_data = train.reset_index().drop(['index'], axis = 1)\n",
        "valid_data = val.reset_index().drop(['index'], axis = 1)\n",
        "test_data = test.reset_index().drop(['index'], axis = 1)"
      ],
      "metadata": {
        "id": "w7QYjFyEGJJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-wlccxKGKXp",
        "outputId": "da373c65-f186-4f1b-9ddd-34e8acf1d0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델, 토크나이저 불러오기\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
      ],
      "metadata": {
        "id": "YP6Juv4EGLYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링"
      ],
      "metadata": {
        "id": "Dpw9SvQKFhHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 프레임의 입력을 받아 (input, target) 형태의 Dataset 생성\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data) -> None: \n",
        "        self.data = data            \n",
        "        self.input, self.label = list(zip(self.data['sentence1'], self.data['sentence2'])), self.data['real-label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label) \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.input[index], self.label[index]  "
      ],
      "metadata": {
        "id": "jFlBtcUWGMbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch, max_length):\n",
        "\n",
        "    global tokenizer\n",
        "  \n",
        "    input_list, target_list = zip(*batch) \n",
        "    tensorized_input = tokenizer.batch_encode_plus(\n",
        "\n",
        "        [(sentences[0], sentences[1]) for sentences in input_list],\n",
        "        max_length = max_length, # \n",
        "        padding= \"max_length\",\n",
        "        add_special_tokens=True,\n",
        "        truncation=True,\n",
        "        return_tensors='pt',\n",
        "    )\n",
        "    \n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "  \n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "POpJ7r_nGNeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = test.reset_index().drop(['index'], axis = 1)\n",
        "test_dataset = CustomDataset(test_data)"
      ],
      "metadata": {
        "id": "ZDlofH3h5PxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_data)\n",
        "valid_dataset = CustomDataset(valid_data)\n",
        "test_dataset = CustomDataset(test_data)"
      ],
      "metadata": {
        "id": "24AI3BEZGO2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n",
        "\n",
        "    file_name = f'/content/drive/MyDrive/AI09/model.ckpt.{epoch}'\n",
        "        \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "BZD0g84oGPyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):    \n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    batch_loss = 0\n",
        "    pred_list = None \n",
        "\n",
        "    for step, batch in enumerate(dataloader):       \n",
        "        batch_count += 1\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input, labels = batch_label.float()) \n",
        "\n",
        "        loss = outputs.loss \n",
        "        pred = outputs.logits.squeeze()\n",
        "        \n",
        "        if pred_list is None:\n",
        "           pred_list = pred.detach().cpu().numpy()\n",
        "           label_list = batch_label.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_list = np.append(pred_list, pred.detach().cpu().numpy(), axis=0)\n",
        "            label_list = np.append(label_list, batch_label.detach().cpu().numpy(), axis=0)        \n",
        "        \n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if (step % 10) == 0 and step != 0:  \n",
        "            print(f\"Step : {step}, valid Loss : {batch_loss / batch_count:.4f}\")\n",
        "            wandb.log({'valid_loss': batch_loss / batch_count})    \n",
        "            batch_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "    fone_pred = np.where(pred_list >=3, 1, 0)\n",
        "    fone_label = np.where(label_list >=3, 1, 0)     \n",
        "    fone = f1_score(fone_pred, fone_label) * 100\n",
        "    p_score = pearsonr(pred_list, label_list)[0] * 100  \n",
        "       \n",
        "    total_valid_loss = total_loss / (step + 1)              \n",
        "           \n",
        "    wandb.log({'total_valid_loss': total_valid_loss, \"total_f1_score \": fone, \"total_pearsonr\" : p_score})     \n",
        "   \n",
        "    return total_valid_loss, fone, p_score"
      ],
      "metadata": {
        "id": "7tfLwwqyGZjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs):   \n",
        "\n",
        "    wandb.watch(model, log=\"all\", log_freq = 10)\n",
        "      \n",
        "    for epoch in range(epochs):\n",
        "        print(f'****** Starting To Train Epoch #{epoch} ******')\n",
        "\n",
        "        total_loss = 0\n",
        "        batch_loss = 0\n",
        "        batch_count = 0      \n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "\n",
        "        \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_count += 1\n",
        "            batch = tuple(item.to(device) for item in batch)\n",
        "           \n",
        "            batch_input, batch_label = batch\n",
        "            model.zero_grad()\n",
        "\n",
        "            outputs = model(**batch_input, labels = batch_label.float())\n",
        "            loss = outputs.loss \n",
        "            \n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "             \n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            \n",
        "            if (step % 10) == 0 and step != 0:\n",
        "                wandb.log({'train_loss': batch_loss / batch_count, 'train_lr': optimizer.param_groups[0]['lr']})                    \n",
        "                print(f\"Epoch: {epoch}, Step : {step}, LR : {optimizer.param_groups[0]['lr']}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "                batch_loss, batch_count = 0,0\n",
        "                \n",
        "        wandb.log({'total_train_loss': total_loss / (step + 1), 'total_train_lr': optimizer.param_groups[0]['lr'], \"epoch\" : (epoch + 1)})\n",
        "        print(f\"Epoch {epoch} total_train_loss : {total_loss/(step+1):.4f}\")\n",
        "        print(f\"***** Finish To Train Epoch {epoch} *****\\n\") \n",
        "\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        total_valid_loss, fone, p_score = validate(model, valid_dataloader)\n",
        "        print('total_valid_loss : ', total_valid_loss, \"val_f1_score : \",  fone,  \"val_pearsonr :\",  p_score)  \n",
        "        print(f\"Epoch {epoch} total_Valid Loss : {total_valid_loss:.4f}\") \n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "        save_checkpoint(model, optimizer, scheduler,  epoch, total_valid_loss)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Train Finished\")"
      ],
      "metadata": {
        "id": "RKoBsyP9GbLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    \"name\" : \"AI09_v0\",   \n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\" : \"total_valid_loss\", \n",
        "        \"goal\" : \"minimize\"\n",
        "                },\n",
        "    \n",
        "    \"parameters\": { \n",
        "        \"epochs\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [4]},                     \n",
        "        \"learning_rate\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-5]},                     \n",
        "        \"eps\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-8]\n",
        "        },\n",
        "        \"train_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [8]\n",
        "        },\n",
        "        \"valid_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [16]\n",
        "        },\n",
        "        \"weight_decay\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0]\n",
        "        },\n",
        "        \"warm_up_ratio\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0]\n",
        "        },\n",
        "        \"max_length\" : {\n",
        "            \"distribution\" : \"categorical\",       \n",
        "            \"values\" : [128]\n",
        "        },\n",
        "        \"grad_norm\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1.0]\n",
        "        },\n",
        "    },         \n",
        "    \"early_terminate\" : {\n",
        "        \"type\": \"hyperband\", \n",
        "        \"min_iter\" : 2,\n",
        "        \"eta\" : 2\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "0atlZKb_Gdhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(config=None):\n",
        "\n",
        "    wandb.init(config=config)\n",
        "    \n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "    \n",
        "\n",
        "    optimizer = AdamW(\n",
        "                      optimizer_grouped_parameters,\n",
        "                      lr = 1e-5,\n",
        "                      eps = 1e-8\n",
        "                      ) \n",
        "    num_training_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                                                optimizer=optimizer, \n",
        "                                                num_warmup_steps= (num_training_steps * 0),\n",
        "                                                num_training_steps = num_training_steps\n",
        "                                                )\n",
        "    \n",
        " \n",
        "    train(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)   "
      ],
      "metadata": {
        "id": "pE-EQXDrGf6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "                              train_dataset,\n",
        "                              batch_size = 8,\n",
        "                              sampler = RandomSampler(train_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length=128)\n",
        "                              )\n",
        "valid_dataloader = DataLoader(\n",
        "                              valid_dataset,\n",
        "                              batch_size = 16,\n",
        "                              sampler = SequentialSampler(valid_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                              )\n",
        "test_dataloader = DataLoader(\n",
        "                            test_dataset, \n",
        "                            batch_size = 16,\n",
        "                            sampler = SequentialSampler(test_dataset),\n",
        "                            collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                            )"
      ],
      "metadata": {
        "id": "i4kPx5QrGhjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 추론 검증"
      ],
      "metadata": {
        "id": "akOLRmE0F5Mj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader):    \n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    batch_loss = 0\n",
        "    \n",
        "    pred_np = None\n",
        "\n",
        "    for step, batch in enumerate(dataloader):       \n",
        "        batch_count += 1\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "             outputs = model(**batch_input, labels = batch_label)\n",
        "    \n",
        "        loss = outputs.loss\n",
        "        pred = outputs.logits.squeeze()\n",
        "\n",
        "        if pred_np is None:\n",
        "            pred_np = pred.detach().cpu().numpy()\n",
        "            label_np = batch_label.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_np = np.append(pred_np, pred.detach().cpu().numpy(), axis=0)\n",
        "            label_np = np.append(label_np, batch_label.detach().cpu().numpy(), axis=0)\n",
        "        \n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "                        \n",
        "        if (step % 10) == 0 and step != 0:\n",
        "            #print('test_loss : ' ,batch_loss / batch_count)                           \n",
        "            batch_loss, batch_count = 0, 0\n",
        "\n",
        "    total_valid_loss = total_loss / (step + 1)\n",
        "\n",
        "    fone_pred = np.where(pred_np >=3, 1, 0)\n",
        "    fone_label = np.where(label_np >=3, 1, 0)\n",
        "       \n",
        "    fone= f1_score(fone_pred , fone_label) * 100\n",
        "    p_score = pearsonr(pred_np, label_np)[0] * 100           \n",
        "    print('total_test_loss : ' , total_valid_loss, \"total_f1_score : \" , fone, \"total_pearsonr:\" , p_score)"
      ],
      "metadata": {
        "id": "KIU9Jz5KKeK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"AI09_v0\")\n",
        "wandb.agent(sweep_id, initializer, count = 1)"
      ],
      "metadata": {
        "id": "pZGA_QQPo7qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt1 = '/content/drive/MyDrive/AI09/model.ckpt.0'\n",
        "ckpt2 = '/content/drive/MyDrive/AI09/model.ckpt.1'\n",
        "ckpt3 = '/content/drive/MyDrive/AI09/model.ckpt.2'\n",
        "ckpt4 = '/content/drive/MyDrive/AI09/model.ckpt.3'"
      ],
      "metadata": {
        "id": "51h2SM_jaCgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "all_checkpoints = [ckpt1, ckpt2, ckpt3, ckpt4]\n",
        "\n",
        "for checkpoint in all_checkpoints:\n",
        "    loaded_ckpt = torch.load(checkpoint)\n",
        "    loaded_ckpt['epoch'], loaded_ckpt['loss']\n",
        "    model.load_state_dict(loaded_ckpt[\"model_state_dict\"])\n",
        "    test(model, test_dataloader)"
      ],
      "metadata": {
        "id": "5VMXrVueuzb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper Parameter Tuning\n",
        "- wandb에 내장된 sweep 기능을 활용하여 하이퍼 파라미터 튜닝을 진행\n",
        "\n",
        "Tuning 한 Parameter들\n",
        "\n",
        "> epochs : 4   \n",
        "> IR : 1e-5, 2e-5, 3e-5  \n",
        "> eps : 1e-8  \n",
        "> train_set_batch_size : 8, 16  \n",
        "> weight_decay : 0, 0.01  \n",
        "> warm_up_ratio : 0, 0.1"
      ],
      "metadata": {
        "id": "ui4LcfA_rFDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://velog.velcdn.com/images/khyait/post/8b8dbd46-12e3-4716-b018-a10a2c183c20/image.png)\n",
        "\n",
        "- 튜닝 결과\n",
        "> epochs : 4   \n",
        "> IR : 2e-5  \n",
        "> eps : 1e-8  \n",
        "> train_set_batch_size : 8  \n",
        "> weight_decay : 0 \n",
        "> warm_up_ratio : 0.1\n",
        "\n",
        "위와 같은 Parameter로 진행"
      ],
      "metadata": {
        "id": "kRpQ_5_XtDjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    \n",
        "    \"name\" : \"AI09_v3\",   \n",
        "    \"method\": \"bayes\",\n",
        "    \"metric\": {\n",
        "        \"name\" : \"total_valid_loss\", \n",
        "        \"goal\" : \"minimize\"\n",
        "                },\n",
        "    \n",
        "    \"parameters\": { \n",
        "        \"epochs\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [4]},                     \n",
        "        \"learning_rate\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [2e-5]},                     \n",
        "        \"eps\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1e-8]\n",
        "        },\n",
        "        \"train_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [8]\n",
        "        },\n",
        "        \"valid_batch_size\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [16]\n",
        "        },\n",
        "        \"weight_decay\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0]\n",
        "        },\n",
        "        \"warm_up_ratio\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [0.1]\n",
        "        },\n",
        "        \"max_length\" : {\n",
        "            \"distribution\" : \"categorical\",       \n",
        "            \"values\" : [128]\n",
        "        },\n",
        "        \"grad_norm\" : {\n",
        "            \"distribution\" : \"categorical\",\n",
        "            \"values\" : [1.0]\n",
        "        },\n",
        "    },         \n",
        "    \"early_terminate\" : {\n",
        "        \"type\": \"hyperband\", \n",
        "        \"min_iter\" : 2,\n",
        "        \"eta\" : 2\n",
        "        }\n",
        "}"
      ],
      "metadata": {
        "id": "xCeOKgFgrEfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n",
        "\n",
        "    file_name = f'/content/drive/MyDrive/AI09/model_v3.ckpt.{epoch}'\n",
        "        \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "tET0hDwYazVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(config=None):\n",
        "\n",
        "    wandb.init(config=config)\n",
        "    \n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': 0},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
        "    \n",
        "\n",
        "    optimizer = AdamW(\n",
        "                      optimizer_grouped_parameters,\n",
        "                      lr = 2e-5,\n",
        "                      eps = 1e-8\n",
        "                      ) \n",
        "    num_training_steps = epochs * len(train_dataloader)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "                                                optimizer=optimizer, \n",
        "                                                num_warmup_steps= (num_training_steps * 0.1),\n",
        "                                                num_training_steps = num_training_steps\n",
        "                                                )\n",
        "    \n",
        " \n",
        "    train(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)   "
      ],
      "metadata": {
        "id": "P76O9NPkuNKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"AI09_v3\")\n",
        "wandb.agent(sweep_id, initializer, count = 1)"
      ],
      "metadata": {
        "id": "qxj_T1T6uaoU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "222459c5ee2944d5bd44cc53ddce4ff3",
            "5234b8b9121e47a5976be5eb465cff3b",
            "1ccc430f513042ccb94bb93cb0f21c16",
            "3453e38bd2604db0b300edb7605fc154",
            "7ce738a064ec406cadcb6dbaf75502ee",
            "d3e8facd521348348a4f5aa96f58c8a2",
            "285adafcb9a7418bbd15fde47a7987ab",
            "f1539ddf240f4aaab9f27b72601939b3"
          ]
        },
        "outputId": "83117d04-7526-4712-e218-22db7cbd72ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: n5zwrnca\n",
            "Sweep URL: https://wandb.ai/kdb/AI09_v3/sweeps/n5zwrnca\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: phuahdr5 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgrad_norm: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220602_031858-phuahdr5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/AI09_v3/runs/phuahdr5\" target=\"_blank\">devoted-sweep-1</a></strong> to <a href=\"https://wandb.ai/kdb/AI09_v3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/AI09_v3/sweeps/n5zwrnca\" target=\"_blank\">https://wandb.ai/kdb/AI09_v3/sweeps/n5zwrnca</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Starting To Train Epoch #0 ******\n",
            "Epoch: 0, Step : 10, LR : 2.814738996929376e-07, Avg Loss : 8.6906\n",
            "Epoch: 0, Step : 20, LR : 5.373592630501535e-07, Avg Loss : 9.3178\n",
            "Epoch: 0, Step : 30, LR : 7.932446264073696e-07, Avg Loss : 6.6221\n",
            "Epoch: 0, Step : 40, LR : 1.0491299897645856e-06, Avg Loss : 7.9820\n",
            "Epoch: 0, Step : 50, LR : 1.3050153531218015e-06, Avg Loss : 5.4882\n",
            "Epoch: 0, Step : 60, LR : 1.5609007164790175e-06, Avg Loss : 6.1227\n",
            "Epoch: 0, Step : 70, LR : 1.8167860798362336e-06, Avg Loss : 5.7768\n",
            "Epoch: 0, Step : 80, LR : 2.0726714431934496e-06, Avg Loss : 4.3740\n",
            "Epoch: 0, Step : 90, LR : 2.3285568065506653e-06, Avg Loss : 3.4860\n",
            "Epoch: 0, Step : 100, LR : 2.5844421699078814e-06, Avg Loss : 2.7580\n",
            "Epoch: 0, Step : 110, LR : 2.8403275332650976e-06, Avg Loss : 3.1223\n",
            "Epoch: 0, Step : 120, LR : 3.0962128966223133e-06, Avg Loss : 3.0410\n",
            "Epoch: 0, Step : 130, LR : 3.3520982599795294e-06, Avg Loss : 2.8393\n",
            "Epoch: 0, Step : 140, LR : 3.6079836233367456e-06, Avg Loss : 2.3835\n",
            "Epoch: 0, Step : 150, LR : 3.863868986693961e-06, Avg Loss : 2.8328\n",
            "Epoch: 0, Step : 160, LR : 4.1197543500511774e-06, Avg Loss : 2.2637\n",
            "Epoch: 0, Step : 170, LR : 4.375639713408393e-06, Avg Loss : 2.9087\n",
            "Epoch: 0, Step : 180, LR : 4.63152507676561e-06, Avg Loss : 2.7333\n",
            "Epoch: 0, Step : 190, LR : 4.8874104401228255e-06, Avg Loss : 2.6934\n",
            "Epoch: 0, Step : 200, LR : 5.143295803480042e-06, Avg Loss : 1.6226\n",
            "Epoch: 0, Step : 210, LR : 5.399181166837258e-06, Avg Loss : 1.8967\n",
            "Epoch: 0, Step : 220, LR : 5.655066530194473e-06, Avg Loss : 1.6740\n",
            "Epoch: 0, Step : 230, LR : 5.91095189355169e-06, Avg Loss : 1.5378\n",
            "Epoch: 0, Step : 240, LR : 6.166837256908905e-06, Avg Loss : 1.1393\n",
            "Epoch: 0, Step : 250, LR : 6.422722620266121e-06, Avg Loss : 1.1734\n",
            "Epoch: 0, Step : 260, LR : 6.678607983623337e-06, Avg Loss : 0.8496\n",
            "Epoch: 0, Step : 270, LR : 6.934493346980553e-06, Avg Loss : 0.8819\n",
            "Epoch: 0, Step : 280, LR : 7.190378710337769e-06, Avg Loss : 0.7368\n",
            "Epoch: 0, Step : 290, LR : 7.446264073694985e-06, Avg Loss : 0.7690\n",
            "Epoch: 0, Step : 300, LR : 7.702149437052202e-06, Avg Loss : 0.6696\n",
            "Epoch: 0, Step : 310, LR : 7.958034800409417e-06, Avg Loss : 0.5952\n",
            "Epoch: 0, Step : 320, LR : 8.213920163766633e-06, Avg Loss : 0.7965\n",
            "Epoch: 0, Step : 330, LR : 8.469805527123848e-06, Avg Loss : 0.7852\n",
            "Epoch: 0, Step : 340, LR : 8.725690890481065e-06, Avg Loss : 0.6046\n",
            "Epoch: 0, Step : 350, LR : 8.981576253838281e-06, Avg Loss : 0.3907\n",
            "Epoch: 0, Step : 360, LR : 9.237461617195496e-06, Avg Loss : 0.9109\n",
            "Epoch: 0, Step : 370, LR : 9.493346980552713e-06, Avg Loss : 0.4023\n",
            "Epoch: 0, Step : 380, LR : 9.74923234390993e-06, Avg Loss : 1.0210\n",
            "Epoch: 0, Step : 390, LR : 1.0005117707267144e-05, Avg Loss : 0.4832\n",
            "Epoch: 0, Step : 400, LR : 1.026100307062436e-05, Avg Loss : 0.7538\n",
            "Epoch: 0, Step : 410, LR : 1.0516888433981576e-05, Avg Loss : 0.4041\n",
            "Epoch: 0, Step : 420, LR : 1.0772773797338794e-05, Avg Loss : 0.6336\n",
            "Epoch: 0, Step : 430, LR : 1.1028659160696009e-05, Avg Loss : 0.5907\n",
            "Epoch: 0, Step : 440, LR : 1.1284544524053225e-05, Avg Loss : 0.7489\n",
            "Epoch: 0, Step : 450, LR : 1.154042988741044e-05, Avg Loss : 0.4765\n",
            "Epoch: 0, Step : 460, LR : 1.1796315250767657e-05, Avg Loss : 0.5379\n",
            "Epoch: 0, Step : 470, LR : 1.2052200614124873e-05, Avg Loss : 0.5425\n",
            "Epoch: 0, Step : 480, LR : 1.230808597748209e-05, Avg Loss : 0.9137\n",
            "Epoch: 0, Step : 490, LR : 1.2563971340839305e-05, Avg Loss : 0.6262\n",
            "Epoch: 0, Step : 500, LR : 1.2819856704196521e-05, Avg Loss : 0.4267\n",
            "Epoch: 0, Step : 510, LR : 1.3075742067553736e-05, Avg Loss : 0.8021\n",
            "Epoch: 0, Step : 520, LR : 1.3331627430910951e-05, Avg Loss : 0.4319\n",
            "Epoch: 0, Step : 530, LR : 1.358751279426817e-05, Avg Loss : 0.5397\n",
            "Epoch: 0, Step : 540, LR : 1.3843398157625386e-05, Avg Loss : 0.6059\n",
            "Epoch: 0, Step : 550, LR : 1.40992835209826e-05, Avg Loss : 0.5012\n",
            "Epoch: 0, Step : 560, LR : 1.4355168884339816e-05, Avg Loss : 0.7687\n",
            "Epoch: 0, Step : 570, LR : 1.4611054247697032e-05, Avg Loss : 0.5289\n",
            "Epoch: 0, Step : 580, LR : 1.4866939611054247e-05, Avg Loss : 0.4012\n",
            "Epoch: 0, Step : 590, LR : 1.5122824974411465e-05, Avg Loss : 0.7415\n",
            "Epoch: 0, Step : 600, LR : 1.5378710337768682e-05, Avg Loss : 0.8975\n",
            "Epoch: 0, Step : 610, LR : 1.5634595701125895e-05, Avg Loss : 0.8855\n",
            "Epoch: 0, Step : 620, LR : 1.589048106448311e-05, Avg Loss : 1.0598\n",
            "Epoch: 0, Step : 630, LR : 1.6146366427840328e-05, Avg Loss : 1.0079\n",
            "Epoch: 0, Step : 640, LR : 1.6402251791197545e-05, Avg Loss : 0.5778\n",
            "Epoch: 0, Step : 650, LR : 1.665813715455476e-05, Avg Loss : 0.5896\n",
            "Epoch: 0, Step : 660, LR : 1.6914022517911978e-05, Avg Loss : 0.3936\n",
            "Epoch: 0, Step : 670, LR : 1.716990788126919e-05, Avg Loss : 0.5408\n",
            "Epoch: 0, Step : 680, LR : 1.7425793244626408e-05, Avg Loss : 0.5240\n",
            "Epoch: 0, Step : 690, LR : 1.7681678607983624e-05, Avg Loss : 0.4417\n",
            "Epoch: 0, Step : 700, LR : 1.793756397134084e-05, Avg Loss : 0.3272\n",
            "Epoch: 0, Step : 710, LR : 1.8193449334698057e-05, Avg Loss : 0.5716\n",
            "Epoch: 0, Step : 720, LR : 1.8449334698055274e-05, Avg Loss : 0.5051\n",
            "Epoch: 0, Step : 730, LR : 1.8705220061412487e-05, Avg Loss : 0.4522\n",
            "Epoch: 0, Step : 740, LR : 1.8961105424769704e-05, Avg Loss : 0.4478\n",
            "Epoch: 0, Step : 750, LR : 1.921699078812692e-05, Avg Loss : 0.5286\n",
            "Epoch: 0, Step : 760, LR : 1.9472876151484137e-05, Avg Loss : 0.6990\n",
            "Epoch: 0, Step : 770, LR : 1.9728761514841353e-05, Avg Loss : 0.4483\n",
            "Epoch: 0, Step : 780, LR : 1.9984646878198567e-05, Avg Loss : 0.4178\n",
            "Epoch: 0, Step : 790, LR : 1.9973274195382694e-05, Avg Loss : 0.6037\n",
            "Epoch: 0, Step : 800, LR : 1.9944842488343003e-05, Avg Loss : 0.4099\n",
            "Epoch: 0, Step : 810, LR : 1.991641078130331e-05, Avg Loss : 0.5570\n",
            "Epoch: 0, Step : 820, LR : 1.9887979074263623e-05, Avg Loss : 0.4090\n",
            "Epoch: 0, Step : 830, LR : 1.985954736722393e-05, Avg Loss : 0.4323\n",
            "Epoch: 0, Step : 840, LR : 1.983111566018424e-05, Avg Loss : 0.6577\n",
            "Epoch: 0, Step : 850, LR : 1.9802683953144552e-05, Avg Loss : 0.5318\n",
            "Epoch: 0, Step : 860, LR : 1.977425224610486e-05, Avg Loss : 0.4531\n",
            "Epoch: 0, Step : 870, LR : 1.974582053906517e-05, Avg Loss : 0.4122\n",
            "Epoch: 0, Step : 880, LR : 1.9717388832025478e-05, Avg Loss : 0.8135\n",
            "Epoch: 0, Step : 890, LR : 1.9688957124985786e-05, Avg Loss : 0.5233\n",
            "Epoch: 0, Step : 900, LR : 1.9660525417946098e-05, Avg Loss : 0.7478\n",
            "Epoch: 0, Step : 910, LR : 1.9632093710906406e-05, Avg Loss : 0.5268\n",
            "Epoch: 0, Step : 920, LR : 1.9603662003866715e-05, Avg Loss : 0.5553\n",
            "Epoch: 0, Step : 930, LR : 1.9575230296827023e-05, Avg Loss : 0.4596\n",
            "Epoch: 0, Step : 940, LR : 1.9546798589787332e-05, Avg Loss : 0.3127\n",
            "Epoch: 0, Step : 950, LR : 1.9518366882747644e-05, Avg Loss : 0.6074\n",
            "Epoch: 0, Step : 960, LR : 1.9489935175707952e-05, Avg Loss : 0.4447\n",
            "Epoch: 0, Step : 970, LR : 1.946150346866826e-05, Avg Loss : 0.6379\n",
            "Epoch: 0, Step : 980, LR : 1.943307176162857e-05, Avg Loss : 0.5883\n",
            "Epoch: 0, Step : 990, LR : 1.9404640054588878e-05, Avg Loss : 0.2137\n",
            "Epoch: 0, Step : 1000, LR : 1.937620834754919e-05, Avg Loss : 0.5511\n",
            "Epoch: 0, Step : 1010, LR : 1.93477766405095e-05, Avg Loss : 0.3016\n",
            "Epoch: 0, Step : 1020, LR : 1.9319344933469807e-05, Avg Loss : 0.6367\n",
            "Epoch: 0, Step : 1030, LR : 1.9290913226430115e-05, Avg Loss : 0.5336\n",
            "Epoch: 0, Step : 1040, LR : 1.9262481519390427e-05, Avg Loss : 0.3853\n",
            "Epoch: 0, Step : 1050, LR : 1.9234049812350736e-05, Avg Loss : 0.6017\n",
            "Epoch: 0, Step : 1060, LR : 1.9205618105311044e-05, Avg Loss : 0.5673\n",
            "Epoch: 0, Step : 1070, LR : 1.9177186398271356e-05, Avg Loss : 0.6517\n",
            "Epoch: 0, Step : 1080, LR : 1.9148754691231665e-05, Avg Loss : 0.5609\n",
            "Epoch: 0, Step : 1090, LR : 1.9120322984191973e-05, Avg Loss : 0.4186\n",
            "Epoch: 0, Step : 1100, LR : 1.9091891277152282e-05, Avg Loss : 0.5881\n",
            "Epoch: 0, Step : 1110, LR : 1.9063459570112594e-05, Avg Loss : 0.3784\n",
            "Epoch: 0, Step : 1120, LR : 1.9035027863072902e-05, Avg Loss : 0.5657\n",
            "Epoch: 0, Step : 1130, LR : 1.900659615603321e-05, Avg Loss : 0.5211\n",
            "Epoch: 0, Step : 1140, LR : 1.897816444899352e-05, Avg Loss : 0.3942\n",
            "Epoch: 0, Step : 1150, LR : 1.894973274195383e-05, Avg Loss : 0.6430\n",
            "Epoch: 0, Step : 1160, LR : 1.892130103491414e-05, Avg Loss : 0.4250\n",
            "Epoch: 0, Step : 1170, LR : 1.8892869327874448e-05, Avg Loss : 0.6289\n",
            "Epoch: 0, Step : 1180, LR : 1.8864437620834757e-05, Avg Loss : 0.5466\n",
            "Epoch: 0, Step : 1190, LR : 1.8836005913795065e-05, Avg Loss : 0.6015\n",
            "Epoch: 0, Step : 1200, LR : 1.8807574206755377e-05, Avg Loss : 0.6032\n",
            "Epoch: 0, Step : 1210, LR : 1.8779142499715686e-05, Avg Loss : 0.4652\n",
            "Epoch: 0, Step : 1220, LR : 1.8750710792675994e-05, Avg Loss : 0.5486\n",
            "Epoch: 0, Step : 1230, LR : 1.8722279085636303e-05, Avg Loss : 0.3923\n",
            "Epoch: 0, Step : 1240, LR : 1.869384737859661e-05, Avg Loss : 0.6359\n",
            "Epoch: 0, Step : 1250, LR : 1.8665415671556923e-05, Avg Loss : 0.2774\n",
            "Epoch: 0, Step : 1260, LR : 1.863698396451723e-05, Avg Loss : 0.3407\n",
            "Epoch: 0, Step : 1270, LR : 1.860855225747754e-05, Avg Loss : 0.3393\n",
            "Epoch: 0, Step : 1280, LR : 1.858012055043785e-05, Avg Loss : 0.4205\n",
            "Epoch: 0, Step : 1290, LR : 1.855168884339816e-05, Avg Loss : 0.3178\n",
            "Epoch: 0, Step : 1300, LR : 1.852325713635847e-05, Avg Loss : 0.4053\n",
            "Epoch: 0, Step : 1310, LR : 1.8494825429318778e-05, Avg Loss : 0.3697\n",
            "Epoch: 0, Step : 1320, LR : 1.846639372227909e-05, Avg Loss : 0.3983\n",
            "Epoch: 0, Step : 1330, LR : 1.8437962015239398e-05, Avg Loss : 0.5642\n",
            "Epoch: 0, Step : 1340, LR : 1.8409530308199707e-05, Avg Loss : 0.7443\n",
            "Epoch: 0, Step : 1350, LR : 1.8381098601160015e-05, Avg Loss : 0.5017\n",
            "Epoch: 0, Step : 1360, LR : 1.8352666894120327e-05, Avg Loss : 0.5601\n",
            "Epoch: 0, Step : 1370, LR : 1.8324235187080636e-05, Avg Loss : 0.3361\n",
            "Epoch: 0, Step : 1380, LR : 1.8295803480040944e-05, Avg Loss : 0.4844\n",
            "Epoch: 0, Step : 1390, LR : 1.8267371773001256e-05, Avg Loss : 0.3871\n",
            "Epoch: 0, Step : 1400, LR : 1.8238940065961564e-05, Avg Loss : 0.3962\n",
            "Epoch: 0, Step : 1410, LR : 1.8210508358921873e-05, Avg Loss : 0.5374\n",
            "Epoch: 0, Step : 1420, LR : 1.818207665188218e-05, Avg Loss : 0.4148\n",
            "Epoch: 0, Step : 1430, LR : 1.815364494484249e-05, Avg Loss : 0.2660\n",
            "Epoch: 0, Step : 1440, LR : 1.8125213237802802e-05, Avg Loss : 0.4910\n",
            "Epoch: 0, Step : 1450, LR : 1.809678153076311e-05, Avg Loss : 0.4088\n",
            "Epoch: 0, Step : 1460, LR : 1.806834982372342e-05, Avg Loss : 0.5210\n",
            "Epoch: 0, Step : 1470, LR : 1.8039918116683727e-05, Avg Loss : 0.4695\n",
            "Epoch: 0, Step : 1480, LR : 1.8011486409644036e-05, Avg Loss : 0.8638\n",
            "Epoch: 0, Step : 1490, LR : 1.7983054702604348e-05, Avg Loss : 0.3892\n",
            "Epoch: 0, Step : 1500, LR : 1.7954622995564656e-05, Avg Loss : 0.4424\n",
            "Epoch: 0, Step : 1510, LR : 1.7926191288524965e-05, Avg Loss : 0.2844\n",
            "Epoch: 0, Step : 1520, LR : 1.7897759581485273e-05, Avg Loss : 0.5603\n",
            "Epoch: 0, Step : 1530, LR : 1.7869327874445582e-05, Avg Loss : 0.6855\n",
            "Epoch: 0, Step : 1540, LR : 1.7840896167405894e-05, Avg Loss : 0.3421\n",
            "Epoch: 0, Step : 1550, LR : 1.7812464460366202e-05, Avg Loss : 0.3009\n",
            "Epoch: 0, Step : 1560, LR : 1.778403275332651e-05, Avg Loss : 0.4502\n",
            "Epoch: 0, Step : 1570, LR : 1.7755601046286823e-05, Avg Loss : 0.5739\n",
            "Epoch: 0, Step : 1580, LR : 1.772716933924713e-05, Avg Loss : 0.4015\n",
            "Epoch: 0, Step : 1590, LR : 1.769873763220744e-05, Avg Loss : 0.5881\n",
            "Epoch: 0, Step : 1600, LR : 1.767030592516775e-05, Avg Loss : 0.4144\n",
            "Epoch: 0, Step : 1610, LR : 1.764187421812806e-05, Avg Loss : 0.4064\n",
            "Epoch: 0, Step : 1620, LR : 1.761344251108837e-05, Avg Loss : 0.3887\n",
            "Epoch: 0, Step : 1630, LR : 1.7585010804048677e-05, Avg Loss : 0.4924\n",
            "Epoch: 0, Step : 1640, LR : 1.755657909700899e-05, Avg Loss : 0.4133\n",
            "Epoch: 0, Step : 1650, LR : 1.7528147389969298e-05, Avg Loss : 0.3362\n",
            "Epoch: 0, Step : 1660, LR : 1.7499715682929606e-05, Avg Loss : 0.3949\n",
            "Epoch: 0, Step : 1670, LR : 1.7471283975889915e-05, Avg Loss : 0.6172\n",
            "Epoch: 0, Step : 1680, LR : 1.7442852268850223e-05, Avg Loss : 0.3926\n",
            "Epoch: 0, Step : 1690, LR : 1.7414420561810535e-05, Avg Loss : 0.3562\n",
            "Epoch: 0, Step : 1700, LR : 1.7385988854770844e-05, Avg Loss : 0.4243\n",
            "Epoch: 0, Step : 1710, LR : 1.7357557147731152e-05, Avg Loss : 0.3602\n",
            "Epoch: 0, Step : 1720, LR : 1.732912544069146e-05, Avg Loss : 0.5038\n",
            "Epoch: 0, Step : 1730, LR : 1.730069373365177e-05, Avg Loss : 0.3426\n",
            "Epoch: 0, Step : 1740, LR : 1.727226202661208e-05, Avg Loss : 0.4262\n",
            "Epoch: 0, Step : 1750, LR : 1.724383031957239e-05, Avg Loss : 0.7046\n",
            "Epoch: 0, Step : 1760, LR : 1.7215398612532698e-05, Avg Loss : 0.6834\n",
            "Epoch: 0, Step : 1770, LR : 1.7186966905493007e-05, Avg Loss : 0.4032\n",
            "Epoch: 0, Step : 1780, LR : 1.7158535198453315e-05, Avg Loss : 0.6005\n",
            "Epoch: 0, Step : 1790, LR : 1.7130103491413627e-05, Avg Loss : 0.4847\n",
            "Epoch: 0, Step : 1800, LR : 1.7101671784373936e-05, Avg Loss : 0.2377\n",
            "Epoch: 0, Step : 1810, LR : 1.7073240077334244e-05, Avg Loss : 0.2874\n",
            "Epoch: 0, Step : 1820, LR : 1.7044808370294556e-05, Avg Loss : 0.5019\n",
            "Epoch: 0, Step : 1830, LR : 1.7016376663254865e-05, Avg Loss : 0.5149\n",
            "Epoch: 0, Step : 1840, LR : 1.6987944956215173e-05, Avg Loss : 0.3897\n",
            "Epoch: 0, Step : 1850, LR : 1.695951324917548e-05, Avg Loss : 0.3579\n",
            "Epoch: 0, Step : 1860, LR : 1.6931081542135794e-05, Avg Loss : 0.5885\n",
            "Epoch: 0, Step : 1870, LR : 1.6902649835096102e-05, Avg Loss : 0.3250\n",
            "Epoch: 0, Step : 1880, LR : 1.687421812805641e-05, Avg Loss : 0.4329\n",
            "Epoch: 0, Step : 1890, LR : 1.6845786421016722e-05, Avg Loss : 0.3314\n",
            "Epoch: 0, Step : 1900, LR : 1.681735471397703e-05, Avg Loss : 0.2547\n",
            "Epoch: 0, Step : 1910, LR : 1.678892300693734e-05, Avg Loss : 0.3762\n",
            "Epoch: 0, Step : 1920, LR : 1.6760491299897648e-05, Avg Loss : 0.4534\n",
            "Epoch: 0, Step : 1930, LR : 1.6732059592857957e-05, Avg Loss : 0.3397\n",
            "Epoch: 0, Step : 1940, LR : 1.670362788581827e-05, Avg Loss : 0.3424\n",
            "Epoch: 0, Step : 1950, LR : 1.6675196178778577e-05, Avg Loss : 0.3363\n",
            "Epoch 0 total_train_loss : 0.9409\n",
            "***** Finish To Train Epoch 0 *****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step : 10, valid Loss : 0.3575\n",
            "Step : 20, valid Loss : 0.2569\n",
            "Step : 30, valid Loss : 0.3974\n",
            "Step : 40, valid Loss : 0.5095\n",
            "Step : 50, valid Loss : 0.2852\n",
            "Step : 60, valid Loss : 0.3280\n",
            "Step : 70, valid Loss : 0.3819\n",
            "Step : 80, valid Loss : 0.4833\n",
            "Step : 90, valid Loss : 0.3636\n",
            "Step : 100, valid Loss : 0.3952\n",
            "total_valid_loss :  0.3761829958049529 val_f1_score :  91.92037470725994 val_pearsonr : 94.05713577741896\n",
            "Epoch 0 total_Valid Loss : 0.3762\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "Saving epoch 0 checkpoint at /content/drive/MyDrive/AI09/model_v3.ckpt.0\n",
            "****** Starting To Train Epoch #1 ******\n",
            "Epoch: 1, Step : 10, LR : 1.663539178892301e-05, Avg Loss : 0.2338\n",
            "Epoch: 1, Step : 20, LR : 1.6606960081883318e-05, Avg Loss : 0.3358\n",
            "Epoch: 1, Step : 30, LR : 1.6578528374843627e-05, Avg Loss : 0.2929\n",
            "Epoch: 1, Step : 40, LR : 1.6550096667803935e-05, Avg Loss : 0.4886\n",
            "Epoch: 1, Step : 50, LR : 1.6521664960764247e-05, Avg Loss : 0.3067\n",
            "Epoch: 1, Step : 60, LR : 1.6493233253724556e-05, Avg Loss : 0.3347\n",
            "Epoch: 1, Step : 70, LR : 1.6464801546684864e-05, Avg Loss : 0.2639\n",
            "Epoch: 1, Step : 80, LR : 1.6436369839645176e-05, Avg Loss : 0.2907\n",
            "Epoch: 1, Step : 90, LR : 1.6407938132605484e-05, Avg Loss : 0.2293\n",
            "Epoch: 1, Step : 100, LR : 1.6379506425565793e-05, Avg Loss : 0.1959\n",
            "Epoch: 1, Step : 110, LR : 1.63510747185261e-05, Avg Loss : 0.2111\n",
            "Epoch: 1, Step : 120, LR : 1.6322643011486413e-05, Avg Loss : 0.2691\n",
            "Epoch: 1, Step : 130, LR : 1.6294211304446722e-05, Avg Loss : 0.2151\n",
            "Epoch: 1, Step : 140, LR : 1.626577959740703e-05, Avg Loss : 0.1978\n",
            "Epoch: 1, Step : 150, LR : 1.623734789036734e-05, Avg Loss : 0.3620\n",
            "Epoch: 1, Step : 160, LR : 1.620891618332765e-05, Avg Loss : 0.1741\n",
            "Epoch: 1, Step : 170, LR : 1.618048447628796e-05, Avg Loss : 0.3368\n",
            "Epoch: 1, Step : 180, LR : 1.6152052769248268e-05, Avg Loss : 0.3790\n",
            "Epoch: 1, Step : 190, LR : 1.6123621062208576e-05, Avg Loss : 0.3039\n",
            "Epoch: 1, Step : 200, LR : 1.609518935516889e-05, Avg Loss : 0.3488\n",
            "Epoch: 1, Step : 210, LR : 1.6066757648129197e-05, Avg Loss : 0.2580\n",
            "Epoch: 1, Step : 220, LR : 1.6038325941089505e-05, Avg Loss : 0.1449\n",
            "Epoch: 1, Step : 230, LR : 1.6009894234049814e-05, Avg Loss : 0.3279\n",
            "Epoch: 1, Step : 240, LR : 1.5981462527010122e-05, Avg Loss : 0.5376\n",
            "Epoch: 1, Step : 250, LR : 1.5953030819970434e-05, Avg Loss : 0.2139\n",
            "Epoch: 1, Step : 260, LR : 1.5924599112930743e-05, Avg Loss : 0.2731\n",
            "Epoch: 1, Step : 270, LR : 1.589616740589105e-05, Avg Loss : 0.2820\n",
            "Epoch: 1, Step : 280, LR : 1.586773569885136e-05, Avg Loss : 0.2547\n",
            "Epoch: 1, Step : 290, LR : 1.583930399181167e-05, Avg Loss : 0.2241\n",
            "Epoch: 1, Step : 300, LR : 1.581087228477198e-05, Avg Loss : 0.2799\n",
            "Epoch: 1, Step : 310, LR : 1.578244057773229e-05, Avg Loss : 0.5373\n",
            "Epoch: 1, Step : 320, LR : 1.5754008870692597e-05, Avg Loss : 0.1878\n",
            "Epoch: 1, Step : 330, LR : 1.572557716365291e-05, Avg Loss : 0.3004\n",
            "Epoch: 1, Step : 340, LR : 1.5697145456613218e-05, Avg Loss : 0.3863\n",
            "Epoch: 1, Step : 350, LR : 1.5668713749573526e-05, Avg Loss : 0.2913\n",
            "Epoch: 1, Step : 360, LR : 1.5640282042533835e-05, Avg Loss : 0.2791\n",
            "Epoch: 1, Step : 370, LR : 1.5611850335494147e-05, Avg Loss : 0.2050\n",
            "Epoch: 1, Step : 380, LR : 1.5583418628454455e-05, Avg Loss : 0.3441\n",
            "Epoch: 1, Step : 390, LR : 1.5554986921414764e-05, Avg Loss : 0.2591\n",
            "Epoch: 1, Step : 400, LR : 1.5526555214375076e-05, Avg Loss : 0.2459\n",
            "Epoch: 1, Step : 410, LR : 1.5498123507335384e-05, Avg Loss : 0.4397\n",
            "Epoch: 1, Step : 420, LR : 1.5469691800295693e-05, Avg Loss : 0.2729\n",
            "Epoch: 1, Step : 430, LR : 1.5441260093256e-05, Avg Loss : 0.1797\n",
            "Epoch: 1, Step : 440, LR : 1.541282838621631e-05, Avg Loss : 0.3332\n",
            "Epoch: 1, Step : 450, LR : 1.538439667917662e-05, Avg Loss : 0.3454\n",
            "Epoch: 1, Step : 460, LR : 1.535596497213693e-05, Avg Loss : 0.2391\n",
            "Epoch: 1, Step : 470, LR : 1.532753326509724e-05, Avg Loss : 0.4071\n",
            "Epoch: 1, Step : 480, LR : 1.5299101558057547e-05, Avg Loss : 0.2960\n",
            "Epoch: 1, Step : 490, LR : 1.5270669851017856e-05, Avg Loss : 0.1901\n",
            "Epoch: 1, Step : 500, LR : 1.5242238143978168e-05, Avg Loss : 0.2555\n",
            "Epoch: 1, Step : 510, LR : 1.5213806436938476e-05, Avg Loss : 0.3633\n",
            "Epoch: 1, Step : 520, LR : 1.5185374729898785e-05, Avg Loss : 0.3444\n",
            "Epoch: 1, Step : 530, LR : 1.5156943022859095e-05, Avg Loss : 0.2181\n",
            "Epoch: 1, Step : 540, LR : 1.5128511315819403e-05, Avg Loss : 0.2345\n",
            "Epoch: 1, Step : 550, LR : 1.5100079608779714e-05, Avg Loss : 0.1611\n",
            "Epoch: 1, Step : 560, LR : 1.5071647901740024e-05, Avg Loss : 0.2283\n",
            "Epoch: 1, Step : 570, LR : 1.5043216194700332e-05, Avg Loss : 0.2613\n",
            "Epoch: 1, Step : 580, LR : 1.501478448766064e-05, Avg Loss : 0.2860\n",
            "Epoch: 1, Step : 590, LR : 1.498635278062095e-05, Avg Loss : 0.2568\n",
            "Epoch: 1, Step : 600, LR : 1.4957921073581261e-05, Avg Loss : 0.3744\n",
            "Epoch: 1, Step : 610, LR : 1.492948936654157e-05, Avg Loss : 0.1623\n",
            "Epoch: 1, Step : 620, LR : 1.4901057659501878e-05, Avg Loss : 0.4013\n",
            "Epoch: 1, Step : 630, LR : 1.4872625952462187e-05, Avg Loss : 0.2041\n",
            "Epoch: 1, Step : 640, LR : 1.4844194245422497e-05, Avg Loss : 0.4694\n",
            "Epoch: 1, Step : 650, LR : 1.4815762538382807e-05, Avg Loss : 0.2774\n",
            "Epoch: 1, Step : 660, LR : 1.4787330831343116e-05, Avg Loss : 0.3609\n",
            "Epoch: 1, Step : 670, LR : 1.4758899124303426e-05, Avg Loss : 0.2026\n",
            "Epoch: 1, Step : 680, LR : 1.4730467417263734e-05, Avg Loss : 0.3979\n",
            "Epoch: 1, Step : 690, LR : 1.4702035710224043e-05, Avg Loss : 0.3703\n",
            "Epoch: 1, Step : 700, LR : 1.4673604003184355e-05, Avg Loss : 0.2016\n",
            "Epoch: 1, Step : 710, LR : 1.4645172296144663e-05, Avg Loss : 0.2391\n",
            "Epoch: 1, Step : 720, LR : 1.4616740589104972e-05, Avg Loss : 0.2641\n",
            "Epoch: 1, Step : 730, LR : 1.458830888206528e-05, Avg Loss : 0.2150\n",
            "Epoch: 1, Step : 740, LR : 1.4559877175025589e-05, Avg Loss : 0.2578\n",
            "Epoch: 1, Step : 750, LR : 1.45314454679859e-05, Avg Loss : 0.3864\n",
            "Epoch: 1, Step : 760, LR : 1.450301376094621e-05, Avg Loss : 0.3050\n",
            "Epoch: 1, Step : 770, LR : 1.4474582053906518e-05, Avg Loss : 0.2257\n",
            "Epoch: 1, Step : 780, LR : 1.4446150346866828e-05, Avg Loss : 0.2786\n",
            "Epoch: 1, Step : 790, LR : 1.4417718639827137e-05, Avg Loss : 0.4022\n",
            "Epoch: 1, Step : 800, LR : 1.4389286932787447e-05, Avg Loss : 0.2474\n",
            "Epoch: 1, Step : 810, LR : 1.4360855225747757e-05, Avg Loss : 0.2247\n",
            "Epoch: 1, Step : 820, LR : 1.4332423518708066e-05, Avg Loss : 0.2415\n",
            "Epoch: 1, Step : 830, LR : 1.4303991811668374e-05, Avg Loss : 0.2222\n",
            "Epoch: 1, Step : 840, LR : 1.4275560104628683e-05, Avg Loss : 0.3422\n",
            "Epoch: 1, Step : 850, LR : 1.4247128397588994e-05, Avg Loss : 0.2049\n",
            "Epoch: 1, Step : 860, LR : 1.4218696690549303e-05, Avg Loss : 0.3133\n",
            "Epoch: 1, Step : 870, LR : 1.4190264983509611e-05, Avg Loss : 0.2303\n",
            "Epoch: 1, Step : 880, LR : 1.416183327646992e-05, Avg Loss : 0.1887\n",
            "Epoch: 1, Step : 890, LR : 1.413340156943023e-05, Avg Loss : 0.2453\n",
            "Epoch: 1, Step : 900, LR : 1.410496986239054e-05, Avg Loss : 0.2000\n",
            "Epoch: 1, Step : 910, LR : 1.4076538155350849e-05, Avg Loss : 0.3325\n",
            "Epoch: 1, Step : 920, LR : 1.4048106448311159e-05, Avg Loss : 0.2071\n",
            "Epoch: 1, Step : 930, LR : 1.4019674741271468e-05, Avg Loss : 0.2112\n",
            "Epoch: 1, Step : 940, LR : 1.3991243034231776e-05, Avg Loss : 0.3178\n",
            "Epoch: 1, Step : 950, LR : 1.3962811327192088e-05, Avg Loss : 0.3268\n",
            "Epoch: 1, Step : 960, LR : 1.3934379620152397e-05, Avg Loss : 0.3810\n",
            "Epoch: 1, Step : 970, LR : 1.3905947913112705e-05, Avg Loss : 0.2504\n",
            "Epoch: 1, Step : 980, LR : 1.3877516206073014e-05, Avg Loss : 0.2707\n",
            "Epoch: 1, Step : 990, LR : 1.3849084499033322e-05, Avg Loss : 0.2685\n",
            "Epoch: 1, Step : 1000, LR : 1.3820652791993634e-05, Avg Loss : 0.2456\n",
            "Epoch: 1, Step : 1010, LR : 1.3792221084953943e-05, Avg Loss : 0.3048\n",
            "Epoch: 1, Step : 1020, LR : 1.3763789377914251e-05, Avg Loss : 0.2614\n",
            "Epoch: 1, Step : 1030, LR : 1.3735357670874561e-05, Avg Loss : 0.3538\n",
            "Epoch: 1, Step : 1040, LR : 1.370692596383487e-05, Avg Loss : 0.1915\n",
            "Epoch: 1, Step : 1050, LR : 1.367849425679518e-05, Avg Loss : 0.3478\n",
            "Epoch: 1, Step : 1060, LR : 1.365006254975549e-05, Avg Loss : 0.3176\n",
            "Epoch: 1, Step : 1070, LR : 1.3621630842715799e-05, Avg Loss : 0.2705\n",
            "Epoch: 1, Step : 1080, LR : 1.3593199135676107e-05, Avg Loss : 0.3726\n",
            "Epoch: 1, Step : 1090, LR : 1.3564767428636416e-05, Avg Loss : 0.3219\n",
            "Epoch: 1, Step : 1100, LR : 1.3536335721596728e-05, Avg Loss : 0.2234\n",
            "Epoch: 1, Step : 1110, LR : 1.3507904014557036e-05, Avg Loss : 0.2307\n",
            "Epoch: 1, Step : 1120, LR : 1.3479472307517345e-05, Avg Loss : 0.2902\n",
            "Epoch: 1, Step : 1130, LR : 1.3451040600477653e-05, Avg Loss : 0.2869\n",
            "Epoch: 1, Step : 1140, LR : 1.3422608893437963e-05, Avg Loss : 0.2924\n",
            "Epoch: 1, Step : 1150, LR : 1.3394177186398274e-05, Avg Loss : 0.1900\n",
            "Epoch: 1, Step : 1160, LR : 1.3365745479358582e-05, Avg Loss : 0.3155\n",
            "Epoch: 1, Step : 1170, LR : 1.3337313772318892e-05, Avg Loss : 0.1688\n",
            "Epoch: 1, Step : 1180, LR : 1.3308882065279201e-05, Avg Loss : 0.2077\n",
            "Epoch: 1, Step : 1190, LR : 1.328045035823951e-05, Avg Loss : 0.2640\n",
            "Epoch: 1, Step : 1200, LR : 1.3252018651199821e-05, Avg Loss : 0.2683\n",
            "Epoch: 1, Step : 1210, LR : 1.322358694416013e-05, Avg Loss : 0.2006\n",
            "Epoch: 1, Step : 1220, LR : 1.3195155237120438e-05, Avg Loss : 0.2533\n",
            "Epoch: 1, Step : 1230, LR : 1.3166723530080747e-05, Avg Loss : 0.3621\n",
            "Epoch: 1, Step : 1240, LR : 1.3138291823041055e-05, Avg Loss : 0.2812\n",
            "Epoch: 1, Step : 1250, LR : 1.3109860116001367e-05, Avg Loss : 0.2466\n",
            "Epoch: 1, Step : 1260, LR : 1.3081428408961676e-05, Avg Loss : 0.2935\n",
            "Epoch: 1, Step : 1270, LR : 1.3052996701921984e-05, Avg Loss : 0.2958\n",
            "Epoch: 1, Step : 1280, LR : 1.3024564994882295e-05, Avg Loss : 0.3180\n",
            "Epoch: 1, Step : 1290, LR : 1.2996133287842603e-05, Avg Loss : 0.2711\n",
            "Epoch: 1, Step : 1300, LR : 1.2967701580802913e-05, Avg Loss : 0.4447\n",
            "Epoch: 1, Step : 1310, LR : 1.2939269873763224e-05, Avg Loss : 0.3319\n",
            "Epoch: 1, Step : 1320, LR : 1.2910838166723532e-05, Avg Loss : 0.5277\n",
            "Epoch: 1, Step : 1330, LR : 1.288240645968384e-05, Avg Loss : 0.3417\n",
            "Epoch: 1, Step : 1340, LR : 1.2853974752644149e-05, Avg Loss : 0.3201\n",
            "Epoch: 1, Step : 1350, LR : 1.2825543045604461e-05, Avg Loss : 0.3661\n",
            "Epoch: 1, Step : 1360, LR : 1.279711133856477e-05, Avg Loss : 0.2732\n",
            "Epoch: 1, Step : 1370, LR : 1.2768679631525078e-05, Avg Loss : 0.3000\n",
            "Epoch: 1, Step : 1380, LR : 1.2740247924485387e-05, Avg Loss : 0.1662\n",
            "Epoch: 1, Step : 1390, LR : 1.2711816217445697e-05, Avg Loss : 0.2092\n",
            "Epoch: 1, Step : 1400, LR : 1.2683384510406007e-05, Avg Loss : 0.2340\n",
            "Epoch: 1, Step : 1410, LR : 1.2654952803366315e-05, Avg Loss : 0.2680\n",
            "Epoch: 1, Step : 1420, LR : 1.2626521096326626e-05, Avg Loss : 0.3868\n",
            "Epoch: 1, Step : 1430, LR : 1.2598089389286934e-05, Avg Loss : 0.3120\n",
            "Epoch: 1, Step : 1440, LR : 1.2569657682247243e-05, Avg Loss : 0.2827\n",
            "Epoch: 1, Step : 1450, LR : 1.2541225975207555e-05, Avg Loss : 0.3223\n",
            "Epoch: 1, Step : 1460, LR : 1.2512794268167863e-05, Avg Loss : 0.2541\n",
            "Epoch: 1, Step : 1470, LR : 1.2484362561128172e-05, Avg Loss : 0.2481\n",
            "Epoch: 1, Step : 1480, LR : 1.245593085408848e-05, Avg Loss : 0.3195\n",
            "Epoch: 1, Step : 1490, LR : 1.2427499147048792e-05, Avg Loss : 0.2045\n",
            "Epoch: 1, Step : 1500, LR : 1.23990674400091e-05, Avg Loss : 0.2368\n",
            "Epoch: 1, Step : 1510, LR : 1.2370635732969409e-05, Avg Loss : 0.2366\n",
            "Epoch: 1, Step : 1520, LR : 1.2342204025929718e-05, Avg Loss : 0.2531\n",
            "Epoch: 1, Step : 1530, LR : 1.2313772318890028e-05, Avg Loss : 0.2360\n",
            "Epoch: 1, Step : 1540, LR : 1.2285340611850338e-05, Avg Loss : 0.4049\n",
            "Epoch: 1, Step : 1550, LR : 1.2256908904810647e-05, Avg Loss : 0.2248\n",
            "Epoch: 1, Step : 1560, LR : 1.2228477197770957e-05, Avg Loss : 0.2876\n",
            "Epoch: 1, Step : 1570, LR : 1.2200045490731265e-05, Avg Loss : 0.3258\n",
            "Epoch: 1, Step : 1580, LR : 1.2171613783691574e-05, Avg Loss : 0.2889\n",
            "Epoch: 1, Step : 1590, LR : 1.2143182076651886e-05, Avg Loss : 0.3398\n",
            "Epoch: 1, Step : 1600, LR : 1.2114750369612194e-05, Avg Loss : 0.2688\n",
            "Epoch: 1, Step : 1610, LR : 1.2086318662572503e-05, Avg Loss : 0.3125\n",
            "Epoch: 1, Step : 1620, LR : 1.2057886955532811e-05, Avg Loss : 0.3438\n",
            "Epoch: 1, Step : 1630, LR : 1.202945524849312e-05, Avg Loss : 0.3572\n",
            "Epoch: 1, Step : 1640, LR : 1.2001023541453432e-05, Avg Loss : 0.2760\n",
            "Epoch: 1, Step : 1650, LR : 1.197259183441374e-05, Avg Loss : 0.2831\n",
            "Epoch: 1, Step : 1660, LR : 1.1944160127374049e-05, Avg Loss : 0.2809\n",
            "Epoch: 1, Step : 1670, LR : 1.1915728420334359e-05, Avg Loss : 0.2578\n",
            "Epoch: 1, Step : 1680, LR : 1.1887296713294667e-05, Avg Loss : 0.2474\n",
            "Epoch: 1, Step : 1690, LR : 1.1858865006254978e-05, Avg Loss : 0.3106\n",
            "Epoch: 1, Step : 1700, LR : 1.1830433299215288e-05, Avg Loss : 0.2322\n",
            "Epoch: 1, Step : 1710, LR : 1.1802001592175596e-05, Avg Loss : 0.3198\n",
            "Epoch: 1, Step : 1720, LR : 1.1773569885135905e-05, Avg Loss : 0.2275\n",
            "Epoch: 1, Step : 1730, LR : 1.1745138178096213e-05, Avg Loss : 0.2995\n",
            "Epoch: 1, Step : 1740, LR : 1.1716706471056525e-05, Avg Loss : 0.2443\n",
            "Epoch: 1, Step : 1750, LR : 1.1688274764016834e-05, Avg Loss : 0.3098\n",
            "Epoch: 1, Step : 1760, LR : 1.1659843056977142e-05, Avg Loss : 0.2567\n",
            "Epoch: 1, Step : 1770, LR : 1.1631411349937451e-05, Avg Loss : 0.1981\n",
            "Epoch: 1, Step : 1780, LR : 1.1602979642897761e-05, Avg Loss : 0.2172\n",
            "Epoch: 1, Step : 1790, LR : 1.1574547935858071e-05, Avg Loss : 0.1948\n",
            "Epoch: 1, Step : 1800, LR : 1.154611622881838e-05, Avg Loss : 0.2367\n",
            "Epoch: 1, Step : 1810, LR : 1.151768452177869e-05, Avg Loss : 0.3171\n",
            "Epoch: 1, Step : 1820, LR : 1.1489252814738999e-05, Avg Loss : 0.1829\n",
            "Epoch: 1, Step : 1830, LR : 1.1460821107699307e-05, Avg Loss : 0.2466\n",
            "Epoch: 1, Step : 1840, LR : 1.1432389400659617e-05, Avg Loss : 0.1545\n",
            "Epoch: 1, Step : 1850, LR : 1.1403957693619927e-05, Avg Loss : 0.3606\n",
            "Epoch: 1, Step : 1860, LR : 1.1375525986580236e-05, Avg Loss : 0.2199\n",
            "Epoch: 1, Step : 1870, LR : 1.1347094279540545e-05, Avg Loss : 0.2384\n",
            "Epoch: 1, Step : 1880, LR : 1.1318662572500853e-05, Avg Loss : 0.2745\n",
            "Epoch: 1, Step : 1890, LR : 1.1290230865461165e-05, Avg Loss : 0.4135\n",
            "Epoch: 1, Step : 1900, LR : 1.1261799158421473e-05, Avg Loss : 0.1928\n",
            "Epoch: 1, Step : 1910, LR : 1.1233367451381782e-05, Avg Loss : 0.1988\n",
            "Epoch: 1, Step : 1920, LR : 1.1204935744342092e-05, Avg Loss : 0.2455\n",
            "Epoch: 1, Step : 1930, LR : 1.11765040373024e-05, Avg Loss : 0.2541\n",
            "Epoch: 1, Step : 1940, LR : 1.1148072330262711e-05, Avg Loss : 0.1889\n",
            "Epoch: 1, Step : 1950, LR : 1.1119640623223021e-05, Avg Loss : 0.3375\n",
            "Epoch 1 total_train_loss : 0.2812\n",
            "***** Finish To Train Epoch 1 *****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step : 10, valid Loss : 0.2571\n",
            "Step : 20, valid Loss : 0.2191\n",
            "Step : 30, valid Loss : 0.3019\n",
            "Step : 40, valid Loss : 0.3733\n",
            "Step : 50, valid Loss : 0.2028\n",
            "Step : 60, valid Loss : 0.2793\n",
            "Step : 70, valid Loss : 0.3075\n",
            "Step : 80, valid Loss : 0.3816\n",
            "Step : 90, valid Loss : 0.2724\n",
            "Step : 100, valid Loss : 0.3030\n",
            "total_valid_loss :  0.2897606731281368 val_f1_score :  92.26225634967513 val_pearsonr : 94.93455928874963\n",
            "Epoch 1 total_Valid Loss : 0.2898\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "Saving epoch 1 checkpoint at /content/drive/MyDrive/AI09/model_v3.ckpt.1\n",
            "****** Starting To Train Epoch #2 ******\n",
            "Epoch: 2, Step : 10, LR : 1.1079836233367454e-05, Avg Loss : 0.2263\n",
            "Epoch: 2, Step : 20, LR : 1.1051404526327762e-05, Avg Loss : 0.1398\n",
            "Epoch: 2, Step : 30, LR : 1.102297281928807e-05, Avg Loss : 0.1617\n",
            "Epoch: 2, Step : 40, LR : 1.0994541112248381e-05, Avg Loss : 0.1474\n",
            "Epoch: 2, Step : 50, LR : 1.096610940520869e-05, Avg Loss : 0.2022\n",
            "Epoch: 2, Step : 60, LR : 1.0937677698169e-05, Avg Loss : 0.1478\n",
            "Epoch: 2, Step : 70, LR : 1.090924599112931e-05, Avg Loss : 0.2056\n",
            "Epoch: 2, Step : 80, LR : 1.0880814284089618e-05, Avg Loss : 0.1417\n",
            "Epoch: 2, Step : 90, LR : 1.0852382577049927e-05, Avg Loss : 0.1533\n",
            "Epoch: 2, Step : 100, LR : 1.0823950870010235e-05, Avg Loss : 0.2135\n",
            "Epoch: 2, Step : 110, LR : 1.0795519162970547e-05, Avg Loss : 0.1373\n",
            "Epoch: 2, Step : 120, LR : 1.0767087455930856e-05, Avg Loss : 0.1357\n",
            "Epoch: 2, Step : 130, LR : 1.0738655748891164e-05, Avg Loss : 0.2377\n",
            "Epoch: 2, Step : 140, LR : 1.0710224041851473e-05, Avg Loss : 0.1969\n",
            "Epoch: 2, Step : 150, LR : 1.0681792334811783e-05, Avg Loss : 0.1815\n",
            "Epoch: 2, Step : 160, LR : 1.0653360627772093e-05, Avg Loss : 0.2629\n",
            "Epoch: 2, Step : 170, LR : 1.0624928920732402e-05, Avg Loss : 0.1669\n",
            "Epoch: 2, Step : 180, LR : 1.0596497213692712e-05, Avg Loss : 0.1693\n",
            "Epoch: 2, Step : 190, LR : 1.056806550665302e-05, Avg Loss : 0.1997\n",
            "Epoch: 2, Step : 200, LR : 1.0539633799613329e-05, Avg Loss : 0.1425\n",
            "Epoch: 2, Step : 210, LR : 1.0511202092573641e-05, Avg Loss : 0.1590\n",
            "Epoch: 2, Step : 220, LR : 1.048277038553395e-05, Avg Loss : 0.1287\n",
            "Epoch: 2, Step : 230, LR : 1.0454338678494258e-05, Avg Loss : 0.1148\n",
            "Epoch: 2, Step : 240, LR : 1.0425906971454567e-05, Avg Loss : 0.1545\n",
            "Epoch: 2, Step : 250, LR : 1.0397475264414875e-05, Avg Loss : 0.0997\n",
            "Epoch: 2, Step : 260, LR : 1.0369043557375187e-05, Avg Loss : 0.1770\n",
            "Epoch: 2, Step : 270, LR : 1.0340611850335495e-05, Avg Loss : 0.1176\n",
            "Epoch: 2, Step : 280, LR : 1.0312180143295804e-05, Avg Loss : 0.1712\n",
            "Epoch: 2, Step : 290, LR : 1.0283748436256114e-05, Avg Loss : 0.2129\n",
            "Epoch: 2, Step : 300, LR : 1.0255316729216424e-05, Avg Loss : 0.1308\n",
            "Epoch: 2, Step : 310, LR : 1.0226885022176733e-05, Avg Loss : 0.1681\n",
            "Epoch: 2, Step : 320, LR : 1.0198453315137043e-05, Avg Loss : 0.2164\n",
            "Epoch: 2, Step : 330, LR : 1.0170021608097352e-05, Avg Loss : 0.1369\n",
            "Epoch: 2, Step : 340, LR : 1.014158990105766e-05, Avg Loss : 0.1257\n",
            "Epoch: 2, Step : 350, LR : 1.0113158194017972e-05, Avg Loss : 0.2306\n",
            "Epoch: 2, Step : 360, LR : 1.008472648697828e-05, Avg Loss : 0.1589\n",
            "Epoch: 2, Step : 370, LR : 1.0056294779938589e-05, Avg Loss : 0.1505\n",
            "Epoch: 2, Step : 380, LR : 1.0027863072898898e-05, Avg Loss : 0.1728\n",
            "Epoch: 2, Step : 390, LR : 9.999431365859208e-06, Avg Loss : 0.1519\n",
            "Epoch: 2, Step : 400, LR : 9.970999658819516e-06, Avg Loss : 0.1522\n",
            "Epoch: 2, Step : 410, LR : 9.942567951779825e-06, Avg Loss : 0.1590\n",
            "Epoch: 2, Step : 420, LR : 9.914136244740135e-06, Avg Loss : 0.1022\n",
            "Epoch: 2, Step : 430, LR : 9.885704537700445e-06, Avg Loss : 0.1790\n",
            "Epoch: 2, Step : 440, LR : 9.857272830660754e-06, Avg Loss : 0.1573\n",
            "Epoch: 2, Step : 450, LR : 9.828841123621064e-06, Avg Loss : 0.1561\n",
            "Epoch: 2, Step : 460, LR : 9.800409416581373e-06, Avg Loss : 0.1986\n",
            "Epoch: 2, Step : 470, LR : 9.771977709541683e-06, Avg Loss : 0.1330\n",
            "Epoch: 2, Step : 480, LR : 9.743546002501991e-06, Avg Loss : 0.1012\n",
            "Epoch: 2, Step : 490, LR : 9.715114295462302e-06, Avg Loss : 0.1515\n",
            "Epoch: 2, Step : 500, LR : 9.68668258842261e-06, Avg Loss : 0.2483\n",
            "Epoch: 2, Step : 510, LR : 9.658250881382919e-06, Avg Loss : 0.2185\n",
            "Epoch: 2, Step : 520, LR : 9.629819174343229e-06, Avg Loss : 0.2018\n",
            "Epoch: 2, Step : 530, LR : 9.601387467303537e-06, Avg Loss : 0.2035\n",
            "Epoch: 2, Step : 540, LR : 9.572955760263847e-06, Avg Loss : 0.1152\n",
            "Epoch: 2, Step : 550, LR : 9.544524053224156e-06, Avg Loss : 0.1150\n",
            "Epoch: 2, Step : 560, LR : 9.516092346184466e-06, Avg Loss : 0.1725\n",
            "Epoch: 2, Step : 570, LR : 9.487660639144776e-06, Avg Loss : 0.1475\n",
            "Epoch: 2, Step : 580, LR : 9.459228932105085e-06, Avg Loss : 0.1525\n",
            "Epoch: 2, Step : 590, LR : 9.430797225065395e-06, Avg Loss : 0.1851\n",
            "Epoch: 2, Step : 600, LR : 9.402365518025704e-06, Avg Loss : 0.2661\n",
            "Epoch: 2, Step : 610, LR : 9.373933810986014e-06, Avg Loss : 0.1803\n",
            "Epoch: 2, Step : 620, LR : 9.345502103946322e-06, Avg Loss : 0.1823\n",
            "Epoch: 2, Step : 630, LR : 9.317070396906631e-06, Avg Loss : 0.1997\n",
            "Epoch: 2, Step : 640, LR : 9.288638689866941e-06, Avg Loss : 0.1372\n",
            "Epoch: 2, Step : 650, LR : 9.26020698282725e-06, Avg Loss : 0.0956\n",
            "Epoch: 2, Step : 660, LR : 9.23177527578756e-06, Avg Loss : 0.2447\n",
            "Epoch: 2, Step : 670, LR : 9.203343568747868e-06, Avg Loss : 0.1159\n",
            "Epoch: 2, Step : 680, LR : 9.174911861708179e-06, Avg Loss : 0.3046\n",
            "Epoch: 2, Step : 690, LR : 9.146480154668487e-06, Avg Loss : 0.1863\n",
            "Epoch: 2, Step : 700, LR : 9.118048447628797e-06, Avg Loss : 0.2096\n",
            "Epoch: 2, Step : 710, LR : 9.089616740589108e-06, Avg Loss : 0.1503\n",
            "Epoch: 2, Step : 720, LR : 9.061185033549416e-06, Avg Loss : 0.1461\n",
            "Epoch: 2, Step : 730, LR : 9.032753326509725e-06, Avg Loss : 0.2024\n",
            "Epoch: 2, Step : 740, LR : 9.004321619470035e-06, Avg Loss : 0.1866\n",
            "Epoch: 2, Step : 750, LR : 8.975889912430343e-06, Avg Loss : 0.2058\n",
            "Epoch: 2, Step : 760, LR : 8.947458205390653e-06, Avg Loss : 0.1555\n",
            "Epoch: 2, Step : 770, LR : 8.919026498350962e-06, Avg Loss : 0.1372\n",
            "Epoch: 2, Step : 780, LR : 8.89059479131127e-06, Avg Loss : 0.1622\n",
            "Epoch: 2, Step : 790, LR : 8.86216308427158e-06, Avg Loss : 0.2127\n",
            "Epoch: 2, Step : 800, LR : 8.83373137723189e-06, Avg Loss : 0.2159\n",
            "Epoch: 2, Step : 810, LR : 8.8052996701922e-06, Avg Loss : 0.2152\n",
            "Epoch: 2, Step : 820, LR : 8.77686796315251e-06, Avg Loss : 0.1330\n",
            "Epoch: 2, Step : 830, LR : 8.748436256112818e-06, Avg Loss : 0.2334\n",
            "Epoch: 2, Step : 840, LR : 8.720004549073128e-06, Avg Loss : 0.1527\n",
            "Epoch: 2, Step : 850, LR : 8.691572842033437e-06, Avg Loss : 0.1321\n",
            "Epoch: 2, Step : 860, LR : 8.663141134993747e-06, Avg Loss : 0.1219\n",
            "Epoch: 2, Step : 870, LR : 8.634709427954056e-06, Avg Loss : 0.0907\n",
            "Epoch: 2, Step : 880, LR : 8.606277720914364e-06, Avg Loss : 0.1154\n",
            "Epoch: 2, Step : 890, LR : 8.577846013874674e-06, Avg Loss : 0.1567\n",
            "Epoch: 2, Step : 900, LR : 8.549414306834983e-06, Avg Loss : 0.1683\n",
            "Epoch: 2, Step : 910, LR : 8.520982599795293e-06, Avg Loss : 0.1715\n",
            "Epoch: 2, Step : 920, LR : 8.492550892755602e-06, Avg Loss : 0.1764\n",
            "Epoch: 2, Step : 930, LR : 8.464119185715912e-06, Avg Loss : 0.1427\n",
            "Epoch: 2, Step : 940, LR : 8.43568747867622e-06, Avg Loss : 0.1860\n",
            "Epoch: 2, Step : 950, LR : 8.40725577163653e-06, Avg Loss : 0.2259\n",
            "Epoch: 2, Step : 960, LR : 8.37882406459684e-06, Avg Loss : 0.1287\n",
            "Epoch: 2, Step : 970, LR : 8.35039235755715e-06, Avg Loss : 0.2269\n",
            "Epoch: 2, Step : 980, LR : 8.321960650517458e-06, Avg Loss : 0.2308\n",
            "Epoch: 2, Step : 990, LR : 8.293528943477768e-06, Avg Loss : 0.1667\n",
            "Epoch: 2, Step : 1000, LR : 8.265097236438077e-06, Avg Loss : 0.1733\n",
            "Epoch: 2, Step : 1010, LR : 8.236665529398387e-06, Avg Loss : 0.1442\n",
            "Epoch: 2, Step : 1020, LR : 8.208233822358695e-06, Avg Loss : 0.1903\n",
            "Epoch: 2, Step : 1030, LR : 8.179802115319004e-06, Avg Loss : 0.2159\n",
            "Epoch: 2, Step : 1040, LR : 8.151370408279314e-06, Avg Loss : 0.1565\n",
            "Epoch: 2, Step : 1050, LR : 8.122938701239623e-06, Avg Loss : 0.2211\n",
            "Epoch: 2, Step : 1060, LR : 8.094506994199933e-06, Avg Loss : 0.1482\n",
            "Epoch: 2, Step : 1070, LR : 8.066075287160243e-06, Avg Loss : 0.1384\n",
            "Epoch: 2, Step : 1080, LR : 8.037643580120551e-06, Avg Loss : 0.1531\n",
            "Epoch: 2, Step : 1090, LR : 8.009211873080862e-06, Avg Loss : 0.1141\n",
            "Epoch: 2, Step : 1100, LR : 7.98078016604117e-06, Avg Loss : 0.1477\n",
            "Epoch: 2, Step : 1110, LR : 7.95234845900148e-06, Avg Loss : 0.1146\n",
            "Epoch: 2, Step : 1120, LR : 7.923916751961789e-06, Avg Loss : 0.1715\n",
            "Epoch: 2, Step : 1130, LR : 7.895485044922097e-06, Avg Loss : 0.1990\n",
            "Epoch: 2, Step : 1140, LR : 7.867053337882408e-06, Avg Loss : 0.2203\n",
            "Epoch: 2, Step : 1150, LR : 7.838621630842716e-06, Avg Loss : 0.1739\n",
            "Epoch: 2, Step : 1160, LR : 7.810189923803026e-06, Avg Loss : 0.1132\n",
            "Epoch: 2, Step : 1170, LR : 7.781758216763335e-06, Avg Loss : 0.1749\n",
            "Epoch: 2, Step : 1180, LR : 7.753326509723645e-06, Avg Loss : 0.1742\n",
            "Epoch: 2, Step : 1190, LR : 7.724894802683954e-06, Avg Loss : 0.1688\n",
            "Epoch: 2, Step : 1200, LR : 7.696463095644264e-06, Avg Loss : 0.1181\n",
            "Epoch: 2, Step : 1210, LR : 7.668031388604572e-06, Avg Loss : 0.1109\n",
            "Epoch: 2, Step : 1220, LR : 7.639599681564883e-06, Avg Loss : 0.1280\n",
            "Epoch: 2, Step : 1230, LR : 7.611167974525191e-06, Avg Loss : 0.1129\n",
            "Epoch: 2, Step : 1240, LR : 7.582736267485501e-06, Avg Loss : 0.1966\n",
            "Epoch: 2, Step : 1250, LR : 7.55430456044581e-06, Avg Loss : 0.1493\n",
            "Epoch: 2, Step : 1260, LR : 7.52587285340612e-06, Avg Loss : 0.1921\n",
            "Epoch: 2, Step : 1270, LR : 7.4974411463664285e-06, Avg Loss : 0.1165\n",
            "Epoch: 2, Step : 1280, LR : 7.469009439326739e-06, Avg Loss : 0.1633\n",
            "Epoch: 2, Step : 1290, LR : 7.440577732287047e-06, Avg Loss : 0.1980\n",
            "Epoch: 2, Step : 1300, LR : 7.412146025247357e-06, Avg Loss : 0.1802\n",
            "Epoch: 2, Step : 1310, LR : 7.383714318207667e-06, Avg Loss : 0.1552\n",
            "Epoch: 2, Step : 1320, LR : 7.355282611167975e-06, Avg Loss : 0.1287\n",
            "Epoch: 2, Step : 1330, LR : 7.3268509041282856e-06, Avg Loss : 0.1308\n",
            "Epoch: 2, Step : 1340, LR : 7.298419197088594e-06, Avg Loss : 0.1317\n",
            "Epoch: 2, Step : 1350, LR : 7.2699874900489034e-06, Avg Loss : 0.2245\n",
            "Epoch: 2, Step : 1360, LR : 7.241555783009213e-06, Avg Loss : 0.1940\n",
            "Epoch: 2, Step : 1370, LR : 7.213124075969522e-06, Avg Loss : 0.3024\n",
            "Epoch: 2, Step : 1380, LR : 7.184692368929832e-06, Avg Loss : 0.2217\n",
            "Epoch: 2, Step : 1390, LR : 7.156260661890141e-06, Avg Loss : 0.1490\n",
            "Epoch: 2, Step : 1400, LR : 7.127828954850449e-06, Avg Loss : 0.1598\n",
            "Epoch: 2, Step : 1410, LR : 7.09939724781076e-06, Avg Loss : 0.2381\n",
            "Epoch: 2, Step : 1420, LR : 7.070965540771069e-06, Avg Loss : 0.1184\n",
            "Epoch: 2, Step : 1430, LR : 7.042533833731378e-06, Avg Loss : 0.0995\n",
            "Epoch: 2, Step : 1440, LR : 7.014102126691688e-06, Avg Loss : 0.1225\n",
            "Epoch: 2, Step : 1450, LR : 6.985670419651996e-06, Avg Loss : 0.1796\n",
            "Epoch: 2, Step : 1460, LR : 6.9572387126123065e-06, Avg Loss : 0.1200\n",
            "Epoch: 2, Step : 1470, LR : 6.928807005572615e-06, Avg Loss : 0.2137\n",
            "Epoch: 2, Step : 1480, LR : 6.900375298532925e-06, Avg Loss : 0.1631\n",
            "Epoch: 2, Step : 1490, LR : 6.8719435914932345e-06, Avg Loss : 0.1380\n",
            "Epoch: 2, Step : 1500, LR : 6.843511884453543e-06, Avg Loss : 0.1319\n",
            "Epoch: 2, Step : 1510, LR : 6.815080177413853e-06, Avg Loss : 0.1590\n",
            "Epoch: 2, Step : 1520, LR : 6.786648470374162e-06, Avg Loss : 0.1229\n",
            "Epoch: 2, Step : 1530, LR : 6.758216763334472e-06, Avg Loss : 0.1884\n",
            "Epoch: 2, Step : 1540, LR : 6.7297850562947805e-06, Avg Loss : 0.1175\n",
            "Epoch: 2, Step : 1550, LR : 6.70135334925509e-06, Avg Loss : 0.1635\n",
            "Epoch: 2, Step : 1560, LR : 6.6729216422154e-06, Avg Loss : 0.1314\n",
            "Epoch: 2, Step : 1570, LR : 6.644489935175709e-06, Avg Loss : 0.1401\n",
            "Epoch: 2, Step : 1580, LR : 6.616058228136019e-06, Avg Loss : 0.1124\n",
            "Epoch: 2, Step : 1590, LR : 6.587626521096327e-06, Avg Loss : 0.1430\n",
            "Epoch: 2, Step : 1600, LR : 6.559194814056637e-06, Avg Loss : 0.1612\n",
            "Epoch: 2, Step : 1610, LR : 6.530763107016946e-06, Avg Loss : 0.1497\n",
            "Epoch: 2, Step : 1620, LR : 6.5023313999772554e-06, Avg Loss : 0.1935\n",
            "Epoch: 2, Step : 1630, LR : 6.473899692937566e-06, Avg Loss : 0.2059\n",
            "Epoch: 2, Step : 1640, LR : 6.445467985897874e-06, Avg Loss : 0.1859\n",
            "Epoch: 2, Step : 1650, LR : 6.417036278858183e-06, Avg Loss : 0.1538\n",
            "Epoch: 2, Step : 1660, LR : 6.388604571818493e-06, Avg Loss : 0.1113\n",
            "Epoch: 2, Step : 1670, LR : 6.360172864778802e-06, Avg Loss : 0.1024\n",
            "Epoch: 2, Step : 1680, LR : 6.331741157739112e-06, Avg Loss : 0.1320\n",
            "Epoch: 2, Step : 1690, LR : 6.303309450699421e-06, Avg Loss : 0.1381\n",
            "Epoch: 2, Step : 1700, LR : 6.2748777436597295e-06, Avg Loss : 0.1961\n",
            "Epoch: 2, Step : 1710, LR : 6.24644603662004e-06, Avg Loss : 0.1231\n",
            "Epoch: 2, Step : 1720, LR : 6.218014329580348e-06, Avg Loss : 0.1467\n",
            "Epoch: 2, Step : 1730, LR : 6.1895826225406584e-06, Avg Loss : 0.1627\n",
            "Epoch: 2, Step : 1740, LR : 6.161150915500968e-06, Avg Loss : 0.1577\n",
            "Epoch: 2, Step : 1750, LR : 6.132719208461276e-06, Avg Loss : 0.1568\n",
            "Epoch: 2, Step : 1760, LR : 6.1042875014215865e-06, Avg Loss : 0.2021\n",
            "Epoch: 2, Step : 1770, LR : 6.075855794381895e-06, Avg Loss : 0.0945\n",
            "Epoch: 2, Step : 1780, LR : 6.047424087342205e-06, Avg Loss : 0.1354\n",
            "Epoch: 2, Step : 1790, LR : 6.018992380302514e-06, Avg Loss : 0.1909\n",
            "Epoch: 2, Step : 1800, LR : 5.990560673262823e-06, Avg Loss : 0.1319\n",
            "Epoch: 2, Step : 1810, LR : 5.962128966223133e-06, Avg Loss : 0.1417\n",
            "Epoch: 2, Step : 1820, LR : 5.933697259183442e-06, Avg Loss : 0.1648\n",
            "Epoch: 2, Step : 1830, LR : 5.905265552143752e-06, Avg Loss : 0.1036\n",
            "Epoch: 2, Step : 1840, LR : 5.876833845104061e-06, Avg Loss : 0.1588\n",
            "Epoch: 2, Step : 1850, LR : 5.84840213806437e-06, Avg Loss : 0.1168\n",
            "Epoch: 2, Step : 1860, LR : 5.819970431024679e-06, Avg Loss : 0.1391\n",
            "Epoch: 2, Step : 1870, LR : 5.791538723984989e-06, Avg Loss : 0.1724\n",
            "Epoch: 2, Step : 1880, LR : 5.763107016945299e-06, Avg Loss : 0.2557\n",
            "Epoch: 2, Step : 1890, LR : 5.734675309905607e-06, Avg Loss : 0.0855\n",
            "Epoch: 2, Step : 1900, LR : 5.706243602865918e-06, Avg Loss : 0.1491\n",
            "Epoch: 2, Step : 1910, LR : 5.677811895826226e-06, Avg Loss : 0.1296\n",
            "Epoch: 2, Step : 1920, LR : 5.6493801887865355e-06, Avg Loss : 0.1248\n",
            "Epoch: 2, Step : 1930, LR : 5.620948481746845e-06, Avg Loss : 0.1503\n",
            "Epoch: 2, Step : 1940, LR : 5.592516774707154e-06, Avg Loss : 0.1457\n",
            "Epoch: 2, Step : 1950, LR : 5.5640850676674645e-06, Avg Loss : 0.2149\n",
            "Epoch 2 total_train_loss : 0.1637\n",
            "***** Finish To Train Epoch 2 *****\n",
            "\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step : 10, valid Loss : 0.2618\n",
            "Step : 20, valid Loss : 0.2268\n",
            "Step : 30, valid Loss : 0.2935\n",
            "Step : 40, valid Loss : 0.3443\n",
            "Step : 50, valid Loss : 0.2138\n",
            "Step : 60, valid Loss : 0.2561\n",
            "Step : 70, valid Loss : 0.3202\n",
            "Step : 80, valid Loss : 0.3993\n",
            "Step : 90, valid Loss : 0.2622\n",
            "Step : 100, valid Loss : 0.2792\n",
            "total_valid_loss :  0.2854138529526258 val_f1_score :  93.20951828206617 val_pearsonr : 95.29737167076809\n",
            "Epoch 2 total_Valid Loss : 0.2854\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "Saving epoch 2 checkpoint at /content/drive/MyDrive/AI09/model_v3.ckpt.2\n",
            "****** Starting To Train Epoch #3 ******\n",
            "Epoch: 3, Step : 10, LR : 5.524280677811896e-06, Avg Loss : 0.0905\n",
            "Epoch: 3, Step : 20, LR : 5.495848970772206e-06, Avg Loss : 0.0756\n",
            "Epoch: 3, Step : 30, LR : 5.467417263732515e-06, Avg Loss : 0.1133\n",
            "Epoch: 3, Step : 40, LR : 5.438985556692824e-06, Avg Loss : 0.1175\n",
            "Epoch: 3, Step : 50, LR : 5.410553849653134e-06, Avg Loss : 0.1519\n",
            "Epoch: 3, Step : 60, LR : 5.382122142613443e-06, Avg Loss : 0.1418\n",
            "Epoch: 3, Step : 70, LR : 5.353690435573753e-06, Avg Loss : 0.1645\n",
            "Epoch: 3, Step : 80, LR : 5.325258728534062e-06, Avg Loss : 0.1046\n",
            "Epoch: 3, Step : 90, LR : 5.296827021494372e-06, Avg Loss : 0.0989\n",
            "Epoch: 3, Step : 100, LR : 5.2683953144546805e-06, Avg Loss : 0.0764\n",
            "Epoch: 3, Step : 110, LR : 5.23996360741499e-06, Avg Loss : 0.1187\n",
            "Epoch: 3, Step : 120, LR : 5.211531900375299e-06, Avg Loss : 0.1500\n",
            "Epoch: 3, Step : 130, LR : 5.183100193335609e-06, Avg Loss : 0.0735\n",
            "Epoch: 3, Step : 140, LR : 5.154668486295919e-06, Avg Loss : 0.1096\n",
            "Epoch: 3, Step : 150, LR : 5.126236779256227e-06, Avg Loss : 0.0775\n",
            "Epoch: 3, Step : 160, LR : 5.097805072216536e-06, Avg Loss : 0.1079\n",
            "Epoch: 3, Step : 170, LR : 5.069373365176846e-06, Avg Loss : 0.1286\n",
            "Epoch: 3, Step : 180, LR : 5.040941658137155e-06, Avg Loss : 0.0981\n",
            "Epoch: 3, Step : 190, LR : 5.012509951097465e-06, Avg Loss : 0.0999\n",
            "Epoch: 3, Step : 200, LR : 4.984078244057774e-06, Avg Loss : 0.0757\n",
            "Epoch: 3, Step : 210, LR : 4.9556465370180835e-06, Avg Loss : 0.1449\n",
            "Epoch: 3, Step : 220, LR : 4.927214829978392e-06, Avg Loss : 0.0861\n",
            "Epoch: 3, Step : 230, LR : 4.898783122938701e-06, Avg Loss : 0.0919\n",
            "Epoch: 3, Step : 240, LR : 4.870351415899012e-06, Avg Loss : 0.1715\n",
            "Epoch: 3, Step : 250, LR : 4.841919708859321e-06, Avg Loss : 0.0985\n",
            "Epoch: 3, Step : 260, LR : 4.81348800181963e-06, Avg Loss : 0.0903\n",
            "Epoch: 3, Step : 270, LR : 4.78505629477994e-06, Avg Loss : 0.1353\n",
            "Epoch: 3, Step : 280, LR : 4.756624587740248e-06, Avg Loss : 0.0638\n",
            "Epoch: 3, Step : 290, LR : 4.7281928807005576e-06, Avg Loss : 0.1031\n",
            "Epoch: 3, Step : 300, LR : 4.699761173660867e-06, Avg Loss : 0.0914\n",
            "Epoch: 3, Step : 310, LR : 4.671329466621176e-06, Avg Loss : 0.1210\n",
            "Epoch: 3, Step : 320, LR : 4.6428977595814865e-06, Avg Loss : 0.1122\n",
            "Epoch: 3, Step : 330, LR : 4.614466052541795e-06, Avg Loss : 0.1024\n",
            "Epoch: 3, Step : 340, LR : 4.586034345502104e-06, Avg Loss : 0.1402\n",
            "Epoch: 3, Step : 350, LR : 4.557602638462414e-06, Avg Loss : 0.1057\n",
            "Epoch: 3, Step : 360, LR : 4.529170931422723e-06, Avg Loss : 0.0921\n",
            "Epoch: 3, Step : 370, LR : 4.5007392243830325e-06, Avg Loss : 0.0956\n",
            "Epoch: 3, Step : 380, LR : 4.472307517343342e-06, Avg Loss : 0.1256\n",
            "Epoch: 3, Step : 390, LR : 4.443875810303651e-06, Avg Loss : 0.0800\n",
            "Epoch: 3, Step : 400, LR : 4.4154441032639606e-06, Avg Loss : 0.0926\n",
            "Epoch: 3, Step : 410, LR : 4.38701239622427e-06, Avg Loss : 0.0809\n",
            "Epoch: 3, Step : 420, LR : 4.358580689184579e-06, Avg Loss : 0.1063\n",
            "Epoch: 3, Step : 430, LR : 4.330148982144889e-06, Avg Loss : 0.1377\n",
            "Epoch: 3, Step : 440, LR : 4.301717275105198e-06, Avg Loss : 0.1451\n",
            "Epoch: 3, Step : 450, LR : 4.273285568065507e-06, Avg Loss : 0.0728\n",
            "Epoch: 3, Step : 460, LR : 4.244853861025817e-06, Avg Loss : 0.1054\n",
            "Epoch: 3, Step : 470, LR : 4.216422153986126e-06, Avg Loss : 0.1822\n",
            "Epoch: 3, Step : 480, LR : 4.187990446946435e-06, Avg Loss : 0.0861\n",
            "Epoch: 3, Step : 490, LR : 4.159558739906744e-06, Avg Loss : 0.0962\n",
            "Epoch: 3, Step : 500, LR : 4.131127032867054e-06, Avg Loss : 0.1280\n",
            "Epoch: 3, Step : 510, LR : 4.1026953258273636e-06, Avg Loss : 0.1218\n",
            "Epoch: 3, Step : 520, LR : 4.074263618787673e-06, Avg Loss : 0.0876\n",
            "Epoch: 3, Step : 530, LR : 4.0458319117479815e-06, Avg Loss : 0.1062\n",
            "Epoch: 3, Step : 540, LR : 4.017400204708291e-06, Avg Loss : 0.0770\n",
            "Epoch: 3, Step : 550, LR : 3.9889684976686e-06, Avg Loss : 0.0940\n",
            "Epoch: 3, Step : 560, LR : 3.9605367906289095e-06, Avg Loss : 0.2176\n",
            "Epoch: 3, Step : 570, LR : 3.93210508358922e-06, Avg Loss : 0.0894\n",
            "Epoch: 3, Step : 580, LR : 3.903673376549529e-06, Avg Loss : 0.0719\n",
            "Epoch: 3, Step : 590, LR : 3.875241669509838e-06, Avg Loss : 0.1184\n",
            "Epoch: 3, Step : 600, LR : 3.846809962470147e-06, Avg Loss : 0.0908\n",
            "Epoch: 3, Step : 610, LR : 3.818378255430456e-06, Avg Loss : 0.0994\n",
            "Epoch: 3, Step : 620, LR : 3.789946548390766e-06, Avg Loss : 0.1366\n",
            "Epoch: 3, Step : 630, LR : 3.7615148413510755e-06, Avg Loss : 0.1358\n",
            "Epoch: 3, Step : 640, LR : 3.7330831343113845e-06, Avg Loss : 0.1219\n",
            "Epoch: 3, Step : 650, LR : 3.704651427271694e-06, Avg Loss : 0.1473\n",
            "Epoch: 3, Step : 660, LR : 3.676219720232003e-06, Avg Loss : 0.0926\n",
            "Epoch: 3, Step : 670, LR : 3.6477880131923126e-06, Avg Loss : 0.1114\n",
            "Epoch: 3, Step : 680, LR : 3.619356306152622e-06, Avg Loss : 0.1290\n",
            "Epoch: 3, Step : 690, LR : 3.590924599112931e-06, Avg Loss : 0.0785\n",
            "Epoch: 3, Step : 700, LR : 3.5624928920732406e-06, Avg Loss : 0.0802\n",
            "Epoch: 3, Step : 710, LR : 3.53406118503355e-06, Avg Loss : 0.1524\n",
            "Epoch: 3, Step : 720, LR : 3.5056294779938594e-06, Avg Loss : 0.0679\n",
            "Epoch: 3, Step : 730, LR : 3.4771977709541687e-06, Avg Loss : 0.1062\n",
            "Epoch: 3, Step : 740, LR : 3.4487660639144777e-06, Avg Loss : 0.1097\n",
            "Epoch: 3, Step : 750, LR : 3.420334356874787e-06, Avg Loss : 0.0840\n",
            "Epoch: 3, Step : 760, LR : 3.3919026498350964e-06, Avg Loss : 0.1017\n",
            "Epoch: 3, Step : 770, LR : 3.3634709427954058e-06, Avg Loss : 0.0752\n",
            "Epoch: 3, Step : 780, LR : 3.3350392357557156e-06, Avg Loss : 0.0887\n",
            "Epoch: 3, Step : 790, LR : 3.3066075287160245e-06, Avg Loss : 0.0994\n",
            "Epoch: 3, Step : 800, LR : 3.278175821676334e-06, Avg Loss : 0.1251\n",
            "Epoch: 3, Step : 810, LR : 3.2497441146366432e-06, Avg Loss : 0.1041\n",
            "Epoch: 3, Step : 820, LR : 3.2213124075969526e-06, Avg Loss : 0.1179\n",
            "Epoch: 3, Step : 830, LR : 3.192880700557262e-06, Avg Loss : 0.0670\n",
            "Epoch: 3, Step : 840, LR : 3.164448993517571e-06, Avg Loss : 0.0778\n",
            "Epoch: 3, Step : 850, LR : 3.1360172864778803e-06, Avg Loss : 0.0976\n",
            "Epoch: 3, Step : 860, LR : 3.10758557943819e-06, Avg Loss : 0.1200\n",
            "Epoch: 3, Step : 870, LR : 3.0791538723984994e-06, Avg Loss : 0.0840\n",
            "Epoch: 3, Step : 880, LR : 3.0507221653588088e-06, Avg Loss : 0.1084\n",
            "Epoch: 3, Step : 890, LR : 3.022290458319118e-06, Avg Loss : 0.1236\n",
            "Epoch: 3, Step : 900, LR : 2.993858751279427e-06, Avg Loss : 0.1048\n",
            "Epoch: 3, Step : 910, LR : 2.9654270442397364e-06, Avg Loss : 0.0827\n",
            "Epoch: 3, Step : 920, LR : 2.936995337200046e-06, Avg Loss : 0.0957\n",
            "Epoch: 3, Step : 930, LR : 2.908563630160355e-06, Avg Loss : 0.1597\n",
            "Epoch: 3, Step : 940, LR : 2.880131923120665e-06, Avg Loss : 0.0588\n",
            "Epoch: 3, Step : 950, LR : 2.851700216080974e-06, Avg Loss : 0.0749\n",
            "Epoch: 3, Step : 960, LR : 2.8232685090412833e-06, Avg Loss : 0.0791\n",
            "Epoch: 3, Step : 970, LR : 2.7948368020015926e-06, Avg Loss : 0.0921\n",
            "Epoch: 3, Step : 980, LR : 2.766405094961902e-06, Avg Loss : 0.0611\n",
            "Epoch: 3, Step : 990, LR : 2.7379733879222114e-06, Avg Loss : 0.0950\n",
            "Epoch: 3, Step : 1000, LR : 2.7095416808825203e-06, Avg Loss : 0.1330\n",
            "Epoch: 3, Step : 1010, LR : 2.6811099738428297e-06, Avg Loss : 0.0741\n",
            "Epoch: 3, Step : 1020, LR : 2.652678266803139e-06, Avg Loss : 0.0958\n",
            "Epoch: 3, Step : 1030, LR : 2.624246559763449e-06, Avg Loss : 0.1111\n",
            "Epoch: 3, Step : 1040, LR : 2.595814852723758e-06, Avg Loss : 0.1136\n",
            "Epoch: 3, Step : 1050, LR : 2.567383145684067e-06, Avg Loss : 0.0765\n",
            "Epoch: 3, Step : 1060, LR : 2.5389514386443765e-06, Avg Loss : 0.0790\n",
            "Epoch: 3, Step : 1070, LR : 2.510519731604686e-06, Avg Loss : 0.1110\n",
            "Epoch: 3, Step : 1080, LR : 2.4820880245649952e-06, Avg Loss : 0.0969\n",
            "Epoch: 3, Step : 1090, LR : 2.4536563175253046e-06, Avg Loss : 0.0967\n",
            "Epoch: 3, Step : 1100, LR : 2.425224610485614e-06, Avg Loss : 0.1148\n",
            "Epoch: 3, Step : 1110, LR : 2.396792903445923e-06, Avg Loss : 0.1163\n",
            "Epoch: 3, Step : 1120, LR : 2.3683611964062327e-06, Avg Loss : 0.0954\n",
            "Epoch: 3, Step : 1130, LR : 2.339929489366542e-06, Avg Loss : 0.0776\n",
            "Epoch: 3, Step : 1140, LR : 2.311497782326851e-06, Avg Loss : 0.1408\n",
            "Epoch: 3, Step : 1150, LR : 2.2830660752871608e-06, Avg Loss : 0.1170\n",
            "Epoch: 3, Step : 1160, LR : 2.25463436824747e-06, Avg Loss : 0.0818\n",
            "Epoch: 3, Step : 1170, LR : 2.226202661207779e-06, Avg Loss : 0.1035\n",
            "Epoch: 3, Step : 1180, LR : 2.1977709541680884e-06, Avg Loss : 0.0770\n",
            "Epoch: 3, Step : 1190, LR : 2.169339247128398e-06, Avg Loss : 0.1032\n",
            "Epoch: 3, Step : 1200, LR : 2.140907540088707e-06, Avg Loss : 0.1196\n",
            "Epoch: 3, Step : 1210, LR : 2.1124758330490165e-06, Avg Loss : 0.1011\n",
            "Epoch: 3, Step : 1220, LR : 2.084044126009326e-06, Avg Loss : 0.0815\n",
            "Epoch: 3, Step : 1230, LR : 2.0556124189696353e-06, Avg Loss : 0.0883\n",
            "Epoch: 3, Step : 1240, LR : 2.0271807119299446e-06, Avg Loss : 0.1728\n",
            "Epoch: 3, Step : 1250, LR : 1.998749004890254e-06, Avg Loss : 0.0756\n",
            "Epoch: 3, Step : 1260, LR : 1.9703172978505633e-06, Avg Loss : 0.0837\n",
            "Epoch: 3, Step : 1270, LR : 1.9418855908108723e-06, Avg Loss : 0.0780\n",
            "Epoch: 3, Step : 1280, LR : 1.913453883771182e-06, Avg Loss : 0.1068\n",
            "Epoch: 3, Step : 1290, LR : 1.8850221767314914e-06, Avg Loss : 0.1379\n",
            "Epoch: 3, Step : 1300, LR : 1.8565904696918006e-06, Avg Loss : 0.1225\n",
            "Epoch: 3, Step : 1310, LR : 1.82815876265211e-06, Avg Loss : 0.0722\n",
            "Epoch: 3, Step : 1320, LR : 1.7997270556124191e-06, Avg Loss : 0.1269\n",
            "Epoch: 3, Step : 1330, LR : 1.7712953485727285e-06, Avg Loss : 0.1137\n",
            "Epoch: 3, Step : 1340, LR : 1.742863641533038e-06, Avg Loss : 0.0961\n",
            "Epoch: 3, Step : 1350, LR : 1.7144319344933472e-06, Avg Loss : 0.1078\n",
            "Epoch: 3, Step : 1360, LR : 1.6860002274536566e-06, Avg Loss : 0.1017\n",
            "Epoch: 3, Step : 1370, LR : 1.6575685204139657e-06, Avg Loss : 0.0802\n",
            "Epoch: 3, Step : 1380, LR : 1.6291368133742753e-06, Avg Loss : 0.1494\n",
            "Epoch: 3, Step : 1390, LR : 1.6007051063345847e-06, Avg Loss : 0.1501\n",
            "Epoch: 3, Step : 1400, LR : 1.5722733992948938e-06, Avg Loss : 0.0998\n",
            "Epoch: 3, Step : 1410, LR : 1.5438416922552032e-06, Avg Loss : 0.1201\n",
            "Epoch: 3, Step : 1420, LR : 1.5154099852155123e-06, Avg Loss : 0.1304\n",
            "Epoch: 3, Step : 1430, LR : 1.486978278175822e-06, Avg Loss : 0.0803\n",
            "Epoch: 3, Step : 1440, LR : 1.4585465711361313e-06, Avg Loss : 0.1101\n",
            "Epoch: 3, Step : 1450, LR : 1.4301148640964404e-06, Avg Loss : 0.1024\n",
            "Epoch: 3, Step : 1460, LR : 1.40168315705675e-06, Avg Loss : 0.0656\n",
            "Epoch: 3, Step : 1470, LR : 1.3732514500170594e-06, Avg Loss : 0.0710\n",
            "Epoch: 3, Step : 1480, LR : 1.3448197429773685e-06, Avg Loss : 0.0838\n",
            "Epoch: 3, Step : 1490, LR : 1.3163880359376779e-06, Avg Loss : 0.0825\n",
            "Epoch: 3, Step : 1500, LR : 1.287956328897987e-06, Avg Loss : 0.1168\n",
            "Epoch: 3, Step : 1510, LR : 1.2595246218582966e-06, Avg Loss : 0.1178\n",
            "Epoch: 3, Step : 1520, LR : 1.2310929148186058e-06, Avg Loss : 0.0871\n",
            "Epoch: 3, Step : 1530, LR : 1.2026612077789151e-06, Avg Loss : 0.1589\n",
            "Epoch: 3, Step : 1540, LR : 1.1742295007392247e-06, Avg Loss : 0.1309\n",
            "Epoch: 3, Step : 1550, LR : 1.1457977936995339e-06, Avg Loss : 0.0692\n",
            "Epoch: 3, Step : 1560, LR : 1.1173660866598432e-06, Avg Loss : 0.0969\n",
            "Epoch: 3, Step : 1570, LR : 1.0889343796201526e-06, Avg Loss : 0.1504\n",
            "Epoch: 3, Step : 1580, LR : 1.060502672580462e-06, Avg Loss : 0.0895\n",
            "Epoch: 3, Step : 1590, LR : 1.0320709655407713e-06, Avg Loss : 0.1082\n",
            "Epoch: 3, Step : 1600, LR : 1.0036392585010805e-06, Avg Loss : 0.1039\n",
            "Epoch: 3, Step : 1610, LR : 9.752075514613898e-07, Avg Loss : 0.1135\n",
            "Epoch: 3, Step : 1620, LR : 9.467758444216992e-07, Avg Loss : 0.1175\n",
            "Epoch: 3, Step : 1630, LR : 9.183441373820086e-07, Avg Loss : 0.1248\n",
            "Epoch: 3, Step : 1640, LR : 8.899124303423179e-07, Avg Loss : 0.0631\n",
            "Epoch: 3, Step : 1650, LR : 8.614807233026272e-07, Avg Loss : 0.1144\n",
            "Epoch: 3, Step : 1660, LR : 8.330490162629365e-07, Avg Loss : 0.0697\n",
            "Epoch: 3, Step : 1670, LR : 8.046173092232459e-07, Avg Loss : 0.0892\n",
            "Epoch: 3, Step : 1680, LR : 7.761856021835553e-07, Avg Loss : 0.1007\n",
            "Epoch: 3, Step : 1690, LR : 7.477538951438645e-07, Avg Loss : 0.0955\n",
            "Epoch: 3, Step : 1700, LR : 7.193221881041739e-07, Avg Loss : 0.1169\n",
            "Epoch: 3, Step : 1710, LR : 6.908904810644831e-07, Avg Loss : 0.0893\n",
            "Epoch: 3, Step : 1720, LR : 6.624587740247926e-07, Avg Loss : 0.0807\n",
            "Epoch: 3, Step : 1730, LR : 6.340270669851019e-07, Avg Loss : 0.1002\n",
            "Epoch: 3, Step : 1740, LR : 6.055953599454111e-07, Avg Loss : 0.1595\n",
            "Epoch: 3, Step : 1750, LR : 5.771636529057206e-07, Avg Loss : 0.0940\n",
            "Epoch: 3, Step : 1760, LR : 5.487319458660299e-07, Avg Loss : 0.1113\n",
            "Epoch: 3, Step : 1770, LR : 5.203002388263392e-07, Avg Loss : 0.1072\n",
            "Epoch: 3, Step : 1780, LR : 4.918685317866485e-07, Avg Loss : 0.0941\n",
            "Epoch: 3, Step : 1790, LR : 4.6343682474695785e-07, Avg Loss : 0.1170\n",
            "Epoch: 3, Step : 1800, LR : 4.350051177072672e-07, Avg Loss : 0.0946\n",
            "Epoch: 3, Step : 1810, LR : 4.065734106675765e-07, Avg Loss : 0.0854\n",
            "Epoch: 3, Step : 1820, LR : 3.781417036278859e-07, Avg Loss : 0.0749\n",
            "Epoch: 3, Step : 1830, LR : 3.497099965881952e-07, Avg Loss : 0.0725\n",
            "Epoch: 3, Step : 1840, LR : 3.2127828954850457e-07, Avg Loss : 0.1047\n",
            "Epoch: 3, Step : 1850, LR : 2.928465825088139e-07, Avg Loss : 0.1401\n",
            "Epoch: 3, Step : 1860, LR : 2.644148754691232e-07, Avg Loss : 0.0865\n",
            "Epoch: 3, Step : 1870, LR : 2.3598316842943253e-07, Avg Loss : 0.0915\n",
            "Epoch: 3, Step : 1880, LR : 2.0755146138974186e-07, Avg Loss : 0.1258\n",
            "Epoch: 3, Step : 1890, LR : 1.791197543500512e-07, Avg Loss : 0.0834\n",
            "Epoch: 3, Step : 1900, LR : 1.5068804731036054e-07, Avg Loss : 0.0860\n",
            "Epoch: 3, Step : 1910, LR : 1.2225634027066985e-07, Avg Loss : 0.0943\n",
            "Epoch: 3, Step : 1920, LR : 9.38246332309792e-08, Avg Loss : 0.1065\n",
            "Epoch: 3, Step : 1930, LR : 6.539292619128854e-08, Avg Loss : 0.0979\n",
            "Epoch: 3, Step : 1940, LR : 3.6961219151597864e-08, Avg Loss : 0.0962\n",
            "Epoch: 3, Step : 1950, LR : 8.529512111907201e-09, Avg Loss : 0.1023\n",
            "Epoch 3 total_train_loss : 0.1042\n",
            "***** Finish To Train Epoch 3 *****\n",
            "\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step : 10, valid Loss : 0.2746\n",
            "Step : 20, valid Loss : 0.2417\n",
            "Step : 30, valid Loss : 0.2791\n",
            "Step : 40, valid Loss : 0.3638\n",
            "Step : 50, valid Loss : 0.2131\n",
            "Step : 60, valid Loss : 0.2728\n",
            "Step : 70, valid Loss : 0.3245\n",
            "Step : 80, valid Loss : 0.3851\n",
            "Step : 90, valid Loss : 0.2847\n",
            "Step : 100, valid Loss : 0.2940\n",
            "total_valid_loss :  0.2942925412720496 val_f1_score :  93.15389116442364 val_pearsonr : 95.27955013851576\n",
            "Epoch 3 total_Valid Loss : 0.2943\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "Saving epoch 3 checkpoint at /content/drive/MyDrive/AI09/model_v3.ckpt.3\n",
            "Train Finished\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "222459c5ee2944d5bd44cc53ddce4ff3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▃██</td></tr><tr><td>total_pearsonr</td><td>▁▆██</td></tr><tr><td>total_train_loss</td><td>█▂▁▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▁▁▂</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>▂▃▅▇███▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁</td></tr><tr><td>valid_loss</td><td>▅▂▅█▃▄▅▇▅▅▂▁▃▅▁▃▃▅▃▃▂▂▃▄▁▂▄▅▂▃▃▂▃▅▁▃▄▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>93.15389</td></tr><tr><td>total_pearsonr</td><td>95.27955</td></tr><tr><td>total_train_loss</td><td>0.10423</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.29429</td></tr><tr><td>train_loss</td><td>0.10227</td></tr><tr><td>train_lr</td><td>0.0</td></tr><tr><td>valid_loss</td><td>0.29395</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">devoted-sweep-1</strong>: <a href=\"https://wandb.ai/kdb/AI09_v3/runs/phuahdr5\" target=\"_blank\">https://wandb.ai/kdb/AI09_v3/runs/phuahdr5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220602_031858-phuahdr5/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt1 = '/content/drive/MyDrive/AI09/model_v3.ckpt.0'\n",
        "ckpt2 = '/content/drive/MyDrive/AI09/model_v3.ckpt.1'\n",
        "ckpt3 = '/content/drive/MyDrive/AI09/model_v3.ckpt.2'\n",
        "ckpt4 = '/content/drive/MyDrive/AI09/model_v3.ckpt.3'\n",
        "ckpt5 = '/content/drive/MyDrive/AI09/model_v3.ckpt.4'"
      ],
      "metadata": {
        "id": "NvZdn8NXuqET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "all_checkpoints = [ckpt1, ckpt2, ckpt3, ckpt4]\n",
        "\n",
        "for checkpoint in all_checkpoints:\n",
        "    loaded_ckpt = torch.load(checkpoint)\n",
        "    loaded_ckpt['epoch'], loaded_ckpt['loss']\n",
        "    model.load_state_dict(loaded_ckpt[\"model_state_dict\"])\n",
        "    test(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqSs_FX0R8Pw",
        "outputId": "018592e8-3e26-4c71-ed68-a220ef86d367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_test_loss :  0.45487885092367386 total_f1_score :  84.95934959349594 total_pearsonr: 90.83055423360408\n",
            "total_test_loss :  0.46361641060870673 total_f1_score :  85.37074148296594 total_pearsonr: 91.37930542388045\n",
            "total_test_loss :  0.44515289203261077 total_f1_score :  85.71428571428572 total_pearsonr: 91.37805093690585\n",
            "total_test_loss :  0.3944186128929831 total_f1_score :  86.18556701030928 total_pearsonr: 91.88289655591177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation \n",
        "- 불균형한 데이터셋을 보강하기 위해 Data Augmentation 실행  \n",
        "\n",
        "진행순서\n",
        "- 문장 생성\n",
        "- 유사도 높은 문장을 모아 data pair를 구성\n",
        "-  data pair의 유사도를 측정해 라벨링"
      ],
      "metadata": {
        "id": "RAQH9xxCu3Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hanspell 을 통해 sentence1, sentence2 맞춤법 검사\n",
        "!pip install git+https://github.com/ssut/py-hanspell.git"
      ],
      "metadata": {
        "id": "EeleQ8Qsv_81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hanspell import spell_checker\n",
        "\n",
        "checked_lst = []\n",
        "for i in range(len(df)):\n",
        "  checked = spell_checker.check(df['sentence1'][i]).as_dict()['checked']\n",
        "  checked_lst.append(checked)"
      ],
      "metadata": {
        "id": "QXHvakmIwArQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp = spell_checker.check(df['sentence2'][1308]).as_dict()"
      ],
      "metadata": {
        "id": "wYlyjd4SwG48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checked_lst2 = []\n",
        "temp_lst = []\n",
        "for i in range(len(df)):\n",
        "  temp = spell_checker.check(df['sentence2'][i]).as_dict()\n",
        "  checked = temp['checked']\n",
        "  err = temp['errors']\n",
        "  checked_lst2.append(checked)\n",
        "  temp_lst.append(err)"
      ],
      "metadata": {
        "id": "E4hVckwOwNFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean 0.45 error fixed\n",
        "np.mean(temp_lst)"
      ],
      "metadata": {
        "id": "igNHyEwbwR3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['spell_checked1'] = checked_lst\n",
        "df['spell_checked2'] = checked_lst2\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KmO6jXSXwTtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/34053b8f-a4b2-4fb6-a56f-cf05fc703f68/image.png)"
      ],
      "metadata": {
        "id": "dol_bF8VwiZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['sentence1','sentence2','real_label','binary_label']].to_csv('klue_sts_train_aug.csv', index=False)"
      ],
      "metadata": {
        "id": "8XVbBkvbwj_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_aug = pd.read_csv('klue_sts_train_aug.csv')"
      ],
      "metadata": {
        "id": "GPgwGYiUwoQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, val = train_test_split((train_aug), test_size=0.1, random_state = 42)"
      ],
      "metadata": {
        "id": "ejtwc6gPwsBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train.reset_index().drop(['index'], axis = 1)\n",
        "valid_data = val.reset_index().drop(['index'], axis = 1)\n",
        "test_data = test_df.reset_index().drop(['index'], axis = 1)"
      ],
      "metadata": {
        "id": "QHuMph9qwzE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(train_data['real_label'])\n",
        "sns.distplot(valid_data['real_label'])"
      ],
      "metadata": {
        "id": "0A1KFMJlwz0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/fe307b0c-c0da-4004-8743-72afa5d6347e/image.png)"
      ],
      "metadata": {
        "id": "DXEHEDB7w4gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_train = pd.concat([train_data, valid_data])"
      ],
      "metadata": {
        "id": "W63nBJ60xEan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.readers import InputExample\n",
        "# sentence_trainsforemrs 패키지의 입력 형태 InputExample(texts, label)\n",
        "gold_samples = []\n",
        "test_samples = []\n",
        "\n",
        "for sentence1, sentence2, score in zip(sum_train['sentence1'], sum_train['sentence2'], sum_train['real_label']):\n",
        "\n",
        "  score = float(score) / 5.0 # 유사도 측정을 위해 scaling\n",
        "  \n",
        "  inp_example = InputExample(\n",
        "      texts = [sentence1, sentence2],\n",
        "      label = score,\n",
        "  )\n",
        "  rev_inp_example = InputExample(       #유사도 측정으로 샘플링 되기 때문에 순서를 바꿔서도 입력\n",
        "      texts = [sentence2, sentence1],\n",
        "      label = score)\n",
        "  \n",
        "  gold_samples.append(inp_example)\n",
        "  gold_samples.append(rev_inp_example)\n",
        "\n",
        "\n",
        "for sentence1, sentence2, score in zip(test_data['sentence1'], test_data['sentence2'], test_data['real_label']):\n",
        "\n",
        "  score = float(score) / 5.0\n",
        "  \n",
        "  inp_example = InputExample(\n",
        "      texts = [sentence1, sentence2],\n",
        "      label = score,\n",
        "  )\n",
        "  rev_inp_example = InputExample(\n",
        "      texts = [sentence2, sentence1],\n",
        "      label = score)\n",
        "  \n",
        "  test_samples.append(inp_example)\n",
        "  test_samples.append(rev_inp_example)"
      ],
      "metadata": {
        "id": "li_yt901xHDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_sentences = set()\n",
        "silver_data = []\n",
        "\n",
        "for sample in gold_samples:\n",
        "    unique_sentences.update(sample.texts)\n",
        "\n",
        "print(len(unique_sentences))"
      ],
      "metadata": {
        "id": "jtQcScCNxWg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_sentences = list(unique_sentences)\n",
        "sent2idx = {sentence: idx for idx, sentence in enumerate(unique_sentences)}\n",
        "duplicates = set((sent2idx[data.texts[0]], sent2idx[data.texts[1]]) for data in gold_samples)"
      ],
      "metadata": {
        "id": "gidK_RE2xXPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 epochs, train_batch\n",
        "semantic_search_model = SentenceTransformer(\"Huffon/sentence-klue-roberta-base\")\n",
        "# 유니크 데이터 임베딩\n",
        "embeddings = semantic_search_model.encode(unique_sentences, batch_size = 128, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "zPZLygSRxZQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 3\n",
        "# cos_sim 기준 top 3 문장 조합을 추가 데이터로 활용\n",
        "progress = tqdm.tqdm(unit=\"docs\", total=len(sent2idx))\n",
        "for idx in range(len(unique_sentences)):\n",
        "    sentence_embedding = embeddings[idx]              \n",
        "    cos_scores = util.cos_sim(sentence_embedding, embeddings)[0]  #인덱스를 바꿔가면서 모든 문장과 비교\n",
        "    cos_scores = cos_scores.cpu()\n",
        "    progress.update(1)\n",
        "\n",
        "    #We use torch.topk to find the highest 3 scores\n",
        "    top_results = torch.topk(cos_scores, k=top_k+1) #Top 3 문장 추출\n",
        "    \n",
        "    for score, iid in zip(top_results[0], top_results[1]):\n",
        "        if iid != idx and (iid, idx) not in duplicates:\n",
        "            silver_data.append((unique_sentences[idx], unique_sentences[iid]))\n",
        "            duplicates.add((idx,iid))\n",
        "\n",
        "progress.reset()\n",
        "progress.close()"
      ],
      "metadata": {
        "id": "T2oVb-H7xb38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers.cross_encoder import CrossEncoder\n",
        "from sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\n",
        "# cross_encoder 훈련 세팅\n",
        "cross_encoder = CrossEncoder(\"klue/roberta-base\", num_labels=1)\n",
        "train_batch_size = 16\n",
        "num_epochs = 4\n",
        "max_seq_length = 128\n",
        "\n",
        "# Train Dataloader 생성\n",
        "train_dataloader = DataLoader(gold_samples, shuffle=True, batch_size=16)\n",
        "# Test data로 성능 지표 확인\n",
        "evaluator = CECorrelationEvaluator.from_input_examples(test_samples, name='sts-test')\n",
        "\n",
        "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n",
        "print(warmup_steps)"
      ],
      "metadata": {
        "id": "vuX7_5x-xeQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cross_encoder.fit(train_dataloader=train_dataloader,\n",
        "          evaluator=evaluator,\n",
        "          epochs=num_epochs,\n",
        "          warmup_steps=warmup_steps)"
      ],
      "metadata": {
        "id": "AEYjMywlxgYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_scores = cross_encoder.predict(silver_data)\n",
        "# All model predictions should be between [0,1]\n",
        "assert all(0.0 <= score <= 1.0 for score in silver_scores)"
      ],
      "metadata": {
        "id": "JojHgqz6xibE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_sentence1, silver_sentence2 = zip(*silver_data)"
      ],
      "metadata": {
        "id": "ZagAHBA8xkfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_data = pd.DataFrame({\n",
        "    \"sentence1\" : silver_sentence1,\n",
        "    \"sentence2\" : silver_sentence2,\n",
        "    \"real_label\" : silver_scores * 5.0}\n",
        ")"
      ],
      "metadata": {
        "id": "_EjX8KoCxlpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_data"
      ],
      "metadata": {
        "id": "VXgsulhLxm_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/0a750598-714c-4d38-a987-3b8bb605d88c/image.png)"
      ],
      "metadata": {
        "id": "E2ghkwb_xVGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.displot(silver_data['real_label'])"
      ],
      "metadata": {
        "id": "pAOZoU78xwGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/912bee07-f7c8-48a5-adae-603cbb847e57/image.png)"
      ],
      "metadata": {
        "id": "pAHsU8gnx0Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_balance(x):\n",
        "  \"\"\"\n",
        "  그래프 상의 x축을 기준으로 A, B, C, Z 섹션을 나눠주기 위한 함수\n",
        "  섹션을 나눠준 기준 : 그래프상에서 가장 undersampling이 필요한 부분을 임의로 나눠줌\n",
        "  Z섹션은 undersampling이 필요하지 않다고 판단한 부분\n",
        "  \"\"\"\n",
        "  if x <= 3:\n",
        "    return 'Z' \n",
        "\n",
        "  elif (x > 3) & (x < 3.5):\n",
        "    return 'A'\n",
        "\n",
        "  elif (x >= 3.5) & (x < 3.8):\n",
        "    return 'B'\n",
        "\n",
        "  elif (x >= 3.8) & (x < 4.6):\n",
        "    return 'C'\n",
        "  else:\n",
        "    return 'Z'"
      ],
      "metadata": {
        "id": "0oBcY6k_x8zI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_data['drop_label'] = silver_data['real_label'].apply(make_balance)"
      ],
      "metadata": {
        "id": "2L89K5AAyAtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silver_shuffled = silver_data.sample(frac=1).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "TyHLWST7yBOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexNames = silver_shuffled[ silver_shuffled['drop_label'] == 3.5 ][:2500].index  \n",
        "silver_shuffled.drop(indexNames , inplace=True)\n",
        "\n",
        "indexNames = silver_shuffled[ silver_shuffled['drop_label'] == 3.8 ][:40000].index\n",
        "silver_shuffled.drop(indexNames , inplace=True)\n",
        "\n",
        "indexNames = silver_shuffled[ silver_shuffled['drop_label'] == 4.75 ][:30000].index\n",
        "silver_shuffled.drop(indexNames , inplace=True)"
      ],
      "metadata": {
        "id": "PpTVLOXCyCxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(silver_shuffled['real_label'])"
      ],
      "metadata": {
        "id": "XdU44oKWyEtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/bfa0363e-47e2-4a23-80e0-2e8093b99faa/image.png)\n"
      ],
      "metadata": {
        "id": "DFWlpv2syLaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = silver_shuffled.sample(n=15000, random_state=17)\n",
        "# 아까 만든 drop_label 컬럼 삭제\n",
        "sampled_df = sampled_df.drop('drop_label', axis=1)\n",
        "\n",
        "# 마지막으로 distplot 확인\n",
        "sns.distplot(sampled_df['real_label'])"
      ],
      "metadata": {
        "id": "ytJL9Y_ByGSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://velog.velcdn.com/images/khyait/post/3cc8081f-eb88-4f7d-9dba-8468260b53c0/image.png)"
      ],
      "metadata": {
        "id": "heOt3JqyySa1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug_train_data.to_csv('aug_klue_sts_train.csv', index=False)\n",
        "aug_valid_data.to_csv('aug_klue_sts_vaild.csv', index=False)"
      ],
      "metadata": {
        "id": "Kntuj2DlyeNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 증강 데이터 다운\n",
        "train = pd.read_csv('/content/drive/MyDrive/NLP/aug_klue_sts_train.csv')\n",
        "val =  pd.read_csv('/content/drive/MyDrive/NLP/aug_klue_sts_vaild.csv')\n",
        "test = pd.read_json('/content/drive/MyDrive/NLP/klue-sts-v1.1/klue-sts-v1.1_dev.json')"
      ],
      "metadata": {
        "id": "g0IWJb6uFRk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = preprocess_train(train)\n",
        "val = preprocess_train(val)"
      ],
      "metadata": {
        "id": "6oXxeKm0A5LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.rename(columns= {'real_label':'real-label', 'binary_label':'binary-label'}, inplace=True)\n",
        "val.rename(columns= {'real_label':'real-label', 'binary_label':'binary-label'}, inplace=True)\n",
        "\n",
        "val['binary-label'] = val['real-label'].apply(lambda x: 0 if x < 3  else  1)\n",
        "\n",
        "train = train.drop_duplicates(['sentence1','sentence2'], keep='first', ignore_index=True)\n",
        "val = val.drop_duplicates(['sentence1','sentence2'], keep='first', ignore_index=True)"
      ],
      "metadata": {
        "id": "2np5nFFZX2QV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 인덱스 초기화\n",
        "train_data = train.reset_index().drop(['index'], axis = 1)\n",
        "valid_data = val.reset_index().drop(['index'], axis = 1)\n",
        "test_data = test.reset_index().drop(['index'], axis = 1)"
      ],
      "metadata": {
        "id": "_2HcNUYkZcZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train_data)\n",
        "valid_dataset = CustomDataset(valid_data)\n",
        "test_dataset = CustomDataset(test_data)"
      ],
      "metadata": {
        "id": "vNXiyBJYZns6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, scheduler, epoch, loss):\n",
        "\n",
        "    file_name = f'/content/drive/MyDrive/AI09/model_v4.ckpt.{epoch}'\n",
        "        \n",
        "    torch.save(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "2FdDCHSYZ2vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "                              train_dataset,\n",
        "                              batch_size = 8,\n",
        "                              sampler = RandomSampler(train_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length=128)\n",
        "                              )\n",
        "valid_dataloader = DataLoader(\n",
        "                              valid_dataset,\n",
        "                              batch_size = 16,\n",
        "                              sampler = SequentialSampler(valid_dataset),\n",
        "                              collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                              )\n",
        "test_dataloader = DataLoader(\n",
        "                            test_dataset, \n",
        "                            batch_size = 16,\n",
        "                            sampler = SequentialSampler(test_dataset),\n",
        "                            collate_fn = partial(custom_collate_fn, max_length= 128)\n",
        "                            )"
      ],
      "metadata": {
        "id": "ODhaLhmIZtgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"AI09_v1\")\n",
        "wandb.agent(sweep_id, initializer, count = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "039300ff15614fda89ca7a69043d2087",
            "8aac4e46c57243438e60e8ba90e71a5b",
            "acd27e6031c24cfebaa9ad6686f4aff4",
            "de97aebc99584dae85c7906c21850a8a",
            "a2050859ec454e22b4b6c8e4aa516fa7",
            "fe3e7a93a57846d4814f989153b33d4b",
            "82c3315ac3784b358dd3bfb48f5374a6",
            "687736a958b14646bd417b19382e67b9"
          ]
        },
        "id": "jYXMXlQBGi8c",
        "outputId": "b7c5746a-4f63-48af-ad62-d1eb64eed210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: zm92hszq\n",
            "Sweep URL: https://wandb.ai/kdb/AI09_v1/sweeps/zm92hszq\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nbb3o43b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \teps: 1e-08\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgrad_norm: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttrain_batch_size: 8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tvalid_batch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarm_up_ratio: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdb\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.17"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/NLP/wandb/run-20220602_012805-nbb3o43b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/kdb/AI09_v1/runs/nbb3o43b\" target=\"_blank\">twilight-sweep-1</a></strong> to <a href=\"https://wandb.ai/kdb/AI09_v1\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/kdb/AI09_v1/sweeps/zm92hszq\" target=\"_blank\">https://wandb.ai/kdb/AI09_v1/sweeps/zm92hszq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****** Starting To Train Epoch #0 ******\n",
            "Epoch: 0, Step : 10, LR : 9.987333026255182e-06, Avg Loss : 5.0947\n",
            "Epoch: 0, Step : 20, LR : 9.975817595578076e-06, Avg Loss : 3.5380\n",
            "Epoch: 0, Step : 30, LR : 9.964302164900968e-06, Avg Loss : 3.0817\n",
            "Epoch: 0, Step : 40, LR : 9.95278673422386e-06, Avg Loss : 2.8018\n",
            "Epoch: 0, Step : 50, LR : 9.941271303546754e-06, Avg Loss : 2.6660\n",
            "Epoch: 0, Step : 60, LR : 9.929755872869646e-06, Avg Loss : 2.6548\n",
            "Epoch: 0, Step : 70, LR : 9.91824044219254e-06, Avg Loss : 2.3422\n",
            "Epoch: 0, Step : 80, LR : 9.906725011515431e-06, Avg Loss : 2.6845\n",
            "Epoch: 0, Step : 90, LR : 9.895209580838325e-06, Avg Loss : 2.6897\n",
            "Epoch: 0, Step : 100, LR : 9.883694150161217e-06, Avg Loss : 2.8178\n",
            "Epoch: 0, Step : 110, LR : 9.872178719484109e-06, Avg Loss : 2.1376\n",
            "Epoch: 0, Step : 120, LR : 9.860663288807002e-06, Avg Loss : 1.1898\n",
            "Epoch: 0, Step : 130, LR : 9.849147858129894e-06, Avg Loss : 1.0888\n",
            "Epoch: 0, Step : 140, LR : 9.837632427452788e-06, Avg Loss : 0.9340\n",
            "Epoch: 0, Step : 150, LR : 9.82611699677568e-06, Avg Loss : 0.9858\n",
            "Epoch: 0, Step : 160, LR : 9.814601566098573e-06, Avg Loss : 0.7846\n",
            "Epoch: 0, Step : 170, LR : 9.803086135421465e-06, Avg Loss : 0.4643\n",
            "Epoch: 0, Step : 180, LR : 9.791570704744359e-06, Avg Loss : 0.4445\n",
            "Epoch: 0, Step : 190, LR : 9.780055274067251e-06, Avg Loss : 0.4315\n",
            "Epoch: 0, Step : 200, LR : 9.768539843390143e-06, Avg Loss : 0.8167\n",
            "Epoch: 0, Step : 210, LR : 9.757024412713037e-06, Avg Loss : 0.7753\n",
            "Epoch: 0, Step : 220, LR : 9.74550898203593e-06, Avg Loss : 0.7618\n",
            "Epoch: 0, Step : 230, LR : 9.733993551358822e-06, Avg Loss : 0.6055\n",
            "Epoch: 0, Step : 240, LR : 9.722478120681714e-06, Avg Loss : 0.6939\n",
            "Epoch: 0, Step : 250, LR : 9.710962690004608e-06, Avg Loss : 0.5089\n",
            "Epoch: 0, Step : 260, LR : 9.6994472593275e-06, Avg Loss : 0.4594\n",
            "Epoch: 0, Step : 270, LR : 9.687931828650392e-06, Avg Loss : 0.6991\n",
            "Epoch: 0, Step : 280, LR : 9.676416397973285e-06, Avg Loss : 0.7754\n",
            "Epoch: 0, Step : 290, LR : 9.664900967296177e-06, Avg Loss : 0.4815\n",
            "Epoch: 0, Step : 300, LR : 9.653385536619069e-06, Avg Loss : 0.6818\n",
            "Epoch: 0, Step : 310, LR : 9.641870105941963e-06, Avg Loss : 0.3804\n",
            "Epoch: 0, Step : 320, LR : 9.630354675264856e-06, Avg Loss : 0.3925\n",
            "Epoch: 0, Step : 330, LR : 9.618839244587748e-06, Avg Loss : 0.4907\n",
            "Epoch: 0, Step : 340, LR : 9.60732381391064e-06, Avg Loss : 0.5618\n",
            "Epoch: 0, Step : 350, LR : 9.595808383233534e-06, Avg Loss : 0.6236\n",
            "Epoch: 0, Step : 360, LR : 9.584292952556426e-06, Avg Loss : 0.3303\n",
            "Epoch: 0, Step : 370, LR : 9.57277752187932e-06, Avg Loss : 0.6764\n",
            "Epoch: 0, Step : 380, LR : 9.561262091202211e-06, Avg Loss : 0.3319\n",
            "Epoch: 0, Step : 390, LR : 9.549746660525105e-06, Avg Loss : 0.3978\n",
            "Epoch: 0, Step : 400, LR : 9.538231229847997e-06, Avg Loss : 0.4980\n",
            "Epoch: 0, Step : 410, LR : 9.52671579917089e-06, Avg Loss : 0.4153\n",
            "Epoch: 0, Step : 420, LR : 9.515200368493783e-06, Avg Loss : 0.3755\n",
            "Epoch: 0, Step : 430, LR : 9.503684937816674e-06, Avg Loss : 0.3261\n",
            "Epoch: 0, Step : 440, LR : 9.492169507139568e-06, Avg Loss : 0.5791\n",
            "Epoch: 0, Step : 450, LR : 9.48065407646246e-06, Avg Loss : 0.4514\n",
            "Epoch: 0, Step : 460, LR : 9.469138645785354e-06, Avg Loss : 0.4024\n",
            "Epoch: 0, Step : 470, LR : 9.457623215108246e-06, Avg Loss : 0.4258\n",
            "Epoch: 0, Step : 480, LR : 9.44610778443114e-06, Avg Loss : 0.6661\n",
            "Epoch: 0, Step : 490, LR : 9.434592353754031e-06, Avg Loss : 0.4324\n",
            "Epoch: 0, Step : 500, LR : 9.423076923076923e-06, Avg Loss : 0.3301\n",
            "Epoch: 0, Step : 510, LR : 9.411561492399817e-06, Avg Loss : 0.4781\n",
            "Epoch: 0, Step : 520, LR : 9.40004606172271e-06, Avg Loss : 0.2835\n",
            "Epoch: 0, Step : 530, LR : 9.3885306310456e-06, Avg Loss : 0.5454\n",
            "Epoch: 0, Step : 540, LR : 9.377015200368494e-06, Avg Loss : 0.5997\n",
            "Epoch: 0, Step : 550, LR : 9.365499769691388e-06, Avg Loss : 0.5265\n",
            "Epoch: 0, Step : 560, LR : 9.35398433901428e-06, Avg Loss : 0.6358\n",
            "Epoch: 0, Step : 570, LR : 9.342468908337172e-06, Avg Loss : 0.3941\n",
            "Epoch: 0, Step : 580, LR : 9.330953477660065e-06, Avg Loss : 0.4242\n",
            "Epoch: 0, Step : 590, LR : 9.319438046982957e-06, Avg Loss : 0.4994\n",
            "Epoch: 0, Step : 600, LR : 9.307922616305851e-06, Avg Loss : 0.3420\n",
            "Epoch: 0, Step : 610, LR : 9.296407185628743e-06, Avg Loss : 0.4275\n",
            "Epoch: 0, Step : 620, LR : 9.284891754951637e-06, Avg Loss : 0.3853\n",
            "Epoch: 0, Step : 630, LR : 9.273376324274529e-06, Avg Loss : 0.5098\n",
            "Epoch: 0, Step : 640, LR : 9.26186089359742e-06, Avg Loss : 0.5073\n",
            "Epoch: 0, Step : 650, LR : 9.250345462920314e-06, Avg Loss : 0.2485\n",
            "Epoch: 0, Step : 660, LR : 9.238830032243206e-06, Avg Loss : 0.3548\n",
            "Epoch: 0, Step : 670, LR : 9.2273146015661e-06, Avg Loss : 0.7638\n",
            "Epoch: 0, Step : 680, LR : 9.215799170888992e-06, Avg Loss : 0.4645\n",
            "Epoch: 0, Step : 690, LR : 9.204283740211885e-06, Avg Loss : 0.4874\n",
            "Epoch: 0, Step : 700, LR : 9.192768309534777e-06, Avg Loss : 0.3069\n",
            "Epoch: 0, Step : 710, LR : 9.181252878857671e-06, Avg Loss : 0.3628\n",
            "Epoch: 0, Step : 720, LR : 9.169737448180563e-06, Avg Loss : 0.4766\n",
            "Epoch: 0, Step : 730, LR : 9.158222017503455e-06, Avg Loss : 0.3834\n",
            "Epoch: 0, Step : 740, LR : 9.146706586826348e-06, Avg Loss : 0.4929\n",
            "Epoch: 0, Step : 750, LR : 9.135191156149242e-06, Avg Loss : 0.5073\n",
            "Epoch: 0, Step : 760, LR : 9.123675725472132e-06, Avg Loss : 0.5683\n",
            "Epoch: 0, Step : 770, LR : 9.112160294795026e-06, Avg Loss : 0.4482\n",
            "Epoch: 0, Step : 780, LR : 9.10064486411792e-06, Avg Loss : 0.4198\n",
            "Epoch: 0, Step : 790, LR : 9.089129433440811e-06, Avg Loss : 0.2454\n",
            "Epoch: 0, Step : 800, LR : 9.077614002763703e-06, Avg Loss : 0.6187\n",
            "Epoch: 0, Step : 810, LR : 9.066098572086597e-06, Avg Loss : 0.5868\n",
            "Epoch: 0, Step : 820, LR : 9.054583141409489e-06, Avg Loss : 0.2795\n",
            "Epoch: 0, Step : 830, LR : 9.043067710732381e-06, Avg Loss : 0.4757\n",
            "Epoch: 0, Step : 840, LR : 9.031552280055275e-06, Avg Loss : 0.4087\n",
            "Epoch: 0, Step : 850, LR : 9.020036849378168e-06, Avg Loss : 0.3957\n",
            "Epoch: 0, Step : 860, LR : 9.00852141870106e-06, Avg Loss : 0.4107\n",
            "Epoch: 0, Step : 870, LR : 8.997005988023952e-06, Avg Loss : 0.4003\n",
            "Epoch: 0, Step : 880, LR : 8.985490557346846e-06, Avg Loss : 0.4386\n",
            "Epoch: 0, Step : 890, LR : 8.973975126669738e-06, Avg Loss : 0.4886\n",
            "Epoch: 0, Step : 900, LR : 8.962459695992631e-06, Avg Loss : 0.4345\n",
            "Epoch: 0, Step : 910, LR : 8.950944265315523e-06, Avg Loss : 0.4647\n",
            "Epoch: 0, Step : 920, LR : 8.939428834638417e-06, Avg Loss : 0.3619\n",
            "Epoch: 0, Step : 930, LR : 8.927913403961309e-06, Avg Loss : 0.3704\n",
            "Epoch: 0, Step : 940, LR : 8.916397973284202e-06, Avg Loss : 0.3989\n",
            "Epoch: 0, Step : 950, LR : 8.904882542607094e-06, Avg Loss : 0.5629\n",
            "Epoch: 0, Step : 960, LR : 8.893367111929986e-06, Avg Loss : 0.2930\n",
            "Epoch: 0, Step : 970, LR : 8.88185168125288e-06, Avg Loss : 0.4079\n",
            "Epoch: 0, Step : 980, LR : 8.870336250575772e-06, Avg Loss : 0.4130\n",
            "Epoch: 0, Step : 990, LR : 8.858820819898666e-06, Avg Loss : 0.4462\n",
            "Epoch: 0, Step : 1000, LR : 8.847305389221558e-06, Avg Loss : 0.2831\n",
            "Epoch: 0, Step : 1010, LR : 8.835789958544451e-06, Avg Loss : 0.7315\n",
            "Epoch: 0, Step : 1020, LR : 8.824274527867343e-06, Avg Loss : 0.3990\n",
            "Epoch: 0, Step : 1030, LR : 8.812759097190235e-06, Avg Loss : 0.3199\n",
            "Epoch: 0, Step : 1040, LR : 8.801243666513129e-06, Avg Loss : 0.5591\n",
            "Epoch: 0, Step : 1050, LR : 8.78972823583602e-06, Avg Loss : 0.4566\n",
            "Epoch: 0, Step : 1060, LR : 8.778212805158913e-06, Avg Loss : 0.4765\n",
            "Epoch: 0, Step : 1070, LR : 8.766697374481806e-06, Avg Loss : 0.3693\n",
            "Epoch: 0, Step : 1080, LR : 8.7551819438047e-06, Avg Loss : 0.4816\n",
            "Epoch: 0, Step : 1090, LR : 8.743666513127592e-06, Avg Loss : 0.6297\n",
            "Epoch: 0, Step : 1100, LR : 8.732151082450484e-06, Avg Loss : 0.3319\n",
            "Epoch: 0, Step : 1110, LR : 8.720635651773377e-06, Avg Loss : 0.4028\n",
            "Epoch: 0, Step : 1120, LR : 8.70912022109627e-06, Avg Loss : 0.5163\n",
            "Epoch: 0, Step : 1130, LR : 8.697604790419161e-06, Avg Loss : 0.2633\n",
            "Epoch: 0, Step : 1140, LR : 8.686089359742055e-06, Avg Loss : 0.3011\n",
            "Epoch: 0, Step : 1150, LR : 8.674573929064948e-06, Avg Loss : 0.6395\n",
            "Epoch: 0, Step : 1160, LR : 8.66305849838784e-06, Avg Loss : 0.3926\n",
            "Epoch: 0, Step : 1170, LR : 8.651543067710732e-06, Avg Loss : 0.2533\n",
            "Epoch: 0, Step : 1180, LR : 8.640027637033626e-06, Avg Loss : 0.3936\n",
            "Epoch: 0, Step : 1190, LR : 8.628512206356518e-06, Avg Loss : 0.3123\n",
            "Epoch: 0, Step : 1200, LR : 8.616996775679412e-06, Avg Loss : 0.4216\n",
            "Epoch: 0, Step : 1210, LR : 8.605481345002304e-06, Avg Loss : 0.3413\n",
            "Epoch: 0, Step : 1220, LR : 8.593965914325197e-06, Avg Loss : 0.4912\n",
            "Epoch: 0, Step : 1230, LR : 8.582450483648089e-06, Avg Loss : 0.3327\n",
            "Epoch: 0, Step : 1240, LR : 8.570935052970983e-06, Avg Loss : 0.4152\n",
            "Epoch: 0, Step : 1250, LR : 8.559419622293875e-06, Avg Loss : 0.3407\n",
            "Epoch: 0, Step : 1260, LR : 8.547904191616767e-06, Avg Loss : 0.5391\n",
            "Epoch: 0, Step : 1270, LR : 8.53638876093966e-06, Avg Loss : 0.3725\n",
            "Epoch: 0, Step : 1280, LR : 8.524873330262554e-06, Avg Loss : 0.4718\n",
            "Epoch: 0, Step : 1290, LR : 8.513357899585444e-06, Avg Loss : 0.2919\n",
            "Epoch: 0, Step : 1300, LR : 8.501842468908338e-06, Avg Loss : 0.4783\n",
            "Epoch: 0, Step : 1310, LR : 8.490327038231231e-06, Avg Loss : 0.1596\n",
            "Epoch: 0, Step : 1320, LR : 8.478811607554123e-06, Avg Loss : 0.2289\n",
            "Epoch: 0, Step : 1330, LR : 8.467296176877015e-06, Avg Loss : 0.4998\n",
            "Epoch: 0, Step : 1340, LR : 8.455780746199909e-06, Avg Loss : 0.6028\n",
            "Epoch: 0, Step : 1350, LR : 8.444265315522801e-06, Avg Loss : 0.3472\n",
            "Epoch: 0, Step : 1360, LR : 8.432749884845693e-06, Avg Loss : 0.3934\n",
            "Epoch: 0, Step : 1370, LR : 8.421234454168586e-06, Avg Loss : 0.3364\n",
            "Epoch: 0, Step : 1380, LR : 8.40971902349148e-06, Avg Loss : 0.5971\n",
            "Epoch: 0, Step : 1390, LR : 8.398203592814372e-06, Avg Loss : 0.4287\n",
            "Epoch: 0, Step : 1400, LR : 8.386688162137264e-06, Avg Loss : 0.4736\n",
            "Epoch: 0, Step : 1410, LR : 8.375172731460158e-06, Avg Loss : 0.2292\n",
            "Epoch: 0, Step : 1420, LR : 8.36365730078305e-06, Avg Loss : 0.3672\n",
            "Epoch: 0, Step : 1430, LR : 8.352141870105943e-06, Avg Loss : 0.4769\n",
            "Epoch: 0, Step : 1440, LR : 8.340626439428835e-06, Avg Loss : 0.3834\n",
            "Epoch: 0, Step : 1450, LR : 8.329111008751729e-06, Avg Loss : 0.4039\n",
            "Epoch: 0, Step : 1460, LR : 8.31759557807462e-06, Avg Loss : 0.4162\n",
            "Epoch: 0, Step : 1470, LR : 8.306080147397514e-06, Avg Loss : 0.5397\n",
            "Epoch: 0, Step : 1480, LR : 8.294564716720406e-06, Avg Loss : 0.3445\n",
            "Epoch: 0, Step : 1490, LR : 8.283049286043298e-06, Avg Loss : 0.3966\n",
            "Epoch: 0, Step : 1500, LR : 8.271533855366192e-06, Avg Loss : 0.6685\n",
            "Epoch: 0, Step : 1510, LR : 8.260018424689084e-06, Avg Loss : 0.4333\n",
            "Epoch: 0, Step : 1520, LR : 8.248502994011976e-06, Avg Loss : 0.6584\n",
            "Epoch: 0, Step : 1530, LR : 8.23698756333487e-06, Avg Loss : 0.3481\n",
            "Epoch: 0, Step : 1540, LR : 8.225472132657763e-06, Avg Loss : 0.3018\n",
            "Epoch: 0, Step : 1550, LR : 8.213956701980655e-06, Avg Loss : 0.2162\n",
            "Epoch: 0, Step : 1560, LR : 8.202441271303547e-06, Avg Loss : 0.4730\n",
            "Epoch: 0, Step : 1570, LR : 8.19092584062644e-06, Avg Loss : 0.3839\n",
            "Epoch: 0, Step : 1580, LR : 8.179410409949332e-06, Avg Loss : 0.3958\n",
            "Epoch: 0, Step : 1590, LR : 8.167894979272224e-06, Avg Loss : 0.4145\n",
            "Epoch: 0, Step : 1600, LR : 8.156379548595118e-06, Avg Loss : 0.2225\n",
            "Epoch: 0, Step : 1610, LR : 8.144864117918012e-06, Avg Loss : 0.3173\n",
            "Epoch: 0, Step : 1620, LR : 8.133348687240904e-06, Avg Loss : 0.3994\n",
            "Epoch: 0, Step : 1630, LR : 8.121833256563796e-06, Avg Loss : 0.4164\n",
            "Epoch: 0, Step : 1640, LR : 8.11031782588669e-06, Avg Loss : 0.3271\n",
            "Epoch: 0, Step : 1650, LR : 8.098802395209581e-06, Avg Loss : 0.5344\n",
            "Epoch: 0, Step : 1660, LR : 8.087286964532473e-06, Avg Loss : 0.3491\n",
            "Epoch: 0, Step : 1670, LR : 8.075771533855367e-06, Avg Loss : 0.4474\n",
            "Epoch: 0, Step : 1680, LR : 8.06425610317826e-06, Avg Loss : 0.2843\n",
            "Epoch: 0, Step : 1690, LR : 8.052740672501152e-06, Avg Loss : 0.4607\n",
            "Epoch: 0, Step : 1700, LR : 8.041225241824044e-06, Avg Loss : 0.3359\n",
            "Epoch: 0, Step : 1710, LR : 8.029709811146938e-06, Avg Loss : 0.1865\n",
            "Epoch: 0, Step : 1720, LR : 8.01819438046983e-06, Avg Loss : 0.4551\n",
            "Epoch: 0, Step : 1730, LR : 8.006678949792723e-06, Avg Loss : 0.2528\n",
            "Epoch: 0, Step : 1740, LR : 7.995163519115615e-06, Avg Loss : 0.4798\n",
            "Epoch: 0, Step : 1750, LR : 7.983648088438507e-06, Avg Loss : 0.2852\n",
            "Epoch: 0, Step : 1760, LR : 7.972132657761401e-06, Avg Loss : 0.3443\n",
            "Epoch: 0, Step : 1770, LR : 7.960617227084295e-06, Avg Loss : 0.5136\n",
            "Epoch: 0, Step : 1780, LR : 7.949101796407187e-06, Avg Loss : 0.3456\n",
            "Epoch: 0, Step : 1790, LR : 7.937586365730078e-06, Avg Loss : 0.3337\n",
            "Epoch: 0, Step : 1800, LR : 7.926070935052972e-06, Avg Loss : 0.5144\n",
            "Epoch: 0, Step : 1810, LR : 7.914555504375864e-06, Avg Loss : 0.2692\n",
            "Epoch: 0, Step : 1820, LR : 7.903040073698756e-06, Avg Loss : 0.3044\n",
            "Epoch: 0, Step : 1830, LR : 7.89152464302165e-06, Avg Loss : 0.2634\n",
            "Epoch: 0, Step : 1840, LR : 7.880009212344543e-06, Avg Loss : 0.5908\n",
            "Epoch: 0, Step : 1850, LR : 7.868493781667435e-06, Avg Loss : 0.4989\n",
            "Epoch: 0, Step : 1860, LR : 7.856978350990327e-06, Avg Loss : 0.3300\n",
            "Epoch: 0, Step : 1870, LR : 7.84546292031322e-06, Avg Loss : 0.3455\n",
            "Epoch: 0, Step : 1880, LR : 7.833947489636113e-06, Avg Loss : 0.3275\n",
            "Epoch: 0, Step : 1890, LR : 7.822432058959005e-06, Avg Loss : 0.4872\n",
            "Epoch: 0, Step : 1900, LR : 7.810916628281898e-06, Avg Loss : 0.3271\n",
            "Epoch: 0, Step : 1910, LR : 7.799401197604792e-06, Avg Loss : 0.5103\n",
            "Epoch: 0, Step : 1920, LR : 7.787885766927684e-06, Avg Loss : 0.2683\n",
            "Epoch: 0, Step : 1930, LR : 7.776370336250576e-06, Avg Loss : 0.3667\n",
            "Epoch: 0, Step : 1940, LR : 7.76485490557347e-06, Avg Loss : 0.4252\n",
            "Epoch: 0, Step : 1950, LR : 7.753339474896361e-06, Avg Loss : 0.3549\n",
            "Epoch: 0, Step : 1960, LR : 7.741824044219255e-06, Avg Loss : 0.3550\n",
            "Epoch: 0, Step : 1970, LR : 7.730308613542147e-06, Avg Loss : 0.3864\n",
            "Epoch: 0, Step : 1980, LR : 7.71879318286504e-06, Avg Loss : 0.5662\n",
            "Epoch: 0, Step : 1990, LR : 7.707277752187933e-06, Avg Loss : 0.4143\n",
            "Epoch: 0, Step : 2000, LR : 7.695762321510824e-06, Avg Loss : 0.5566\n",
            "Epoch: 0, Step : 2010, LR : 7.684246890833718e-06, Avg Loss : 0.3764\n",
            "Epoch: 0, Step : 2020, LR : 7.67273146015661e-06, Avg Loss : 0.4490\n",
            "Epoch: 0, Step : 2030, LR : 7.661216029479504e-06, Avg Loss : 0.3144\n",
            "Epoch: 0, Step : 2040, LR : 7.649700598802396e-06, Avg Loss : 0.4957\n",
            "Epoch: 0, Step : 2050, LR : 7.638185168125288e-06, Avg Loss : 0.6016\n",
            "Epoch: 0, Step : 2060, LR : 7.626669737448181e-06, Avg Loss : 0.3381\n",
            "Epoch: 0, Step : 2070, LR : 7.615154306771074e-06, Avg Loss : 0.4930\n",
            "Epoch: 0, Step : 2080, LR : 7.603638876093966e-06, Avg Loss : 0.3812\n",
            "Epoch: 0, Step : 2090, LR : 7.592123445416859e-06, Avg Loss : 0.2069\n",
            "Epoch: 0, Step : 2100, LR : 7.580608014739752e-06, Avg Loss : 0.3185\n",
            "Epoch: 0, Step : 2110, LR : 7.569092584062645e-06, Avg Loss : 0.4334\n",
            "Epoch: 0, Step : 2120, LR : 7.557577153385537e-06, Avg Loss : 0.3782\n",
            "Epoch: 0, Step : 2130, LR : 7.54606172270843e-06, Avg Loss : 0.2769\n",
            "Epoch: 0, Step : 2140, LR : 7.534546292031323e-06, Avg Loss : 0.2316\n",
            "Epoch: 0, Step : 2150, LR : 7.5230308613542155e-06, Avg Loss : 0.3247\n",
            "Epoch: 0, Step : 2160, LR : 7.511515430677107e-06, Avg Loss : 0.3083\n",
            "Epoch: 0, Step : 2170, LR : 7.500000000000001e-06, Avg Loss : 0.4362\n",
            "Epoch 0 total_train_loss : 0.5734\n",
            "***** Finish To Train Epoch 0 *****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Step : 10, valid Loss : 0.2170\n",
            "Step : 20, valid Loss : 0.1887\n",
            "Step : 30, valid Loss : 0.2237\n",
            "Step : 40, valid Loss : 0.1912\n",
            "Step : 50, valid Loss : 0.2193\n",
            "Step : 60, valid Loss : 0.1858\n",
            "Step : 70, valid Loss : 0.2000\n",
            "Step : 80, valid Loss : 0.2181\n",
            "Step : 90, valid Loss : 0.1732\n",
            "Step : 100, valid Loss : 0.2297\n",
            "Step : 110, valid Loss : 0.2239\n",
            "Step : 120, valid Loss : 0.2157\n",
            "Step : 130, valid Loss : 0.2069\n",
            "Step : 140, valid Loss : 0.2603\n",
            "Step : 150, valid Loss : 0.2347\n",
            "Step : 160, valid Loss : 0.1879\n",
            "Step : 170, valid Loss : 0.2481\n",
            "Step : 180, valid Loss : 0.2205\n",
            "Step : 190, valid Loss : 0.2256\n",
            "Step : 200, valid Loss : 0.1872\n",
            "Step : 210, valid Loss : 0.1998\n",
            "Step : 220, valid Loss : 0.2162\n",
            "Step : 230, valid Loss : 0.1984\n",
            "Step : 240, valid Loss : 0.2053\n",
            "Step : 250, valid Loss : 0.2367\n",
            "Step : 260, valid Loss : 0.2243\n",
            "Step : 270, valid Loss : 0.1942\n",
            "Step : 280, valid Loss : 0.2070\n",
            "Step : 290, valid Loss : 0.2912\n",
            "Step : 300, valid Loss : 0.2092\n",
            "Step : 310, valid Loss : 0.1975\n",
            "Step : 320, valid Loss : 0.2113\n",
            "Step : 330, valid Loss : 0.2523\n",
            "Step : 340, valid Loss : 0.2189\n",
            "Step : 350, valid Loss : 0.1997\n",
            "Step : 360, valid Loss : 0.2236\n",
            "Step : 370, valid Loss : 0.2173\n",
            "Step : 380, valid Loss : 0.1888\n",
            "Step : 390, valid Loss : 0.2353\n",
            "Step : 400, valid Loss : 0.2654\n",
            "Step : 410, valid Loss : 0.2137\n",
            "Step : 420, valid Loss : 0.2474\n",
            "Step : 430, valid Loss : 0.2073\n",
            "Step : 440, valid Loss : 0.2541\n",
            "Step : 450, valid Loss : 0.1683\n",
            "Step : 460, valid Loss : 0.2360\n",
            "total_valid_loss :  0.21843930845385168 val_f1_score :  91.95012078083175 val_pearsonr : 95.69617840001328\n",
            "Epoch 0 total_Valid Loss : 0.2184\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "Saving epoch 0 checkpoint at /content/drive/MyDrive/AI09/model_v4.ckpt.0\n",
            "****** Starting To Train Epoch #1 ******\n",
            "Epoch: 1, Step : 10, LR : 7.487333026255183e-06, Avg Loss : 0.2896\n",
            "Epoch: 1, Step : 20, LR : 7.4758175955780755e-06, Avg Loss : 0.1908\n",
            "Epoch: 1, Step : 30, LR : 7.464302164900968e-06, Avg Loss : 0.3523\n",
            "Epoch: 1, Step : 40, LR : 7.45278673422386e-06, Avg Loss : 0.2428\n",
            "Epoch: 1, Step : 50, LR : 7.441271303546753e-06, Avg Loss : 0.2435\n",
            "Epoch: 1, Step : 60, LR : 7.429755872869646e-06, Avg Loss : 0.2647\n",
            "Epoch: 1, Step : 70, LR : 7.418240442192539e-06, Avg Loss : 0.1644\n",
            "Epoch: 1, Step : 80, LR : 7.406725011515431e-06, Avg Loss : 0.2737\n",
            "Epoch: 1, Step : 90, LR : 7.395209580838324e-06, Avg Loss : 0.1503\n",
            "Epoch: 1, Step : 100, LR : 7.383694150161217e-06, Avg Loss : 0.1891\n",
            "Epoch: 1, Step : 110, LR : 7.372178719484109e-06, Avg Loss : 0.1740\n",
            "Epoch: 1, Step : 120, LR : 7.360663288807002e-06, Avg Loss : 0.3387\n",
            "Epoch: 1, Step : 130, LR : 7.3491478581298944e-06, Avg Loss : 0.2127\n",
            "Epoch: 1, Step : 140, LR : 7.337632427452788e-06, Avg Loss : 0.2510\n",
            "Epoch: 1, Step : 150, LR : 7.326116996775679e-06, Avg Loss : 0.2385\n",
            "Epoch: 1, Step : 160, LR : 7.314601566098573e-06, Avg Loss : 0.1600\n",
            "Epoch: 1, Step : 170, LR : 7.303086135421466e-06, Avg Loss : 0.3461\n",
            "Epoch: 1, Step : 180, LR : 7.291570704744358e-06, Avg Loss : 0.2408\n",
            "Epoch: 1, Step : 190, LR : 7.28005527406725e-06, Avg Loss : 0.2003\n",
            "Epoch: 1, Step : 200, LR : 7.268539843390143e-06, Avg Loss : 0.2904\n",
            "Epoch: 1, Step : 210, LR : 7.257024412713036e-06, Avg Loss : 0.3395\n",
            "Epoch: 1, Step : 220, LR : 7.2455089820359295e-06, Avg Loss : 0.2587\n",
            "Epoch: 1, Step : 230, LR : 7.2339935513588215e-06, Avg Loss : 0.2809\n",
            "Epoch: 1, Step : 240, LR : 7.222478120681714e-06, Avg Loss : 0.2399\n",
            "Epoch: 1, Step : 250, LR : 7.210962690004607e-06, Avg Loss : 0.2470\n",
            "Epoch: 1, Step : 260, LR : 7.199447259327499e-06, Avg Loss : 0.2471\n",
            "Epoch: 1, Step : 270, LR : 7.187931828650392e-06, Avg Loss : 0.3479\n",
            "Epoch: 1, Step : 280, LR : 7.176416397973285e-06, Avg Loss : 0.2073\n",
            "Epoch: 1, Step : 290, LR : 7.164900967296177e-06, Avg Loss : 0.2305\n",
            "Epoch: 1, Step : 300, LR : 7.153385536619069e-06, Avg Loss : 0.2431\n",
            "Epoch: 1, Step : 310, LR : 7.141870105941963e-06, Avg Loss : 0.2420\n",
            "Epoch: 1, Step : 320, LR : 7.130354675264856e-06, Avg Loss : 0.2447\n",
            "Epoch: 1, Step : 330, LR : 7.1188392445877485e-06, Avg Loss : 0.1902\n",
            "Epoch: 1, Step : 340, LR : 7.1073238139106405e-06, Avg Loss : 0.2317\n",
            "Epoch: 1, Step : 350, LR : 7.095808383233533e-06, Avg Loss : 0.3321\n",
            "Epoch: 1, Step : 360, LR : 7.084292952556426e-06, Avg Loss : 0.2183\n",
            "Epoch: 1, Step : 370, LR : 7.07277752187932e-06, Avg Loss : 0.1455\n",
            "Epoch: 1, Step : 380, LR : 7.061262091202211e-06, Avg Loss : 0.2582\n",
            "Epoch: 1, Step : 390, LR : 7.049746660525104e-06, Avg Loss : 0.2852\n",
            "Epoch: 1, Step : 400, LR : 7.038231229847997e-06, Avg Loss : 0.1963\n",
            "Epoch: 1, Step : 410, LR : 7.02671579917089e-06, Avg Loss : 0.1866\n",
            "Epoch: 1, Step : 420, LR : 7.015200368493782e-06, Avg Loss : 0.2021\n",
            "Epoch: 1, Step : 430, LR : 7.003684937816675e-06, Avg Loss : 0.2532\n",
            "Epoch: 1, Step : 440, LR : 6.9921695071395675e-06, Avg Loss : 0.1907\n",
            "Epoch: 1, Step : 450, LR : 6.9806540764624594e-06, Avg Loss : 0.3862\n",
            "Epoch: 1, Step : 460, LR : 6.969138645785353e-06, Avg Loss : 0.3315\n",
            "Epoch: 1, Step : 470, LR : 6.957623215108246e-06, Avg Loss : 0.2583\n",
            "Epoch: 1, Step : 480, LR : 6.946107784431139e-06, Avg Loss : 0.3691\n",
            "Epoch: 1, Step : 490, LR : 6.934592353754031e-06, Avg Loss : 0.2207\n",
            "Epoch: 1, Step : 500, LR : 6.923076923076923e-06, Avg Loss : 0.3311\n",
            "Epoch: 1, Step : 510, LR : 6.911561492399816e-06, Avg Loss : 0.2611\n",
            "Epoch: 1, Step : 520, LR : 6.90004606172271e-06, Avg Loss : 0.3228\n",
            "Epoch: 1, Step : 530, LR : 6.888530631045601e-06, Avg Loss : 0.2134\n",
            "Epoch: 1, Step : 540, LR : 6.8770152003684945e-06, Avg Loss : 0.3269\n",
            "Epoch: 1, Step : 550, LR : 6.865499769691387e-06, Avg Loss : 0.3187\n",
            "Epoch: 1, Step : 560, LR : 6.85398433901428e-06, Avg Loss : 0.1958\n",
            "Epoch: 1, Step : 570, LR : 6.842468908337172e-06, Avg Loss : 0.2605\n",
            "Epoch: 1, Step : 580, LR : 6.830953477660065e-06, Avg Loss : 0.2284\n",
            "Epoch: 1, Step : 590, LR : 6.819438046982958e-06, Avg Loss : 0.3841\n",
            "Epoch: 1, Step : 600, LR : 6.807922616305851e-06, Avg Loss : 0.2143\n",
            "Epoch: 1, Step : 610, LR : 6.796407185628743e-06, Avg Loss : 0.3255\n",
            "Epoch: 1, Step : 620, LR : 6.784891754951636e-06, Avg Loss : 0.1867\n",
            "Epoch: 1, Step : 630, LR : 6.773376324274529e-06, Avg Loss : 0.2246\n",
            "Epoch: 1, Step : 640, LR : 6.761860893597421e-06, Avg Loss : 0.2525\n",
            "Epoch: 1, Step : 650, LR : 6.7503454629203135e-06, Avg Loss : 0.2117\n",
            "Epoch: 1, Step : 660, LR : 6.738830032243206e-06, Avg Loss : 0.1733\n",
            "Epoch: 1, Step : 670, LR : 6.727314601566099e-06, Avg Loss : 0.2590\n",
            "Epoch: 1, Step : 680, LR : 6.715799170888991e-06, Avg Loss : 0.2289\n",
            "Epoch: 1, Step : 690, LR : 6.704283740211885e-06, Avg Loss : 0.2223\n",
            "Epoch: 1, Step : 700, LR : 6.6927683095347775e-06, Avg Loss : 0.2504\n",
            "Epoch: 1, Step : 710, LR : 6.68125287885767e-06, Avg Loss : 0.2636\n",
            "Epoch: 1, Step : 720, LR : 6.669737448180562e-06, Avg Loss : 0.2666\n",
            "Epoch: 1, Step : 730, LR : 6.658222017503455e-06, Avg Loss : 0.1782\n",
            "Epoch: 1, Step : 740, LR : 6.646706586826348e-06, Avg Loss : 0.3117\n",
            "Epoch: 1, Step : 750, LR : 6.635191156149241e-06, Avg Loss : 0.2733\n",
            "Epoch: 1, Step : 760, LR : 6.6236757254721325e-06, Avg Loss : 0.2389\n",
            "Epoch: 1, Step : 770, LR : 6.612160294795026e-06, Avg Loss : 0.3559\n",
            "Epoch: 1, Step : 780, LR : 6.600644864117919e-06, Avg Loss : 0.2046\n",
            "Epoch: 1, Step : 790, LR : 6.589129433440811e-06, Avg Loss : 0.2498\n",
            "Epoch: 1, Step : 800, LR : 6.577614002763704e-06, Avg Loss : 0.2512\n",
            "Epoch: 1, Step : 810, LR : 6.5660985720865964e-06, Avg Loss : 0.2127\n",
            "Epoch: 1, Step : 820, LR : 6.554583141409489e-06, Avg Loss : 0.2834\n",
            "Epoch: 1, Step : 830, LR : 6.543067710732381e-06, Avg Loss : 0.1985\n",
            "Epoch: 1, Step : 840, LR : 6.531552280055275e-06, Avg Loss : 0.2006\n",
            "Epoch: 1, Step : 850, LR : 6.520036849378168e-06, Avg Loss : 0.2158\n",
            "Epoch: 1, Step : 860, LR : 6.50852141870106e-06, Avg Loss : 0.2337\n",
            "Epoch: 1, Step : 870, LR : 6.497005988023952e-06, Avg Loss : 0.2723\n",
            "Epoch: 1, Step : 880, LR : 6.485490557346845e-06, Avg Loss : 0.3302\n",
            "Epoch: 1, Step : 890, LR : 6.473975126669738e-06, Avg Loss : 0.1697\n",
            "Epoch: 1, Step : 900, LR : 6.4624596959926315e-06, Avg Loss : 0.2340\n",
            "Epoch: 1, Step : 910, LR : 6.450944265315523e-06, Avg Loss : 0.2082\n",
            "Epoch: 1, Step : 920, LR : 6.439428834638416e-06, Avg Loss : 0.1870\n",
            "Epoch: 1, Step : 930, LR : 6.427913403961309e-06, Avg Loss : 0.2194\n",
            "Epoch: 1, Step : 940, LR : 6.416397973284202e-06, Avg Loss : 0.2479\n",
            "Epoch: 1, Step : 950, LR : 6.404882542607094e-06, Avg Loss : 0.2710\n",
            "Epoch: 1, Step : 960, LR : 6.3933671119299866e-06, Avg Loss : 0.2856\n",
            "Epoch: 1, Step : 970, LR : 6.381851681252879e-06, Avg Loss : 0.2449\n",
            "Epoch: 1, Step : 980, LR : 6.370336250575771e-06, Avg Loss : 0.3560\n",
            "Epoch: 1, Step : 990, LR : 6.358820819898665e-06, Avg Loss : 0.2541\n",
            "Epoch: 1, Step : 1000, LR : 6.347305389221558e-06, Avg Loss : 0.3571\n",
            "Epoch: 1, Step : 1010, LR : 6.3357899585444505e-06, Avg Loss : 0.3858\n",
            "Epoch: 1, Step : 1020, LR : 6.3242745278673424e-06, Avg Loss : 0.2831\n",
            "Epoch: 1, Step : 1030, LR : 6.312759097190235e-06, Avg Loss : 0.1797\n",
            "Epoch: 1, Step : 1040, LR : 6.301243666513128e-06, Avg Loss : 0.3137\n",
            "Epoch: 1, Step : 1050, LR : 6.289728235836021e-06, Avg Loss : 0.2353\n",
            "Epoch: 1, Step : 1060, LR : 6.278212805158913e-06, Avg Loss : 0.1494\n",
            "Epoch: 1, Step : 1070, LR : 6.266697374481806e-06, Avg Loss : 0.3120\n",
            "Epoch: 1, Step : 1080, LR : 6.255181943804699e-06, Avg Loss : 0.2645\n",
            "Epoch: 1, Step : 1090, LR : 6.243666513127592e-06, Avg Loss : 0.2680\n",
            "Epoch: 1, Step : 1100, LR : 6.232151082450484e-06, Avg Loss : 0.4134\n",
            "Epoch: 1, Step : 1110, LR : 6.220635651773377e-06, Avg Loss : 0.2230\n",
            "Epoch: 1, Step : 1120, LR : 6.2091202210962695e-06, Avg Loss : 0.3342\n",
            "Epoch: 1, Step : 1130, LR : 6.1976047904191614e-06, Avg Loss : 0.2650\n",
            "Epoch: 1, Step : 1140, LR : 6.186089359742054e-06, Avg Loss : 0.1954\n",
            "Epoch: 1, Step : 1150, LR : 6.174573929064948e-06, Avg Loss : 0.2681\n",
            "Epoch: 1, Step : 1160, LR : 6.163058498387841e-06, Avg Loss : 0.2266\n",
            "Epoch: 1, Step : 1170, LR : 6.151543067710733e-06, Avg Loss : 0.3593\n",
            "Epoch: 1, Step : 1180, LR : 6.140027637033625e-06, Avg Loss : 0.2717\n",
            "Epoch: 1, Step : 1190, LR : 6.128512206356518e-06, Avg Loss : 0.1540\n",
            "Epoch: 1, Step : 1200, LR : 6.116996775679411e-06, Avg Loss : 0.2494\n",
            "Epoch: 1, Step : 1210, LR : 6.105481345002303e-06, Avg Loss : 0.3766\n",
            "Epoch: 1, Step : 1220, LR : 6.0939659143251965e-06, Avg Loss : 0.1699\n",
            "Epoch: 1, Step : 1230, LR : 6.082450483648089e-06, Avg Loss : 0.1813\n",
            "Epoch: 1, Step : 1240, LR : 6.070935052970982e-06, Avg Loss : 0.2752\n",
            "Epoch: 1, Step : 1250, LR : 6.059419622293874e-06, Avg Loss : 0.2453\n",
            "Epoch: 1, Step : 1260, LR : 6.047904191616767e-06, Avg Loss : 0.2534\n",
            "Epoch: 1, Step : 1270, LR : 6.03638876093966e-06, Avg Loss : 0.2260\n",
            "Epoch: 1, Step : 1280, LR : 6.024873330262553e-06, Avg Loss : 0.4469\n",
            "Epoch: 1, Step : 1290, LR : 6.013357899585444e-06, Avg Loss : 0.3904\n",
            "Epoch: 1, Step : 1300, LR : 6.001842468908338e-06, Avg Loss : 0.2958\n",
            "Epoch: 1, Step : 1310, LR : 5.990327038231231e-06, Avg Loss : 0.1905\n",
            "Epoch: 1, Step : 1320, LR : 5.978811607554123e-06, Avg Loss : 0.2286\n",
            "Epoch: 1, Step : 1330, LR : 5.9672961768770155e-06, Avg Loss : 0.2615\n",
            "Epoch: 1, Step : 1340, LR : 5.955780746199908e-06, Avg Loss : 0.2399\n",
            "Epoch: 1, Step : 1350, LR : 5.944265315522801e-06, Avg Loss : 0.2173\n",
            "Epoch: 1, Step : 1360, LR : 5.932749884845693e-06, Avg Loss : 0.3090\n",
            "Epoch: 1, Step : 1370, LR : 5.921234454168586e-06, Avg Loss : 0.2030\n",
            "Epoch: 1, Step : 1380, LR : 5.9097190234914794e-06, Avg Loss : 0.2223\n",
            "Epoch: 1, Step : 1390, LR : 5.898203592814372e-06, Avg Loss : 0.2209\n",
            "Epoch: 1, Step : 1400, LR : 5.886688162137264e-06, Avg Loss : 0.1943\n",
            "Epoch: 1, Step : 1410, LR : 5.875172731460157e-06, Avg Loss : 0.2801\n",
            "Epoch: 1, Step : 1420, LR : 5.86365730078305e-06, Avg Loss : 0.1379\n",
            "Epoch: 1, Step : 1430, LR : 5.8521418701059425e-06, Avg Loss : 0.2181\n",
            "Epoch: 1, Step : 1440, LR : 5.8406264394288345e-06, Avg Loss : 0.2625\n",
            "Epoch: 1, Step : 1450, LR : 5.829111008751728e-06, Avg Loss : 0.2120\n",
            "Epoch: 1, Step : 1460, LR : 5.817595578074621e-06, Avg Loss : 0.2407\n",
            "Epoch: 1, Step : 1470, LR : 5.806080147397514e-06, Avg Loss : 0.2711\n",
            "Epoch: 1, Step : 1480, LR : 5.794564716720406e-06, Avg Loss : 0.3018\n",
            "Epoch: 1, Step : 1490, LR : 5.783049286043298e-06, Avg Loss : 0.2680\n",
            "Epoch: 1, Step : 1500, LR : 5.771533855366191e-06, Avg Loss : 0.2242\n",
            "Epoch: 1, Step : 1510, LR : 5.760018424689083e-06, Avg Loss : 0.3247\n",
            "Epoch: 1, Step : 1520, LR : 5.748502994011976e-06, Avg Loss : 0.1920\n",
            "Epoch: 1, Step : 1530, LR : 5.7369875633348696e-06, Avg Loss : 0.3637\n",
            "Epoch: 1, Step : 1540, LR : 5.725472132657762e-06, Avg Loss : 0.2215\n",
            "Epoch: 1, Step : 1550, LR : 5.713956701980654e-06, Avg Loss : 0.2245\n",
            "Epoch: 1, Step : 1560, LR : 5.702441271303547e-06, Avg Loss : 0.2397\n",
            "Epoch: 1, Step : 1570, LR : 5.69092584062644e-06, Avg Loss : 0.3617\n",
            "Epoch: 1, Step : 1580, LR : 5.679410409949333e-06, Avg Loss : 0.3264\n",
            "Epoch: 1, Step : 1590, LR : 5.667894979272225e-06, Avg Loss : 0.2116\n",
            "Epoch: 1, Step : 1600, LR : 5.656379548595118e-06, Avg Loss : 0.2047\n",
            "Epoch: 1, Step : 1610, LR : 5.644864117918011e-06, Avg Loss : 0.3780\n",
            "Epoch: 1, Step : 1620, LR : 5.633348687240904e-06, Avg Loss : 0.1843\n",
            "Epoch: 1, Step : 1630, LR : 5.621833256563796e-06, Avg Loss : 0.2861\n",
            "Epoch: 1, Step : 1640, LR : 5.6103178258866886e-06, Avg Loss : 0.2494\n",
            "Epoch: 1, Step : 1650, LR : 5.598802395209581e-06, Avg Loss : 0.1574\n",
            "Epoch: 1, Step : 1660, LR : 5.587286964532473e-06, Avg Loss : 0.1180\n",
            "Epoch: 1, Step : 1670, LR : 5.575771533855366e-06, Avg Loss : 0.2613\n",
            "Epoch: 1, Step : 1680, LR : 5.56425610317826e-06, Avg Loss : 0.1997\n",
            "Epoch: 1, Step : 1690, LR : 5.5527406725011525e-06, Avg Loss : 0.2559\n",
            "Epoch: 1, Step : 1700, LR : 5.5412252418240444e-06, Avg Loss : 0.1988\n",
            "Epoch: 1, Step : 1710, LR : 5.529709811146937e-06, Avg Loss : 0.3355\n",
            "Epoch: 1, Step : 1720, LR : 5.51819438046983e-06, Avg Loss : 0.2637\n",
            "Epoch: 1, Step : 1730, LR : 5.506678949792723e-06, Avg Loss : 0.2657\n",
            "Epoch: 1, Step : 1740, LR : 5.495163519115615e-06, Avg Loss : 0.1944\n",
            "Epoch: 1, Step : 1750, LR : 5.4836480884385075e-06, Avg Loss : 0.2508\n",
            "Epoch: 1, Step : 1760, LR : 5.472132657761401e-06, Avg Loss : 0.2114\n",
            "Epoch: 1, Step : 1770, LR : 5.460617227084294e-06, Avg Loss : 0.2356\n",
            "Epoch: 1, Step : 1780, LR : 5.449101796407186e-06, Avg Loss : 0.2578\n",
            "Epoch: 1, Step : 1790, LR : 5.437586365730079e-06, Avg Loss : 0.2855\n",
            "Epoch: 1, Step : 1800, LR : 5.4260709350529715e-06, Avg Loss : 0.1909\n",
            "Epoch: 1, Step : 1810, LR : 5.414555504375864e-06, Avg Loss : 0.2181\n",
            "Epoch: 1, Step : 1820, LR : 5.403040073698756e-06, Avg Loss : 0.3534\n",
            "Epoch: 1, Step : 1830, LR : 5.39152464302165e-06, Avg Loss : 0.3367\n",
            "Epoch: 1, Step : 1840, LR : 5.380009212344543e-06, Avg Loss : 0.1903\n",
            "Epoch: 1, Step : 1850, LR : 5.3684937816674346e-06, Avg Loss : 0.2554\n",
            "Epoch: 1, Step : 1860, LR : 5.356978350990327e-06, Avg Loss : 0.3500\n",
            "Epoch: 1, Step : 1870, LR : 5.34546292031322e-06, Avg Loss : 0.2694\n",
            "Epoch: 1, Step : 1880, LR : 5.333947489636113e-06, Avg Loss : 0.2854\n",
            "Epoch: 1, Step : 1890, LR : 5.322432058959005e-06, Avg Loss : 0.3161\n",
            "Epoch: 1, Step : 1900, LR : 5.310916628281898e-06, Avg Loss : 0.2105\n",
            "Epoch: 1, Step : 1910, LR : 5.299401197604791e-06, Avg Loss : 0.2072\n",
            "Epoch: 1, Step : 1920, LR : 5.287885766927684e-06, Avg Loss : 0.2480\n",
            "Epoch: 1, Step : 1930, LR : 5.276370336250576e-06, Avg Loss : 0.3311\n",
            "Epoch: 1, Step : 1940, LR : 5.264854905573469e-06, Avg Loss : 0.1961\n",
            "Epoch: 1, Step : 1950, LR : 5.253339474896362e-06, Avg Loss : 0.2606\n",
            "Epoch: 1, Step : 1960, LR : 5.241824044219254e-06, Avg Loss : 0.3219\n",
            "Epoch: 1, Step : 1970, LR : 5.230308613542146e-06, Avg Loss : 0.1811\n",
            "Epoch: 1, Step : 1980, LR : 5.21879318286504e-06, Avg Loss : 0.2783\n",
            "Epoch: 1, Step : 1990, LR : 5.207277752187933e-06, Avg Loss : 0.3297\n",
            "Epoch: 1, Step : 2000, LR : 5.195762321510825e-06, Avg Loss : 0.3045\n",
            "Epoch: 1, Step : 2010, LR : 5.1842468908337175e-06, Avg Loss : 0.1317\n",
            "Epoch: 1, Step : 2020, LR : 5.17273146015661e-06, Avg Loss : 0.3232\n",
            "Epoch: 1, Step : 2030, LR : 5.161216029479503e-06, Avg Loss : 0.3038\n",
            "Epoch: 1, Step : 2040, LR : 5.149700598802395e-06, Avg Loss : 0.3397\n",
            "Epoch: 1, Step : 2050, LR : 5.138185168125288e-06, Avg Loss : 0.2377\n",
            "Epoch: 1, Step : 2060, LR : 5.1266697374481814e-06, Avg Loss : 0.3396\n",
            "Epoch: 1, Step : 2070, LR : 5.115154306771074e-06, Avg Loss : 0.1986\n",
            "Epoch: 1, Step : 2080, LR : 5.103638876093966e-06, Avg Loss : 0.1582\n",
            "Epoch: 1, Step : 2090, LR : 5.092123445416859e-06, Avg Loss : 0.1438\n",
            "Epoch: 1, Step : 2100, LR : 5.080608014739752e-06, Avg Loss : 0.1869\n",
            "Epoch: 1, Step : 2110, LR : 5.0690925840626445e-06, Avg Loss : 0.4066\n",
            "Epoch: 1, Step : 2120, LR : 5.0575771533855365e-06, Avg Loss : 0.1973\n",
            "Epoch: 1, Step : 2130, LR : 5.046061722708429e-06, Avg Loss : 0.2004\n",
            "Epoch: 1, Step : 2140, LR : 5.034546292031323e-06, Avg Loss : 0.1385\n",
            "Epoch: 1, Step : 2150, LR : 5.023030861354216e-06, Avg Loss : 0.3137\n",
            "Epoch: 1, Step : 2160, LR : 5.011515430677108e-06, Avg Loss : 0.2999\n",
            "Epoch: 1, Step : 2170, LR : 5e-06, Avg Loss : 0.1732\n",
            "Epoch 1 total_train_loss : 0.2538\n",
            "***** Finish To Train Epoch 1 *****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Step : 10, valid Loss : 0.1513\n",
            "Step : 20, valid Loss : 0.1991\n",
            "Step : 30, valid Loss : 0.2019\n",
            "Step : 40, valid Loss : 0.1350\n",
            "Step : 50, valid Loss : 0.1591\n",
            "Step : 60, valid Loss : 0.1443\n",
            "Step : 70, valid Loss : 0.1645\n",
            "Step : 80, valid Loss : 0.1508\n",
            "Step : 90, valid Loss : 0.1692\n",
            "Step : 100, valid Loss : 0.1517\n",
            "Step : 110, valid Loss : 0.1560\n",
            "Step : 120, valid Loss : 0.1494\n",
            "Step : 130, valid Loss : 0.1615\n",
            "Step : 140, valid Loss : 0.1751\n",
            "Step : 150, valid Loss : 0.1729\n",
            "Step : 160, valid Loss : 0.1423\n",
            "Step : 170, valid Loss : 0.1644\n",
            "Step : 180, valid Loss : 0.1920\n",
            "Step : 190, valid Loss : 0.1506\n",
            "Step : 200, valid Loss : 0.1555\n",
            "Step : 210, valid Loss : 0.1608\n",
            "Step : 220, valid Loss : 0.1500\n",
            "Step : 230, valid Loss : 0.1524\n",
            "Step : 240, valid Loss : 0.1368\n",
            "Step : 250, valid Loss : 0.1412\n",
            "Step : 260, valid Loss : 0.1491\n",
            "Step : 270, valid Loss : 0.1639\n",
            "Step : 280, valid Loss : 0.1922\n",
            "Step : 290, valid Loss : 0.1823\n",
            "Step : 300, valid Loss : 0.1652\n",
            "Step : 310, valid Loss : 0.1418\n",
            "Step : 320, valid Loss : 0.1738\n",
            "Step : 330, valid Loss : 0.1783\n",
            "Step : 340, valid Loss : 0.1380\n",
            "Step : 350, valid Loss : 0.1786\n",
            "Step : 360, valid Loss : 0.1379\n",
            "Step : 370, valid Loss : 0.1705\n",
            "Step : 380, valid Loss : 0.1679\n",
            "Step : 390, valid Loss : 0.1425\n",
            "Step : 400, valid Loss : 0.1352\n",
            "Step : 410, valid Loss : 0.1372\n",
            "Step : 420, valid Loss : 0.1671\n",
            "Step : 430, valid Loss : 0.1446\n",
            "Step : 440, valid Loss : 0.1831\n",
            "Step : 450, valid Loss : 0.1460\n",
            "Step : 460, valid Loss : 0.1388\n",
            "total_valid_loss :  0.1593213453888893 val_f1_score :  94.14559172837782 val_pearsonr : 96.82931799199666\n",
            "Epoch 1 total_Valid Loss : 0.1593\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "Saving epoch 1 checkpoint at /content/drive/MyDrive/AI09/model_v4.ckpt.1\n",
            "****** Starting To Train Epoch #2 ******\n",
            "Epoch: 2, Step : 10, LR : 4.987333026255182e-06, Avg Loss : 0.1641\n",
            "Epoch: 2, Step : 20, LR : 4.975817595578075e-06, Avg Loss : 0.1851\n",
            "Epoch: 2, Step : 30, LR : 4.964302164900968e-06, Avg Loss : 0.2388\n",
            "Epoch: 2, Step : 40, LR : 4.95278673422386e-06, Avg Loss : 0.1675\n",
            "Epoch: 2, Step : 50, LR : 4.941271303546753e-06, Avg Loss : 0.1914\n",
            "Epoch: 2, Step : 60, LR : 4.929755872869646e-06, Avg Loss : 0.1812\n",
            "Epoch: 2, Step : 70, LR : 4.918240442192539e-06, Avg Loss : 0.1201\n",
            "Epoch: 2, Step : 80, LR : 4.9067250115154316e-06, Avg Loss : 0.1743\n",
            "Epoch: 2, Step : 90, LR : 4.8952095808383235e-06, Avg Loss : 0.1613\n",
            "Epoch: 2, Step : 100, LR : 4.883694150161216e-06, Avg Loss : 0.1774\n",
            "Epoch: 2, Step : 110, LR : 4.872178719484109e-06, Avg Loss : 0.1382\n",
            "Epoch: 2, Step : 120, LR : 4.860663288807002e-06, Avg Loss : 0.1453\n",
            "Epoch: 2, Step : 130, LR : 4.849147858129895e-06, Avg Loss : 0.1147\n",
            "Epoch: 2, Step : 140, LR : 4.837632427452787e-06, Avg Loss : 0.2024\n",
            "Epoch: 2, Step : 150, LR : 4.82611699677568e-06, Avg Loss : 0.2696\n",
            "Epoch: 2, Step : 160, LR : 4.814601566098572e-06, Avg Loss : 0.2999\n",
            "Epoch: 2, Step : 170, LR : 4.803086135421465e-06, Avg Loss : 0.1469\n",
            "Epoch: 2, Step : 180, LR : 4.791570704744358e-06, Avg Loss : 0.1903\n",
            "Epoch: 2, Step : 190, LR : 4.7800552740672505e-06, Avg Loss : 0.2766\n",
            "Epoch: 2, Step : 200, LR : 4.768539843390143e-06, Avg Loss : 0.1260\n",
            "Epoch: 2, Step : 210, LR : 4.757024412713036e-06, Avg Loss : 0.1653\n",
            "Epoch: 2, Step : 220, LR : 4.745508982035928e-06, Avg Loss : 0.1306\n",
            "Epoch: 2, Step : 230, LR : 4.733993551358822e-06, Avg Loss : 0.1598\n",
            "Epoch: 2, Step : 240, LR : 4.722478120681714e-06, Avg Loss : 0.1784\n",
            "Epoch: 2, Step : 250, LR : 4.7109626900046064e-06, Avg Loss : 0.1805\n",
            "Epoch: 2, Step : 260, LR : 4.699447259327499e-06, Avg Loss : 0.2516\n",
            "Epoch: 2, Step : 270, LR : 4.687931828650392e-06, Avg Loss : 0.1439\n",
            "Epoch: 2, Step : 280, LR : 4.676416397973285e-06, Avg Loss : 0.1977\n",
            "Epoch: 2, Step : 290, LR : 4.664900967296177e-06, Avg Loss : 0.1413\n",
            "Epoch: 2, Step : 300, LR : 4.65338553661907e-06, Avg Loss : 0.1330\n",
            "Epoch: 2, Step : 310, LR : 4.641870105941962e-06, Avg Loss : 0.1729\n",
            "Epoch: 2, Step : 320, LR : 4.630354675264855e-06, Avg Loss : 0.1173\n",
            "Epoch: 2, Step : 330, LR : 4.618839244587748e-06, Avg Loss : 0.1420\n",
            "Epoch: 2, Step : 340, LR : 4.607323813910641e-06, Avg Loss : 0.1406\n",
            "Epoch: 2, Step : 350, LR : 4.5958083832335335e-06, Avg Loss : 0.1355\n",
            "Epoch: 2, Step : 360, LR : 4.584292952556426e-06, Avg Loss : 0.1716\n",
            "Epoch: 2, Step : 370, LR : 4.572777521879318e-06, Avg Loss : 0.1775\n",
            "Epoch: 2, Step : 380, LR : 4.561262091202212e-06, Avg Loss : 0.1584\n",
            "Epoch: 2, Step : 390, LR : 4.549746660525104e-06, Avg Loss : 0.1847\n",
            "Epoch: 2, Step : 400, LR : 4.5382312298479966e-06, Avg Loss : 0.1404\n",
            "Epoch: 2, Step : 410, LR : 4.526715799170889e-06, Avg Loss : 0.1209\n",
            "Epoch: 2, Step : 420, LR : 4.515200368493782e-06, Avg Loss : 0.3555\n",
            "Epoch: 2, Step : 430, LR : 4.503684937816675e-06, Avg Loss : 0.1514\n",
            "Epoch: 2, Step : 440, LR : 4.492169507139567e-06, Avg Loss : 0.1507\n",
            "Epoch: 2, Step : 450, LR : 4.4806540764624605e-06, Avg Loss : 0.2176\n",
            "Epoch: 2, Step : 460, LR : 4.4691386457853524e-06, Avg Loss : 0.1634\n",
            "Epoch: 2, Step : 470, LR : 4.457623215108245e-06, Avg Loss : 0.1585\n",
            "Epoch: 2, Step : 480, LR : 4.446107784431138e-06, Avg Loss : 0.1429\n",
            "Epoch: 2, Step : 490, LR : 4.434592353754031e-06, Avg Loss : 0.1454\n",
            "Epoch: 2, Step : 500, LR : 4.423076923076924e-06, Avg Loss : 0.1958\n",
            "Epoch: 2, Step : 510, LR : 4.411561492399816e-06, Avg Loss : 0.1449\n",
            "Epoch: 2, Step : 520, LR : 4.400046061722708e-06, Avg Loss : 0.2453\n",
            "Epoch: 2, Step : 530, LR : 4.388530631045602e-06, Avg Loss : 0.1792\n",
            "Epoch: 2, Step : 540, LR : 4.377015200368494e-06, Avg Loss : 0.1627\n",
            "Epoch: 2, Step : 550, LR : 4.365499769691387e-06, Avg Loss : 0.2534\n",
            "Epoch: 2, Step : 560, LR : 4.3539843390142795e-06, Avg Loss : 0.1898\n",
            "Epoch: 2, Step : 570, LR : 4.342468908337172e-06, Avg Loss : 0.2301\n",
            "Epoch: 2, Step : 580, LR : 4.330953477660065e-06, Avg Loss : 0.1561\n",
            "Epoch: 2, Step : 590, LR : 4.319438046982958e-06, Avg Loss : 0.1780\n",
            "Epoch: 2, Step : 600, LR : 4.30792261630585e-06, Avg Loss : 0.1316\n",
            "Epoch: 2, Step : 610, LR : 4.296407185628743e-06, Avg Loss : 0.1357\n",
            "Epoch: 2, Step : 620, LR : 4.284891754951635e-06, Avg Loss : 0.1183\n",
            "Epoch: 2, Step : 630, LR : 4.273376324274528e-06, Avg Loss : 0.2408\n",
            "Epoch: 2, Step : 640, LR : 4.261860893597421e-06, Avg Loss : 0.1310\n",
            "Epoch: 2, Step : 650, LR : 4.250345462920314e-06, Avg Loss : 0.2003\n",
            "Epoch: 2, Step : 660, LR : 4.2388300322432065e-06, Avg Loss : 0.1585\n",
            "Epoch: 2, Step : 670, LR : 4.2273146015660985e-06, Avg Loss : 0.1522\n",
            "Epoch: 2, Step : 680, LR : 4.215799170888992e-06, Avg Loss : 0.1160\n",
            "Epoch: 2, Step : 690, LR : 4.204283740211884e-06, Avg Loss : 0.1366\n",
            "Epoch: 2, Step : 700, LR : 4.192768309534777e-06, Avg Loss : 0.1422\n",
            "Epoch: 2, Step : 710, LR : 4.18125287885767e-06, Avg Loss : 0.1112\n",
            "Epoch: 2, Step : 720, LR : 4.169737448180562e-06, Avg Loss : 0.1940\n",
            "Epoch: 2, Step : 730, LR : 4.158222017503455e-06, Avg Loss : 0.1303\n",
            "Epoch: 2, Step : 740, LR : 4.146706586826348e-06, Avg Loss : 0.1445\n",
            "Epoch: 2, Step : 750, LR : 4.13519115614924e-06, Avg Loss : 0.1764\n",
            "Epoch: 2, Step : 760, LR : 4.1236757254721336e-06, Avg Loss : 0.2031\n",
            "Epoch: 2, Step : 770, LR : 4.1121602947950255e-06, Avg Loss : 0.1029\n",
            "Epoch: 2, Step : 780, LR : 4.100644864117918e-06, Avg Loss : 0.1928\n",
            "Epoch: 2, Step : 790, LR : 4.089129433440811e-06, Avg Loss : 0.1540\n",
            "Epoch: 2, Step : 800, LR : 4.077614002763704e-06, Avg Loss : 0.1890\n",
            "Epoch: 2, Step : 810, LR : 4.066098572086597e-06, Avg Loss : 0.2262\n",
            "Epoch: 2, Step : 820, LR : 4.054583141409489e-06, Avg Loss : 0.1482\n",
            "Epoch: 2, Step : 830, LR : 4.043067710732381e-06, Avg Loss : 0.1329\n",
            "Epoch: 2, Step : 840, LR : 4.031552280055274e-06, Avg Loss : 0.1287\n",
            "Epoch: 2, Step : 850, LR : 4.020036849378167e-06, Avg Loss : 0.1065\n",
            "Epoch: 2, Step : 860, LR : 4.00852141870106e-06, Avg Loss : 0.1998\n",
            "Epoch: 2, Step : 870, LR : 3.9970059880239525e-06, Avg Loss : 0.1425\n",
            "Epoch: 2, Step : 880, LR : 3.985490557346845e-06, Avg Loss : 0.1395\n",
            "Epoch: 2, Step : 890, LR : 3.973975126669738e-06, Avg Loss : 0.1775\n",
            "Epoch: 2, Step : 900, LR : 3.96245969599263e-06, Avg Loss : 0.1390\n",
            "Epoch: 2, Step : 910, LR : 3.950944265315524e-06, Avg Loss : 0.1138\n",
            "Epoch: 2, Step : 920, LR : 3.939428834638416e-06, Avg Loss : 0.1561\n",
            "Epoch: 2, Step : 930, LR : 3.927913403961308e-06, Avg Loss : 0.1726\n",
            "Epoch: 2, Step : 940, LR : 3.916397973284201e-06, Avg Loss : 0.1548\n",
            "Epoch: 2, Step : 950, LR : 3.904882542607094e-06, Avg Loss : 0.1567\n",
            "Epoch: 2, Step : 960, LR : 3.893367111929987e-06, Avg Loss : 0.1815\n",
            "Epoch: 2, Step : 970, LR : 3.881851681252879e-06, Avg Loss : 0.1812\n",
            "Epoch: 2, Step : 980, LR : 3.8703362505757715e-06, Avg Loss : 0.1073\n",
            "Epoch: 2, Step : 990, LR : 3.858820819898664e-06, Avg Loss : 0.1375\n",
            "Epoch: 2, Step : 1000, LR : 3.847305389221557e-06, Avg Loss : 0.2579\n",
            "Epoch: 2, Step : 1010, LR : 3.83578995854445e-06, Avg Loss : 0.1762\n",
            "Epoch: 2, Step : 1020, LR : 3.824274527867343e-06, Avg Loss : 0.1494\n",
            "Epoch: 2, Step : 1030, LR : 3.812759097190235e-06, Avg Loss : 0.1824\n",
            "Epoch: 2, Step : 1040, LR : 3.8012436665131282e-06, Avg Loss : 0.1916\n",
            "Epoch: 2, Step : 1050, LR : 3.7897282358360206e-06, Avg Loss : 0.2727\n",
            "Epoch: 2, Step : 1060, LR : 3.7782128051589134e-06, Avg Loss : 0.1728\n",
            "Epoch: 2, Step : 1070, LR : 3.7666973744818058e-06, Avg Loss : 0.0991\n",
            "Epoch: 2, Step : 1080, LR : 3.755181943804699e-06, Avg Loss : 0.2068\n",
            "Epoch: 2, Step : 1090, LR : 3.7436665131275913e-06, Avg Loss : 0.2494\n",
            "Epoch: 2, Step : 1100, LR : 3.732151082450484e-06, Avg Loss : 0.1721\n",
            "Epoch: 2, Step : 1110, LR : 3.7206356517733765e-06, Avg Loss : 0.2338\n",
            "Epoch: 2, Step : 1120, LR : 3.7091202210962697e-06, Avg Loss : 0.1428\n",
            "Epoch: 2, Step : 1130, LR : 3.697604790419162e-06, Avg Loss : 0.1891\n",
            "Epoch: 2, Step : 1140, LR : 3.6860893597420544e-06, Avg Loss : 0.1225\n",
            "Epoch: 2, Step : 1150, LR : 3.6745739290649472e-06, Avg Loss : 0.3933\n",
            "Epoch: 2, Step : 1160, LR : 3.6630584983878396e-06, Avg Loss : 0.2034\n",
            "Epoch: 2, Step : 1170, LR : 3.651543067710733e-06, Avg Loss : 0.1744\n",
            "Epoch: 2, Step : 1180, LR : 3.640027637033625e-06, Avg Loss : 0.1188\n",
            "Epoch: 2, Step : 1190, LR : 3.628512206356518e-06, Avg Loss : 0.1558\n",
            "Epoch: 2, Step : 1200, LR : 3.6169967756794107e-06, Avg Loss : 0.3056\n",
            "Epoch: 2, Step : 1210, LR : 3.6054813450023035e-06, Avg Loss : 0.1364\n",
            "Epoch: 2, Step : 1220, LR : 3.593965914325196e-06, Avg Loss : 0.1552\n",
            "Epoch: 2, Step : 1230, LR : 3.5824504836480887e-06, Avg Loss : 0.1619\n",
            "Epoch: 2, Step : 1240, LR : 3.5709350529709815e-06, Avg Loss : 0.1412\n",
            "Epoch: 2, Step : 1250, LR : 3.5594196222938743e-06, Avg Loss : 0.1783\n",
            "Epoch: 2, Step : 1260, LR : 3.5479041916167666e-06, Avg Loss : 0.2140\n",
            "Epoch: 2, Step : 1270, LR : 3.53638876093966e-06, Avg Loss : 0.1956\n",
            "Epoch: 2, Step : 1280, LR : 3.524873330262552e-06, Avg Loss : 0.1359\n",
            "Epoch: 2, Step : 1290, LR : 3.513357899585445e-06, Avg Loss : 0.1683\n",
            "Epoch: 2, Step : 1300, LR : 3.5018424689083374e-06, Avg Loss : 0.1547\n",
            "Epoch: 2, Step : 1310, LR : 3.4903270382312297e-06, Avg Loss : 0.1760\n",
            "Epoch: 2, Step : 1320, LR : 3.478811607554123e-06, Avg Loss : 0.1232\n",
            "Epoch: 2, Step : 1330, LR : 3.4672961768770153e-06, Avg Loss : 0.1471\n",
            "Epoch: 2, Step : 1340, LR : 3.455780746199908e-06, Avg Loss : 0.2278\n",
            "Epoch: 2, Step : 1350, LR : 3.4442653155228005e-06, Avg Loss : 0.1701\n",
            "Epoch: 2, Step : 1360, LR : 3.4327498848456937e-06, Avg Loss : 0.2353\n",
            "Epoch: 2, Step : 1370, LR : 3.421234454168586e-06, Avg Loss : 0.1611\n",
            "Epoch: 2, Step : 1380, LR : 3.409719023491479e-06, Avg Loss : 0.1879\n",
            "Epoch: 2, Step : 1390, LR : 3.3982035928143716e-06, Avg Loss : 0.1796\n",
            "Epoch: 2, Step : 1400, LR : 3.3866881621372644e-06, Avg Loss : 0.1635\n",
            "Epoch: 2, Step : 1410, LR : 3.3751727314601568e-06, Avg Loss : 0.1163\n",
            "Epoch: 2, Step : 1420, LR : 3.3636573007830495e-06, Avg Loss : 0.1641\n",
            "Epoch: 2, Step : 1430, LR : 3.3521418701059423e-06, Avg Loss : 0.2320\n",
            "Epoch: 2, Step : 1440, LR : 3.340626439428835e-06, Avg Loss : 0.1906\n",
            "Epoch: 2, Step : 1450, LR : 3.3291110087517275e-06, Avg Loss : 0.1307\n",
            "Epoch: 2, Step : 1460, LR : 3.3175955780746207e-06, Avg Loss : 0.1942\n",
            "Epoch: 2, Step : 1470, LR : 3.306080147397513e-06, Avg Loss : 0.1556\n",
            "Epoch: 2, Step : 1480, LR : 3.2945647167204054e-06, Avg Loss : 0.1618\n",
            "Epoch: 2, Step : 1490, LR : 3.2830492860432982e-06, Avg Loss : 0.2504\n",
            "Epoch: 2, Step : 1500, LR : 3.2715338553661906e-06, Avg Loss : 0.1786\n",
            "Epoch: 2, Step : 1510, LR : 3.260018424689084e-06, Avg Loss : 0.1486\n",
            "Epoch: 2, Step : 1520, LR : 3.248502994011976e-06, Avg Loss : 0.1157\n",
            "Epoch: 2, Step : 1530, LR : 3.236987563334869e-06, Avg Loss : 0.1476\n",
            "Epoch: 2, Step : 1540, LR : 3.2254721326577613e-06, Avg Loss : 0.1777\n",
            "Epoch: 2, Step : 1550, LR : 3.2139567019806545e-06, Avg Loss : 0.2072\n",
            "Epoch: 2, Step : 1560, LR : 3.202441271303547e-06, Avg Loss : 0.1277\n",
            "Epoch: 2, Step : 1570, LR : 3.1909258406264397e-06, Avg Loss : 0.1946\n",
            "Epoch: 2, Step : 1580, LR : 3.1794104099493325e-06, Avg Loss : 0.1838\n",
            "Epoch: 2, Step : 1590, LR : 3.1678949792722253e-06, Avg Loss : 0.1654\n",
            "Epoch: 2, Step : 1600, LR : 3.1563795485951176e-06, Avg Loss : 0.1220\n",
            "Epoch: 2, Step : 1610, LR : 3.1448641179180104e-06, Avg Loss : 0.1350\n",
            "Epoch: 2, Step : 1620, LR : 3.133348687240903e-06, Avg Loss : 0.1870\n",
            "Epoch: 2, Step : 1630, LR : 3.121833256563796e-06, Avg Loss : 0.1736\n",
            "Epoch: 2, Step : 1640, LR : 3.1103178258866883e-06, Avg Loss : 0.2005\n",
            "Epoch: 2, Step : 1650, LR : 3.0988023952095807e-06, Avg Loss : 0.1784\n",
            "Epoch: 2, Step : 1660, LR : 3.087286964532474e-06, Avg Loss : 0.1430\n",
            "Epoch: 2, Step : 1670, LR : 3.0757715338553663e-06, Avg Loss : 0.1479\n",
            "Epoch: 2, Step : 1680, LR : 3.064256103178259e-06, Avg Loss : 0.2276\n",
            "Epoch: 2, Step : 1690, LR : 3.0527406725011514e-06, Avg Loss : 0.2093\n",
            "Epoch: 2, Step : 1700, LR : 3.0412252418240447e-06, Avg Loss : 0.2993\n",
            "Epoch: 2, Step : 1710, LR : 3.029709811146937e-06, Avg Loss : 0.1845\n",
            "Epoch: 2, Step : 1720, LR : 3.01819438046983e-06, Avg Loss : 0.1170\n",
            "Epoch: 2, Step : 1730, LR : 3.006678949792722e-06, Avg Loss : 0.1676\n",
            "Epoch: 2, Step : 1740, LR : 2.9951635191156154e-06, Avg Loss : 0.1409\n",
            "Epoch: 2, Step : 1750, LR : 2.9836480884385078e-06, Avg Loss : 0.0894\n",
            "Epoch: 2, Step : 1760, LR : 2.9721326577614005e-06, Avg Loss : 0.1269\n",
            "Epoch: 2, Step : 1770, LR : 2.960617227084293e-06, Avg Loss : 0.1585\n",
            "Epoch: 2, Step : 1780, LR : 2.949101796407186e-06, Avg Loss : 0.1810\n",
            "Epoch: 2, Step : 1790, LR : 2.9375863657300785e-06, Avg Loss : 0.1974\n",
            "Epoch: 2, Step : 1800, LR : 2.9260709350529713e-06, Avg Loss : 0.1843\n",
            "Epoch: 2, Step : 1810, LR : 2.914555504375864e-06, Avg Loss : 0.1944\n",
            "Epoch: 2, Step : 1820, LR : 2.903040073698757e-06, Avg Loss : 0.2673\n",
            "Epoch: 2, Step : 1830, LR : 2.891524643021649e-06, Avg Loss : 0.1757\n",
            "Epoch: 2, Step : 1840, LR : 2.8800092123445416e-06, Avg Loss : 0.1558\n",
            "Epoch: 2, Step : 1850, LR : 2.8684937816674348e-06, Avg Loss : 0.1695\n",
            "Epoch: 2, Step : 1860, LR : 2.856978350990327e-06, Avg Loss : 0.1775\n",
            "Epoch: 2, Step : 1870, LR : 2.84546292031322e-06, Avg Loss : 0.1534\n",
            "Epoch: 2, Step : 1880, LR : 2.8339474896361123e-06, Avg Loss : 0.0840\n",
            "Epoch: 2, Step : 1890, LR : 2.8224320589590055e-06, Avg Loss : 0.1906\n",
            "Epoch: 2, Step : 1900, LR : 2.810916628281898e-06, Avg Loss : 0.2028\n",
            "Epoch: 2, Step : 1910, LR : 2.7994011976047907e-06, Avg Loss : 0.2108\n",
            "Epoch: 2, Step : 1920, LR : 2.787885766927683e-06, Avg Loss : 0.1725\n",
            "Epoch: 2, Step : 1930, LR : 2.7763703362505762e-06, Avg Loss : 0.2088\n",
            "Epoch: 2, Step : 1940, LR : 2.7648549055734686e-06, Avg Loss : 0.1137\n",
            "Epoch: 2, Step : 1950, LR : 2.7533394748963614e-06, Avg Loss : 0.1729\n",
            "Epoch: 2, Step : 1960, LR : 2.7418240442192538e-06, Avg Loss : 0.2654\n",
            "Epoch: 2, Step : 1970, LR : 2.730308613542147e-06, Avg Loss : 0.1721\n",
            "Epoch: 2, Step : 1980, LR : 2.7187931828650393e-06, Avg Loss : 0.1571\n",
            "Epoch: 2, Step : 1990, LR : 2.707277752187932e-06, Avg Loss : 0.1289\n",
            "Epoch: 2, Step : 2000, LR : 2.695762321510825e-06, Avg Loss : 0.1426\n",
            "Epoch: 2, Step : 2010, LR : 2.6842468908337173e-06, Avg Loss : 0.1937\n",
            "Epoch: 2, Step : 2020, LR : 2.67273146015661e-06, Avg Loss : 0.2006\n",
            "Epoch: 2, Step : 2030, LR : 2.6612160294795024e-06, Avg Loss : 0.1361\n",
            "Epoch: 2, Step : 2040, LR : 2.6497005988023956e-06, Avg Loss : 0.1837\n",
            "Epoch: 2, Step : 2050, LR : 2.638185168125288e-06, Avg Loss : 0.2450\n",
            "Epoch: 2, Step : 2060, LR : 2.626669737448181e-06, Avg Loss : 0.1258\n",
            "Epoch: 2, Step : 2070, LR : 2.615154306771073e-06, Avg Loss : 0.1288\n",
            "Epoch: 2, Step : 2080, LR : 2.6036388760939664e-06, Avg Loss : 0.2395\n",
            "Epoch: 2, Step : 2090, LR : 2.5921234454168587e-06, Avg Loss : 0.1300\n",
            "Epoch: 2, Step : 2100, LR : 2.5806080147397515e-06, Avg Loss : 0.1560\n",
            "Epoch: 2, Step : 2110, LR : 2.569092584062644e-06, Avg Loss : 0.1248\n",
            "Epoch: 2, Step : 2120, LR : 2.557577153385537e-06, Avg Loss : 0.1132\n",
            "Epoch: 2, Step : 2130, LR : 2.5460617227084295e-06, Avg Loss : 0.1021\n",
            "Epoch: 2, Step : 2140, LR : 2.5345462920313223e-06, Avg Loss : 0.2358\n",
            "Epoch: 2, Step : 2150, LR : 2.5230308613542146e-06, Avg Loss : 0.0747\n",
            "Epoch: 2, Step : 2160, LR : 2.511515430677108e-06, Avg Loss : 0.1255\n",
            "Epoch: 2, Step : 2170, LR : 2.5e-06, Avg Loss : 0.1389\n",
            "Epoch 2 total_train_loss : 0.1713\n",
            "***** Finish To Train Epoch 2 *****\n",
            "\n",
            "*****Epoch 2 Valid Start*****\n",
            "Step : 10, valid Loss : 0.1010\n",
            "Step : 20, valid Loss : 0.1079\n",
            "Step : 30, valid Loss : 0.1267\n",
            "Step : 40, valid Loss : 0.0895\n",
            "Step : 50, valid Loss : 0.1036\n",
            "Step : 60, valid Loss : 0.0889\n",
            "Step : 70, valid Loss : 0.1069\n",
            "Step : 80, valid Loss : 0.0888\n",
            "Step : 90, valid Loss : 0.1034\n",
            "Step : 100, valid Loss : 0.0870\n",
            "Step : 110, valid Loss : 0.0987\n",
            "Step : 120, valid Loss : 0.1129\n",
            "Step : 130, valid Loss : 0.0928\n",
            "Step : 140, valid Loss : 0.1216\n",
            "Step : 150, valid Loss : 0.1326\n",
            "Step : 160, valid Loss : 0.0959\n",
            "Step : 170, valid Loss : 0.1151\n",
            "Step : 180, valid Loss : 0.1319\n",
            "Step : 190, valid Loss : 0.1035\n",
            "Step : 200, valid Loss : 0.0786\n",
            "Step : 210, valid Loss : 0.0917\n",
            "Step : 220, valid Loss : 0.0968\n",
            "Step : 230, valid Loss : 0.0891\n",
            "Step : 240, valid Loss : 0.0964\n",
            "Step : 250, valid Loss : 0.1012\n",
            "Step : 260, valid Loss : 0.0968\n",
            "Step : 270, valid Loss : 0.1233\n",
            "Step : 280, valid Loss : 0.1217\n",
            "Step : 290, valid Loss : 0.1168\n",
            "Step : 300, valid Loss : 0.0928\n",
            "Step : 310, valid Loss : 0.0913\n",
            "Step : 320, valid Loss : 0.0958\n",
            "Step : 330, valid Loss : 0.1263\n",
            "Step : 340, valid Loss : 0.0846\n",
            "Step : 350, valid Loss : 0.1016\n",
            "Step : 360, valid Loss : 0.0924\n",
            "Step : 370, valid Loss : 0.0929\n",
            "Step : 380, valid Loss : 0.1037\n",
            "Step : 390, valid Loss : 0.0973\n",
            "Step : 400, valid Loss : 0.0964\n",
            "Step : 410, valid Loss : 0.1000\n",
            "Step : 420, valid Loss : 0.0789\n",
            "Step : 430, valid Loss : 0.0833\n",
            "Step : 440, valid Loss : 0.0987\n",
            "Step : 450, valid Loss : 0.0779\n",
            "Step : 460, valid Loss : 0.1003\n",
            "total_valid_loss :  0.10092201421478156 val_f1_score :  94.77824494778245 val_pearsonr : 97.65895894197587\n",
            "Epoch 2 total_Valid Loss : 0.1009\n",
            "*****Epoch 2 Valid Finish*****\n",
            "\n",
            "Saving epoch 2 checkpoint at /content/drive/MyDrive/AI09/model_v4.ckpt.2\n",
            "****** Starting To Train Epoch #3 ******\n",
            "Epoch: 3, Step : 10, LR : 2.487333026255182e-06, Avg Loss : 0.1101\n",
            "Epoch: 3, Step : 20, LR : 2.4758175955780746e-06, Avg Loss : 0.1346\n",
            "Epoch: 3, Step : 30, LR : 2.4643021649009674e-06, Avg Loss : 0.1023\n",
            "Epoch: 3, Step : 40, LR : 2.4527867342238602e-06, Avg Loss : 0.1214\n",
            "Epoch: 3, Step : 50, LR : 2.441271303546753e-06, Avg Loss : 0.1087\n",
            "Epoch: 3, Step : 60, LR : 2.4297558728696454e-06, Avg Loss : 0.1199\n",
            "Epoch: 3, Step : 70, LR : 2.418240442192538e-06, Avg Loss : 0.1730\n",
            "Epoch: 3, Step : 80, LR : 2.406725011515431e-06, Avg Loss : 0.1066\n",
            "Epoch: 3, Step : 90, LR : 2.3952095808383237e-06, Avg Loss : 0.1172\n",
            "Epoch: 3, Step : 100, LR : 2.383694150161216e-06, Avg Loss : 0.1323\n",
            "Epoch: 3, Step : 110, LR : 2.372178719484109e-06, Avg Loss : 0.1223\n",
            "Epoch: 3, Step : 120, LR : 2.3606632888070017e-06, Avg Loss : 0.1569\n",
            "Epoch: 3, Step : 130, LR : 2.3491478581298945e-06, Avg Loss : 0.1186\n",
            "Epoch: 3, Step : 140, LR : 2.337632427452787e-06, Avg Loss : 0.1138\n",
            "Epoch: 3, Step : 150, LR : 2.3261169967756796e-06, Avg Loss : 0.1208\n",
            "Epoch: 3, Step : 160, LR : 2.3146015660985724e-06, Avg Loss : 0.0999\n",
            "Epoch: 3, Step : 170, LR : 2.303086135421465e-06, Avg Loss : 0.1622\n",
            "Epoch: 3, Step : 180, LR : 2.2915707047443576e-06, Avg Loss : 0.1267\n",
            "Epoch: 3, Step : 190, LR : 2.2800552740672503e-06, Avg Loss : 0.1127\n",
            "Epoch: 3, Step : 200, LR : 2.2685398433901427e-06, Avg Loss : 0.1166\n",
            "Epoch: 3, Step : 210, LR : 2.2570244127130355e-06, Avg Loss : 0.0711\n",
            "Epoch: 3, Step : 220, LR : 2.2455089820359283e-06, Avg Loss : 0.1839\n",
            "Epoch: 3, Step : 230, LR : 2.233993551358821e-06, Avg Loss : 0.1614\n",
            "Epoch: 3, Step : 240, LR : 2.2224781206817134e-06, Avg Loss : 0.1535\n",
            "Epoch: 3, Step : 250, LR : 2.2109626900046062e-06, Avg Loss : 0.1381\n",
            "Epoch: 3, Step : 260, LR : 2.199447259327499e-06, Avg Loss : 0.1512\n",
            "Epoch: 3, Step : 270, LR : 2.187931828650392e-06, Avg Loss : 0.1337\n",
            "Epoch: 3, Step : 280, LR : 2.1764163979732846e-06, Avg Loss : 0.1225\n",
            "Epoch: 3, Step : 290, LR : 2.164900967296177e-06, Avg Loss : 0.1293\n",
            "Epoch: 3, Step : 300, LR : 2.1533855366190697e-06, Avg Loss : 0.1297\n",
            "Epoch: 3, Step : 310, LR : 2.1418701059419625e-06, Avg Loss : 0.1097\n",
            "Epoch: 3, Step : 320, LR : 2.1303546752648553e-06, Avg Loss : 0.1153\n",
            "Epoch: 3, Step : 330, LR : 2.1188392445877477e-06, Avg Loss : 0.0967\n",
            "Epoch: 3, Step : 340, LR : 2.1073238139106405e-06, Avg Loss : 0.0965\n",
            "Epoch: 3, Step : 350, LR : 2.095808383233533e-06, Avg Loss : 0.1318\n",
            "Epoch: 3, Step : 360, LR : 2.0842929525564256e-06, Avg Loss : 0.1294\n",
            "Epoch: 3, Step : 370, LR : 2.0727775218793184e-06, Avg Loss : 0.0867\n",
            "Epoch: 3, Step : 380, LR : 2.061262091202211e-06, Avg Loss : 0.0868\n",
            "Epoch: 3, Step : 390, LR : 2.0497466605251036e-06, Avg Loss : 0.1133\n",
            "Epoch: 3, Step : 400, LR : 2.0382312298479964e-06, Avg Loss : 0.0844\n",
            "Epoch: 3, Step : 410, LR : 2.026715799170889e-06, Avg Loss : 0.1302\n",
            "Epoch: 3, Step : 420, LR : 2.015200368493782e-06, Avg Loss : 0.0970\n",
            "Epoch: 3, Step : 430, LR : 2.0036849378166743e-06, Avg Loss : 0.1561\n",
            "Epoch: 3, Step : 440, LR : 1.992169507139567e-06, Avg Loss : 0.0935\n",
            "Epoch: 3, Step : 450, LR : 1.98065407646246e-06, Avg Loss : 0.1355\n",
            "Epoch: 3, Step : 460, LR : 1.9691386457853527e-06, Avg Loss : 0.1417\n",
            "Epoch: 3, Step : 470, LR : 1.9576232151082455e-06, Avg Loss : 0.1537\n",
            "Epoch: 3, Step : 480, LR : 1.946107784431138e-06, Avg Loss : 0.0764\n",
            "Epoch: 3, Step : 490, LR : 1.9345923537540306e-06, Avg Loss : 0.1726\n",
            "Epoch: 3, Step : 500, LR : 1.9230769230769234e-06, Avg Loss : 0.1243\n",
            "Epoch: 3, Step : 510, LR : 1.911561492399816e-06, Avg Loss : 0.1449\n",
            "Epoch: 3, Step : 520, LR : 1.9000460617227088e-06, Avg Loss : 0.0759\n",
            "Epoch: 3, Step : 530, LR : 1.8885306310456011e-06, Avg Loss : 0.1506\n",
            "Epoch: 3, Step : 540, LR : 1.877015200368494e-06, Avg Loss : 0.0948\n",
            "Epoch: 3, Step : 550, LR : 1.8654997696913865e-06, Avg Loss : 0.2303\n",
            "Epoch: 3, Step : 560, LR : 1.8539843390142793e-06, Avg Loss : 0.1181\n",
            "Epoch: 3, Step : 570, LR : 1.8424689083371719e-06, Avg Loss : 0.1048\n",
            "Epoch: 3, Step : 580, LR : 1.8309534776600646e-06, Avg Loss : 0.1074\n",
            "Epoch: 3, Step : 590, LR : 1.8194380469829572e-06, Avg Loss : 0.0867\n",
            "Epoch: 3, Step : 600, LR : 1.80792261630585e-06, Avg Loss : 0.1524\n",
            "Epoch: 3, Step : 610, LR : 1.7964071856287426e-06, Avg Loss : 0.1990\n",
            "Epoch: 3, Step : 620, LR : 1.7848917549516354e-06, Avg Loss : 0.1163\n",
            "Epoch: 3, Step : 630, LR : 1.773376324274528e-06, Avg Loss : 0.0883\n",
            "Epoch: 3, Step : 640, LR : 1.7618608935974207e-06, Avg Loss : 0.1371\n",
            "Epoch: 3, Step : 650, LR : 1.7503454629203135e-06, Avg Loss : 0.1056\n",
            "Epoch: 3, Step : 660, LR : 1.738830032243206e-06, Avg Loss : 0.1741\n",
            "Epoch: 3, Step : 670, LR : 1.7273146015660989e-06, Avg Loss : 0.0907\n",
            "Epoch: 3, Step : 680, LR : 1.7157991708889915e-06, Avg Loss : 0.1183\n",
            "Epoch: 3, Step : 690, LR : 1.7042837402118843e-06, Avg Loss : 0.1487\n",
            "Epoch: 3, Step : 700, LR : 1.6927683095347766e-06, Avg Loss : 0.1247\n",
            "Epoch: 3, Step : 710, LR : 1.6812528788576692e-06, Avg Loss : 0.0859\n",
            "Epoch: 3, Step : 720, LR : 1.669737448180562e-06, Avg Loss : 0.1054\n",
            "Epoch: 3, Step : 730, LR : 1.6582220175034548e-06, Avg Loss : 0.0929\n",
            "Epoch: 3, Step : 740, LR : 1.6467065868263474e-06, Avg Loss : 0.1156\n",
            "Epoch: 3, Step : 750, LR : 1.6351911561492401e-06, Avg Loss : 0.0869\n",
            "Epoch: 3, Step : 760, LR : 1.6236757254721327e-06, Avg Loss : 0.0969\n",
            "Epoch: 3, Step : 770, LR : 1.6121602947950255e-06, Avg Loss : 0.0992\n",
            "Epoch: 3, Step : 780, LR : 1.600644864117918e-06, Avg Loss : 0.1066\n",
            "Epoch: 3, Step : 790, LR : 1.5891294334408109e-06, Avg Loss : 0.0930\n",
            "Epoch: 3, Step : 800, LR : 1.5776140027637034e-06, Avg Loss : 0.1530\n",
            "Epoch: 3, Step : 810, LR : 1.5660985720865962e-06, Avg Loss : 0.0832\n",
            "Epoch: 3, Step : 820, LR : 1.5545831414094888e-06, Avg Loss : 0.1363\n",
            "Epoch: 3, Step : 830, LR : 1.5430677107323816e-06, Avg Loss : 0.1736\n",
            "Epoch: 3, Step : 840, LR : 1.5315522800552744e-06, Avg Loss : 0.1310\n",
            "Epoch: 3, Step : 850, LR : 1.520036849378167e-06, Avg Loss : 0.1622\n",
            "Epoch: 3, Step : 860, LR : 1.5085214187010598e-06, Avg Loss : 0.1097\n",
            "Epoch: 3, Step : 870, LR : 1.4970059880239521e-06, Avg Loss : 0.1032\n",
            "Epoch: 3, Step : 880, LR : 1.4854905573468447e-06, Avg Loss : 0.0958\n",
            "Epoch: 3, Step : 890, LR : 1.4739751266697375e-06, Avg Loss : 0.1145\n",
            "Epoch: 3, Step : 900, LR : 1.46245969599263e-06, Avg Loss : 0.1704\n",
            "Epoch: 3, Step : 910, LR : 1.4509442653155229e-06, Avg Loss : 0.1266\n",
            "Epoch: 3, Step : 920, LR : 1.4394288346384156e-06, Avg Loss : 0.1092\n",
            "Epoch: 3, Step : 930, LR : 1.4279134039613082e-06, Avg Loss : 0.0885\n",
            "Epoch: 3, Step : 940, LR : 1.416397973284201e-06, Avg Loss : 0.1587\n",
            "Epoch: 3, Step : 950, LR : 1.4048825426070936e-06, Avg Loss : 0.1779\n",
            "Epoch: 3, Step : 960, LR : 1.3933671119299864e-06, Avg Loss : 0.1172\n",
            "Epoch: 3, Step : 970, LR : 1.381851681252879e-06, Avg Loss : 0.1325\n",
            "Epoch: 3, Step : 980, LR : 1.3703362505757717e-06, Avg Loss : 0.1425\n",
            "Epoch: 3, Step : 990, LR : 1.3588208198986643e-06, Avg Loss : 0.0937\n",
            "Epoch: 3, Step : 1000, LR : 1.347305389221557e-06, Avg Loss : 0.1119\n",
            "Epoch: 3, Step : 1010, LR : 1.3357899585444497e-06, Avg Loss : 0.0867\n",
            "Epoch: 3, Step : 1020, LR : 1.3242745278673425e-06, Avg Loss : 0.0919\n",
            "Epoch: 3, Step : 1030, LR : 1.312759097190235e-06, Avg Loss : 0.1301\n",
            "Epoch: 3, Step : 1040, LR : 1.3012436665131278e-06, Avg Loss : 0.1642\n",
            "Epoch: 3, Step : 1050, LR : 1.2897282358360202e-06, Avg Loss : 0.1087\n",
            "Epoch: 3, Step : 1060, LR : 1.278212805158913e-06, Avg Loss : 0.0982\n",
            "Epoch: 3, Step : 1070, LR : 1.2666973744818056e-06, Avg Loss : 0.0860\n",
            "Epoch: 3, Step : 1080, LR : 1.2551819438046983e-06, Avg Loss : 0.2035\n",
            "Epoch: 3, Step : 1090, LR : 1.243666513127591e-06, Avg Loss : 0.0912\n",
            "Epoch: 3, Step : 1100, LR : 1.2321510824504837e-06, Avg Loss : 0.1408\n",
            "Epoch: 3, Step : 1110, LR : 1.2206356517733765e-06, Avg Loss : 0.0825\n",
            "Epoch: 3, Step : 1120, LR : 1.209120221096269e-06, Avg Loss : 0.1512\n",
            "Epoch: 3, Step : 1130, LR : 1.1976047904191619e-06, Avg Loss : 0.1660\n",
            "Epoch: 3, Step : 1140, LR : 1.1860893597420544e-06, Avg Loss : 0.1206\n",
            "Epoch: 3, Step : 1150, LR : 1.1745739290649472e-06, Avg Loss : 0.1233\n",
            "Epoch: 3, Step : 1160, LR : 1.1630584983878398e-06, Avg Loss : 0.1402\n",
            "Epoch: 3, Step : 1170, LR : 1.1515430677107326e-06, Avg Loss : 0.1008\n",
            "Epoch: 3, Step : 1180, LR : 1.1400276370336252e-06, Avg Loss : 0.1414\n",
            "Epoch: 3, Step : 1190, LR : 1.1285122063565177e-06, Avg Loss : 0.1921\n",
            "Epoch: 3, Step : 1200, LR : 1.1169967756794105e-06, Avg Loss : 0.1036\n",
            "Epoch: 3, Step : 1210, LR : 1.1054813450023031e-06, Avg Loss : 0.1201\n",
            "Epoch: 3, Step : 1220, LR : 1.093965914325196e-06, Avg Loss : 0.1531\n",
            "Epoch: 3, Step : 1230, LR : 1.0824504836480885e-06, Avg Loss : 0.1262\n",
            "Epoch: 3, Step : 1240, LR : 1.0709350529709813e-06, Avg Loss : 0.1001\n",
            "Epoch: 3, Step : 1250, LR : 1.0594196222938738e-06, Avg Loss : 0.1593\n",
            "Epoch: 3, Step : 1260, LR : 1.0479041916167664e-06, Avg Loss : 0.0783\n",
            "Epoch: 3, Step : 1270, LR : 1.0363887609396592e-06, Avg Loss : 0.0786\n",
            "Epoch: 3, Step : 1280, LR : 1.0248733302625518e-06, Avg Loss : 0.1271\n",
            "Epoch: 3, Step : 1290, LR : 1.0133578995854446e-06, Avg Loss : 0.0868\n",
            "Epoch: 3, Step : 1300, LR : 1.0018424689083372e-06, Avg Loss : 0.1090\n",
            "Epoch: 3, Step : 1310, LR : 9.9032703823123e-07, Avg Loss : 0.0785\n",
            "Epoch: 3, Step : 1320, LR : 9.788116075541227e-07, Avg Loss : 0.1187\n",
            "Epoch: 3, Step : 1330, LR : 9.672961768770153e-07, Avg Loss : 0.1051\n",
            "Epoch: 3, Step : 1340, LR : 9.55780746199908e-07, Avg Loss : 0.0631\n",
            "Epoch: 3, Step : 1350, LR : 9.442653155228006e-07, Avg Loss : 0.2108\n",
            "Epoch: 3, Step : 1360, LR : 9.327498848456932e-07, Avg Loss : 0.2294\n",
            "Epoch: 3, Step : 1370, LR : 9.212344541685859e-07, Avg Loss : 0.1242\n",
            "Epoch: 3, Step : 1380, LR : 9.097190234914786e-07, Avg Loss : 0.0911\n",
            "Epoch: 3, Step : 1390, LR : 8.982035928143713e-07, Avg Loss : 0.1092\n",
            "Epoch: 3, Step : 1400, LR : 8.86688162137264e-07, Avg Loss : 0.1852\n",
            "Epoch: 3, Step : 1410, LR : 8.751727314601568e-07, Avg Loss : 0.1652\n",
            "Epoch: 3, Step : 1420, LR : 8.636573007830494e-07, Avg Loss : 0.1843\n",
            "Epoch: 3, Step : 1430, LR : 8.521418701059421e-07, Avg Loss : 0.1128\n",
            "Epoch: 3, Step : 1440, LR : 8.406264394288346e-07, Avg Loss : 0.0994\n",
            "Epoch: 3, Step : 1450, LR : 8.291110087517274e-07, Avg Loss : 0.1758\n",
            "Epoch: 3, Step : 1460, LR : 8.175955780746201e-07, Avg Loss : 0.2203\n",
            "Epoch: 3, Step : 1470, LR : 8.060801473975128e-07, Avg Loss : 0.2161\n",
            "Epoch: 3, Step : 1480, LR : 7.945647167204054e-07, Avg Loss : 0.1221\n",
            "Epoch: 3, Step : 1490, LR : 7.830492860432981e-07, Avg Loss : 0.1108\n",
            "Epoch: 3, Step : 1500, LR : 7.715338553661908e-07, Avg Loss : 0.1436\n",
            "Epoch: 3, Step : 1510, LR : 7.600184246890835e-07, Avg Loss : 0.0861\n",
            "Epoch: 3, Step : 1520, LR : 7.485029940119761e-07, Avg Loss : 0.1264\n",
            "Epoch: 3, Step : 1530, LR : 7.369875633348687e-07, Avg Loss : 0.1514\n",
            "Epoch: 3, Step : 1540, LR : 7.254721326577614e-07, Avg Loss : 0.1743\n",
            "Epoch: 3, Step : 1550, LR : 7.139567019806541e-07, Avg Loss : 0.1440\n",
            "Epoch: 3, Step : 1560, LR : 7.024412713035468e-07, Avg Loss : 0.1076\n",
            "Epoch: 3, Step : 1570, LR : 6.909258406264395e-07, Avg Loss : 0.1194\n",
            "Epoch: 3, Step : 1580, LR : 6.794104099493322e-07, Avg Loss : 0.1214\n",
            "Epoch: 3, Step : 1590, LR : 6.678949792722248e-07, Avg Loss : 0.1403\n",
            "Epoch: 3, Step : 1600, LR : 6.563795485951175e-07, Avg Loss : 0.1371\n",
            "Epoch: 3, Step : 1610, LR : 6.448641179180101e-07, Avg Loss : 0.1209\n",
            "Epoch: 3, Step : 1620, LR : 6.333486872409028e-07, Avg Loss : 0.1107\n",
            "Epoch: 3, Step : 1630, LR : 6.218332565637955e-07, Avg Loss : 0.1477\n",
            "Epoch: 3, Step : 1640, LR : 6.103178258866883e-07, Avg Loss : 0.1608\n",
            "Epoch: 3, Step : 1650, LR : 5.988023952095809e-07, Avg Loss : 0.1049\n",
            "Epoch: 3, Step : 1660, LR : 5.872869645324736e-07, Avg Loss : 0.1781\n",
            "Epoch: 3, Step : 1670, LR : 5.757715338553663e-07, Avg Loss : 0.1156\n",
            "Epoch: 3, Step : 1680, LR : 5.642561031782589e-07, Avg Loss : 0.1061\n",
            "Epoch: 3, Step : 1690, LR : 5.527406725011516e-07, Avg Loss : 0.1940\n",
            "Epoch: 3, Step : 1700, LR : 5.412252418240442e-07, Avg Loss : 0.1227\n",
            "Epoch: 3, Step : 1710, LR : 5.297098111469369e-07, Avg Loss : 0.2193\n",
            "Epoch: 3, Step : 1720, LR : 5.181943804698296e-07, Avg Loss : 0.0915\n",
            "Epoch: 3, Step : 1730, LR : 5.066789497927223e-07, Avg Loss : 0.1033\n",
            "Epoch: 3, Step : 1740, LR : 4.95163519115615e-07, Avg Loss : 0.1123\n",
            "Epoch: 3, Step : 1750, LR : 4.836480884385077e-07, Avg Loss : 0.1520\n",
            "Epoch: 3, Step : 1760, LR : 4.721326577614003e-07, Avg Loss : 0.1432\n",
            "Epoch: 3, Step : 1770, LR : 4.6061722708429296e-07, Avg Loss : 0.2026\n",
            "Epoch: 3, Step : 1780, LR : 4.4910179640718565e-07, Avg Loss : 0.1016\n",
            "Epoch: 3, Step : 1790, LR : 4.375863657300784e-07, Avg Loss : 0.0914\n",
            "Epoch: 3, Step : 1800, LR : 4.2607093505297106e-07, Avg Loss : 0.0854\n",
            "Epoch: 3, Step : 1810, LR : 4.145555043758637e-07, Avg Loss : 0.1789\n",
            "Epoch: 3, Step : 1820, LR : 4.030400736987564e-07, Avg Loss : 0.1673\n",
            "Epoch: 3, Step : 1830, LR : 3.9152464302164906e-07, Avg Loss : 0.1121\n",
            "Epoch: 3, Step : 1840, LR : 3.8000921234454174e-07, Avg Loss : 0.0871\n",
            "Epoch: 3, Step : 1850, LR : 3.6849378166743437e-07, Avg Loss : 0.1692\n",
            "Epoch: 3, Step : 1860, LR : 3.5697835099032705e-07, Avg Loss : 0.1070\n",
            "Epoch: 3, Step : 1870, LR : 3.4546292031321974e-07, Avg Loss : 0.0925\n",
            "Epoch: 3, Step : 1880, LR : 3.339474896361124e-07, Avg Loss : 0.1341\n",
            "Epoch: 3, Step : 1890, LR : 3.2243205895900505e-07, Avg Loss : 0.1284\n",
            "Epoch: 3, Step : 1900, LR : 3.1091662828189773e-07, Avg Loss : 0.1254\n",
            "Epoch: 3, Step : 1910, LR : 2.9940119760479047e-07, Avg Loss : 0.1356\n",
            "Epoch: 3, Step : 1920, LR : 2.8788576692768315e-07, Avg Loss : 0.1203\n",
            "Epoch: 3, Step : 1930, LR : 2.763703362505758e-07, Avg Loss : 0.0891\n",
            "Epoch: 3, Step : 1940, LR : 2.6485490557346846e-07, Avg Loss : 0.2697\n",
            "Epoch: 3, Step : 1950, LR : 2.5333947489636114e-07, Avg Loss : 0.1075\n",
            "Epoch: 3, Step : 1960, LR : 2.418240442192538e-07, Avg Loss : 0.0804\n",
            "Epoch: 3, Step : 1970, LR : 2.3030861354214648e-07, Avg Loss : 0.0832\n",
            "Epoch: 3, Step : 1980, LR : 2.187931828650392e-07, Avg Loss : 0.0944\n",
            "Epoch: 3, Step : 1990, LR : 2.0727775218793185e-07, Avg Loss : 0.1005\n",
            "Epoch: 3, Step : 2000, LR : 1.9576232151082453e-07, Avg Loss : 0.1049\n",
            "Epoch: 3, Step : 2010, LR : 1.8424689083371719e-07, Avg Loss : 0.0956\n",
            "Epoch: 3, Step : 2020, LR : 1.7273146015660987e-07, Avg Loss : 0.1006\n",
            "Epoch: 3, Step : 2030, LR : 1.6121602947950252e-07, Avg Loss : 0.0708\n",
            "Epoch: 3, Step : 2040, LR : 1.4970059880239523e-07, Avg Loss : 0.1093\n",
            "Epoch: 3, Step : 2050, LR : 1.381851681252879e-07, Avg Loss : 0.0902\n",
            "Epoch: 3, Step : 2060, LR : 1.2666973744818057e-07, Avg Loss : 0.0911\n",
            "Epoch: 3, Step : 2070, LR : 1.1515430677107324e-07, Avg Loss : 0.1165\n",
            "Epoch: 3, Step : 2080, LR : 1.0363887609396592e-07, Avg Loss : 0.0890\n",
            "Epoch: 3, Step : 2090, LR : 9.212344541685859e-08, Avg Loss : 0.0922\n",
            "Epoch: 3, Step : 2100, LR : 8.060801473975126e-08, Avg Loss : 0.0890\n",
            "Epoch: 3, Step : 2110, LR : 6.909258406264394e-08, Avg Loss : 0.1223\n",
            "Epoch: 3, Step : 2120, LR : 5.757715338553662e-08, Avg Loss : 0.1252\n",
            "Epoch: 3, Step : 2130, LR : 4.6061722708429296e-08, Avg Loss : 0.2147\n",
            "Epoch: 3, Step : 2140, LR : 3.454629203132197e-08, Avg Loss : 0.0929\n",
            "Epoch: 3, Step : 2150, LR : 2.3030861354214648e-08, Avg Loss : 0.1180\n",
            "Epoch: 3, Step : 2160, LR : 1.1515430677107324e-08, Avg Loss : 0.0890\n",
            "Epoch: 3, Step : 2170, LR : 0.0, Avg Loss : 0.1299\n",
            "Epoch 3 total_train_loss : 0.1251\n",
            "***** Finish To Train Epoch 3 *****\n",
            "\n",
            "*****Epoch 3 Valid Start*****\n",
            "Step : 10, valid Loss : 0.0832\n",
            "Step : 20, valid Loss : 0.0841\n",
            "Step : 30, valid Loss : 0.0995\n",
            "Step : 40, valid Loss : 0.0831\n",
            "Step : 50, valid Loss : 0.0862\n",
            "Step : 60, valid Loss : 0.0731\n",
            "Step : 70, valid Loss : 0.0823\n",
            "Step : 80, valid Loss : 0.0775\n",
            "Step : 90, valid Loss : 0.0821\n",
            "Step : 100, valid Loss : 0.0788\n",
            "Step : 110, valid Loss : 0.0807\n",
            "Step : 120, valid Loss : 0.0929\n",
            "Step : 130, valid Loss : 0.0817\n",
            "Step : 140, valid Loss : 0.1075\n",
            "Step : 150, valid Loss : 0.1128\n",
            "Step : 160, valid Loss : 0.0781\n",
            "Step : 170, valid Loss : 0.1006\n",
            "Step : 180, valid Loss : 0.1111\n",
            "Step : 190, valid Loss : 0.0960\n",
            "Step : 200, valid Loss : 0.0673\n",
            "Step : 210, valid Loss : 0.0713\n",
            "Step : 220, valid Loss : 0.0871\n",
            "Step : 230, valid Loss : 0.0779\n",
            "Step : 240, valid Loss : 0.0885\n",
            "Step : 250, valid Loss : 0.0964\n",
            "Step : 260, valid Loss : 0.0786\n",
            "Step : 270, valid Loss : 0.1017\n",
            "Step : 280, valid Loss : 0.0979\n",
            "Step : 290, valid Loss : 0.1013\n",
            "Step : 300, valid Loss : 0.0872\n",
            "Step : 310, valid Loss : 0.0889\n",
            "Step : 320, valid Loss : 0.0771\n",
            "Step : 330, valid Loss : 0.1089\n",
            "Step : 340, valid Loss : 0.0774\n",
            "Step : 350, valid Loss : 0.0849\n",
            "Step : 360, valid Loss : 0.0934\n",
            "Step : 370, valid Loss : 0.0761\n",
            "Step : 380, valid Loss : 0.0797\n",
            "Step : 390, valid Loss : 0.0912\n",
            "Step : 400, valid Loss : 0.0878\n",
            "Step : 410, valid Loss : 0.0865\n",
            "Step : 420, valid Loss : 0.0799\n",
            "Step : 430, valid Loss : 0.0674\n",
            "Step : 440, valid Loss : 0.0729\n",
            "Step : 450, valid Loss : 0.0740\n",
            "Step : 460, valid Loss : 0.0889\n",
            "total_valid_loss :  0.08664588309300225 val_f1_score :  95.03930604499865 val_pearsonr : 97.96204531400944\n",
            "Epoch 3 total_Valid Loss : 0.0866\n",
            "*****Epoch 3 Valid Finish*****\n",
            "\n",
            "Saving epoch 3 checkpoint at /content/drive/MyDrive/AI09/model_v4.ckpt.3\n",
            "Train Finished\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "039300ff15614fda89ca7a69043d2087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>total_f1_score </td><td>▁▆▇█</td></tr><tr><td>total_pearsonr</td><td>▁▅▇█</td></tr><tr><td>total_train_loss</td><td>█▃▂▁</td></tr><tr><td>total_train_lr</td><td>█▆▃▁</td></tr><tr><td>total_valid_loss</td><td>█▅▂▁</td></tr><tr><td>train_loss</td><td>█▃▃▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_lr</td><td>███▇▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>valid_loss</td><td>▅▅▇▅▅▆▆▇▅█▆▄▄▄▄▄▄▃▃▅▂▁▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>total_f1_score </td><td>95.03931</td></tr><tr><td>total_pearsonr</td><td>97.96205</td></tr><tr><td>total_train_loss</td><td>0.1251</td></tr><tr><td>total_train_lr</td><td>0.0</td></tr><tr><td>total_valid_loss</td><td>0.08665</td></tr><tr><td>train_loss</td><td>0.12985</td></tr><tr><td>train_lr</td><td>0.0</td></tr><tr><td>valid_loss</td><td>0.08885</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">twilight-sweep-1</strong>: <a href=\"https://wandb.ai/kdb/AI09_v1/runs/nbb3o43b\" target=\"_blank\">https://wandb.ai/kdb/AI09_v1/runs/nbb3o43b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220602_012805-nbb3o43b/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt1 = '/content/drive/MyDrive/AI09/model_v4.ckpt.0'\n",
        "ckpt2 = '/content/drive/MyDrive/AI09/model_v4.ckpt.1'\n",
        "ckpt3 = '/content/drive/MyDrive/AI09/model_v4.ckpt.2'\n",
        "ckpt4 = '/content/drive/MyDrive/AI09/model_v4.ckpt.3'"
      ],
      "metadata": {
        "id": "s1lgrfduHBt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader):    \n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    batch_count = 0\n",
        "    batch_loss = 0\n",
        "    \n",
        "    pred_np = None\n",
        "\n",
        "    for step, batch in enumerate(dataloader):       \n",
        "        batch_count += 1\n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "\n",
        "        batch_input, batch_label = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "             outputs = model(**batch_input, labels = batch_label)\n",
        "    \n",
        "        loss = outputs.loss\n",
        "        pred = outputs.logits.squeeze()\n",
        "\n",
        "        if pred_np is None:\n",
        "            pred_np = pred.detach().cpu().numpy()\n",
        "            label_np = batch_label.detach().cpu().numpy()\n",
        "        else:\n",
        "            pred_np = np.append(pred_np, pred.detach().cpu().numpy(), axis=0)\n",
        "            label_np = np.append(label_np, batch_label.detach().cpu().numpy(), axis=0)\n",
        "        \n",
        "        batch_loss += loss.item()\n",
        "        total_loss += loss.item()\n",
        "                        \n",
        "        if (step % 10) == 0 and step != 0:\n",
        "                          \n",
        "            batch_loss, batch_count = 0, 0\n",
        "\n",
        "    total_valid_loss = total_loss / (step + 1)\n",
        "\n",
        "    fone_pred = np.where(pred_np >=3, 1, 0)\n",
        "    fone_label = np.where(label_np >=3, 1, 0)\n",
        "       \n",
        "    fone= f1_score(fone_pred , fone_label) * 100\n",
        "    p_score = pearsonr(pred_np, label_np)[0] * 100           \n",
        "    print('total_test_loss : ' , total_valid_loss, \"total_f1_score : \" , fone, \"total_pearsonr:\" , p_score)"
      ],
      "metadata": {
        "id": "Z1FVyXbOc1nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", num_labels = 1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMVWQM8BdWFk",
        "outputId": "4b1105a1-f8fe-41ee-e036-3633dc811509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_checkpoints = [ckpt1, ckpt2, ckpt3, ckpt4]\n",
        "\n",
        "for checkpoint in all_checkpoints:\n",
        "    loaded_ckpt = torch.load(checkpoint, map_location=device)\n",
        "    loaded_ckpt['epoch'], loaded_ckpt['loss']\n",
        "    model.load_state_dict(loaded_ckpt[\"model_state_dict\"])\n",
        "    test(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QADzBUEAXgQ",
        "outputId": "40fc67f4-2124-4b02-e4bd-239899c19ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_test_loss :  0.5456787615794046 total_f1_score :  85.94377510040161 total_pearsonr: 91.0554006124478\n",
            "total_test_loss :  0.3652469205585013 total_f1_score :  84.07643312101911 total_pearsonr: 91.61072143340733\n",
            "total_test_loss :  0.36333515667818367 total_f1_score :  85.77319587628865 total_pearsonr: 92.3367245640512\n",
            "total_test_loss :  0.36961695699515 total_f1_score :  86.00823045267488 total_pearsonr: 92.35652059000972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종모델 선정"
      ],
      "metadata": {
        "id": "pdIOHMsgEf3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_ckpt = torch.load(ckpt4, map_location=device)\n",
        "model.load_state_dict(loaded_ckpt[\"model_state_dict\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT4gQo3k5qSa",
        "outputId": "2aa4a98c-dce0-4208-8c39-d1d187f37372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qR3IHH_G5tsJ",
        "outputId": "da240a99-120b-46ce-983f-3e6b7b3364c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_test_loss :  0.36961695699515 total_f1_score :  86.00823045267488 total_pearsonr: 92.35652059000972\n"
          ]
        }
      ]
    }
  ]
}